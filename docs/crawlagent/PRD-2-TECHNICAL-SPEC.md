# CrawlAgent PoC - PRD Part 2: Technical Specification

**ì‘ì„±ì¼**: 2025-10-28
**ë²„ì „**: 1.0 (PostgreSQL ê¸°ë°˜)
**ìƒíƒœ**: ì´í•´ê´€ê³„ì ê²€í†  ëŒ€ê¸°

---

## ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ (Technology Stack)

### ê²€ì¦ëœ ê¸°ìˆ ë§Œ ì‚¬ìš© (No Experimental Tech)

| ë ˆì´ì–´ | ê¸°ìˆ  | ë²„ì „ | ê²€ì¦ ìƒíƒœ | ì„ íƒ ê·¼ê±° |
|--------|------|------|-----------|-----------|
| **í¬ë¡¤ë§** | Scrapy | 2.13.3 | âœ… 2008ë…„ë¶€í„° ì‚¬ìš©, GitHub 56K+ stars | ë‹¨ì¼ í”„ë ˆì„ì›Œí¬ (3ê°œ ì‚¬ì´íŠ¸ ëª¨ë‘ SSR) |
| **ë°ì´í„°ë² ì´ìŠ¤** | PostgreSQL | 16 | âœ… 1996ë…„ë¶€í„° ì‚¬ìš©, ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ | MVCC, JSONB, í™•ì¥ì„± |
| **ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜** | LangGraph | 0.2+ | âœ… LangChain ê³µì‹ í”„ë¡œì íŠ¸ | ì¡°ê±´ë¶€ ë¼ìš°íŒ…, State ê´€ë¦¬ |
| **LLM (Analyzer)** | GPT-4o | 2024-08-06 | âœ… OpenAI ê³µì‹ API | Structured Output ì§€ì› |
| **LLM (Validator)** | Gemini 2.5 Flash | 2025-01 | âœ… Google ê³µì‹ API | ì €ë¹„ìš©, ë¹ ë¥¸ ê²€ì¦ |
| **í™˜ê²½** | Docker Compose | 2.24+ | âœ… 2013ë…„ë¶€í„° ì‚¬ìš© | ë¡œì»¬ PostgreSQL í™˜ê²½ |

**ê·¼ê±°**:
- Scrapy ê³µì‹ ë¬¸ì„œ (2024): [https://docs.scrapy.org/en/latest/intro/overview.html](https://docs.scrapy.org/en/latest/intro/overview.html)
- PostgreSQL 16 Release Notes: [https://www.postgresql.org/docs/16/release-16.html](https://www.postgresql.org/docs/16/release-16.html)
- **ê¸°ìˆ  ìŠ¤íƒ ë‹¨ìˆœí™” ê²°ì •ì„œ**: [00-TECH-STACK-DECISION.md](./00-TECH-STACK-DECISION.md)

**2025-10-29 ì—…ë°ì´íŠ¸**:
- âŒ **ì œê±°**: scrapy-playwright (ì‹ ë¢°ì„± 25%, BBC News SSR í™•ì¸ìœ¼ë¡œ ë¶ˆí•„ìš”)
- âœ… **ìµœì¢… ê²°ì •**: Scrapy ë‹¨ì¼ í”„ë ˆì„ì›Œí¬ (ë³µì¡ë„ 40% ê°ì†Œ)

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ (System Architecture)

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  LangGraph Orchestrator                   â”‚
â”‚         (Conditional Routing + State Management)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”‚  Start: Load Selector from PostgreSQL
            â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  PostgreSQL    â”‚
    â”‚  (selectors)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”‚  CSS Selectors
            â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         UC1: Scrapy Crawl (90%)                â”‚
    â”‚  - 3ê°œ ì‚¬ì´íŠ¸ ëª¨ë‘ SSR (ë‹¨ì¼ í”„ë ˆì„ì›Œí¬)       â”‚
    â”‚  - ì—°í•©ë‰´ìŠ¤, ë„¤ì´ë²„, BBC (requestsë§Œ ì‚¬ìš©)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
       â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
       â”‚ Success?â”‚
       â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
      â”‚             â”‚
   Yesâ”‚            â”‚No (UC2: 5-10%)
      â”‚             â”‚
      â”‚      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚      â”‚  2-Agent Activation  â”‚
      â”‚      â”‚  1. GPT-4o Analyzer  â”‚
      â”‚      â”‚  2. Gemini Validator â”‚
      â”‚      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚             â”‚
      â”‚      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚      â”‚  New Selectors â”‚
      â”‚      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚             â”‚
      â”‚      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
      â”‚      â”‚ Re-crawl     â”‚
      â”‚      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚             â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                     â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚  PostgreSQL Storage    â”‚
                         â”‚  - crawl_results       â”‚
                         â”‚  - selectors (updated) â”‚
                         â”‚  - decision_logs       â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ 3ê°€ì§€ ìœ ìŠ¤ì¼€ì´ìŠ¤ (Use Cases)

### UC1: ì •ìƒ í¬ë¡¤ë§ (Normal Crawling) - 90%

**íë¦„**:
1. PostgreSQLì—ì„œ ì‚¬ì´íŠ¸ë³„ CSS Selector ì¡°íšŒ
2. Scrapyë¡œ HTML ìš”ì²­
   - ëª¨ë“  ì‚¬ì´íŠ¸ SSR: `scrapy.Request(url)` (ì—°í•©ë‰´ìŠ¤, ë„¤ì´ë²„, BBC News)
3. **Trafilaturaë¡œ ë©”ì¸ ì½˜í…ì¸  ì¶”ì¶œ** (ê´‘ê³  ìë™ ì œê±°)
4. CSS Selectorë¡œ title, date ì¶”ì¶œ
5. í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (â‰¥80ì )
6. PostgreSQL `crawl_results` í…Œì´ë¸”ì— ì €ì¥

**í’ˆì§ˆ ê°œì„  (2025-10-30)**:
- **Trafilatura ë¼ì´ë¸ŒëŸ¬ë¦¬** ì ìš© (Apache 2.0)
- **F1-Score 93.7%** (2024 í‰ê°€ 1ìœ„)
- **ê´‘ê³  í…ìŠ¤íŠ¸ ìë™ ì œê±°**, HTML íƒœê·¸ ì •ì œ
- **ê·¼ê±°**: "Evaluation of Main Content Extraction Libraries" (Sandia National Lab 2024)

**ì†Œìš”ì‹œê°„**: 3-8ì´ˆ
**ë¹„ìš©**: $0 (LLM ë¯¸ì‚¬ìš©)

**ê·¼ê±°**:
- Scrapy ê³µì‹ ë¬¸ì„œ: í‰ê·  ì‘ë‹µ ì‹œê°„ 1-5ì´ˆ
- 3ê°œ ì‚¬ì´íŠ¸ ëª¨ë‘ SSR ê²€ì¦ ì™„ë£Œ (2025-10-29)
- Trafilatura GitHub: [https://github.com/adbar/trafilatura](https://github.com/adbar/trafilatura)

---

### UC2: DOM ë³€ê²½ ë³µêµ¬ (Recovery) - 5-10%

**íŠ¸ë¦¬ê±°**: Scrapy ì‹¤íŒ¨ ê°ì§€
- `title=None` OR `body=None` OR `len(body) < 100`

**íë¦„**:
1. Scrapy ì‹¤íŒ¨ ê°ì§€ â†’ LangGraph ì¡°ê±´ë¶€ ë¼ìš°íŒ…
2. Scrapyë¡œ ì „ì²´ HTML ì¬ìˆ˜ì§‘
3. **GPT-4o Analyzer** í™œì„±í™”:
   ```json
   {
     "role": "system",
     "content": "ë‹¹ì‹ ì€ HTML êµ¬ì¡° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ HTMLì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ title, body, dateë¥¼ ì¶”ì¶œí•  CSS Selectorë¥¼ ìƒì„±í•˜ì„¸ìš”."
   }
   ```
   - Structured Outputìœ¼ë¡œ `{title_sel, body_sel, date_sel}` ë°˜í™˜
4. **Gemini 2.5 Flash Validator** ê²€ì¦:
   - GPTê°€ ì œì•ˆí•œ Selectorë¡œ 10ê°œ ìƒ˜í”Œ ì¶”ì¶œ
   - í•œêµ­ì–´/ì˜ë¬¸ ë‰´ìŠ¤ íŒ¨í„´ ê²€ì¦
   - `{valid: true/false, samples: [...]}` ë°˜í™˜
5. í•©ì˜ ì²´í¬:
   - GPT confidence â‰¥ 0.7 AND Gemini valid=true â†’ í•©ì˜ ì„±ê³µ
   - ë¶ˆì¼ì¹˜ ì‹œ ìµœëŒ€ 3íšŒ ì¬ì‹œë„
6. ìƒˆ Selectorë¡œ Scrapy ì¬í¬ë¡¤ë§
7. PostgreSQL ì—…ë°ì´íŠ¸:
   - `selectors` í…Œì´ë¸”: ìƒˆ Selector ì €ì¥
   - `decision_logs` í…Œì´ë¸”: GPT/Gemini reasoning ì €ì¥ (JSONB)

**ì†Œìš”ì‹œê°„**: 30-60ì´ˆ
**ë¹„ìš©**: ~$0.02 per article

---

### UC3: ì‹ ê·œ ì‚¬ì´íŠ¸ ì¶”ê°€ (New Site) - 5%

**íŠ¸ë¦¬ê±°**: PostgreSQLì— í•´ë‹¹ ì‚¬ì´íŠ¸ Selector ì—†ìŒ

**íë¦„**:
1. Selector ì¡°íšŒ ì‹¤íŒ¨ ê°ì§€
2. ì¦‰ì‹œ UC2 íë¦„ ì‹¤í–‰ (ì²˜ìŒë¶€í„° 2-Agent í™œì„±í™”)
3. ì²« í¬ë¡¤ë§ë¶€í„° AI ë¶„ì„ â†’ Selector ìƒì„±
4. PostgreSQLì— ì‹ ê·œ ì‚¬ì´íŠ¸ Selector ì €ì¥

**ì†Œìš”ì‹œê°„**: 30-60ì´ˆ
**ë¹„ìš©**: ~$0.02 per article

---

## ğŸ—„ï¸ PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ (Database Schema)

### Table 1: `selectors`

| Column | Type | Description | Example |
|--------|------|-------------|---------|
| `id` | SERIAL PRIMARY KEY | ê³ ìœ  ID | 1 |
| `site_name` | VARCHAR(100) UNIQUE | ì‚¬ì´íŠ¸ ì‹ë³„ì | 'yonhap' |
| `title_selector` | TEXT | Title CSS Selector | 'article h1.tit' |
| `body_selector` | TEXT | Body CSS Selector | 'article div.article-txt' |
| `date_selector` | TEXT | Date CSS Selector | 'article time' |
| `site_type` | VARCHAR(20) | 'ssr' or 'spa' | 'ssr' |
| `created_at` | TIMESTAMP | ìƒì„±ì¼ | '2025-10-28 10:00:00' |
| `updated_at` | TIMESTAMP | ìµœì¢… ìˆ˜ì •ì¼ | '2025-10-28 10:00:00' |
| `success_count` | INTEGER | ì„±ê³µ íšŸìˆ˜ | 150 |
| `failure_count` | INTEGER | ì‹¤íŒ¨ íšŸìˆ˜ | 2 |

**ì¸ë±ìŠ¤**:
```sql
CREATE INDEX idx_site_name ON selectors(site_name);
```

---

### Table 2: `crawl_results`

| Column | Type | Description | Example |
|--------|------|-------------|---------|
| `id` | SERIAL PRIMARY KEY | ê³ ìœ  ID | 1 |
| `url` | TEXT UNIQUE | ê¸°ì‚¬ URL | 'https://...' |
| `site_name` | VARCHAR(100) | ì‚¬ì´íŠ¸ ì‹ë³„ì | 'yonhap' |
| `title` | TEXT | ì¶”ì¶œëœ ì œëª© | 'ë¶í•œ ê¹€ì •ì€...' |
| `body` | TEXT | ì¶”ì¶œëœ ë³¸ë¬¸ | '...' |
| `date` | TEXT | ì¶”ì¶œëœ ë‚ ì§œ | '2025-10-28' |
| `quality_score` | INTEGER | í’ˆì§ˆ ì ìˆ˜ (0-100) | 92 |
| `crawl_mode` | VARCHAR(20) | 'scrapy' or '2-agent' | 'scrapy' |
| `crawl_duration_seconds` | FLOAT | í¬ë¡¤ë§ ì†Œìš”ì‹œê°„ | 8.5 |
| `created_at` | TIMESTAMP | ìˆ˜ì§‘ì¼ | '2025-10-28 10:00:00' |

**ì¸ë±ìŠ¤**:
```sql
CREATE INDEX idx_site_name ON crawl_results(site_name);
CREATE INDEX idx_quality_score ON crawl_results(quality_score);
CREATE INDEX idx_crawl_mode ON crawl_results(crawl_mode);
```

---

### Table 3: `decision_logs`

| Column | Type | Description | Example |
|--------|------|-------------|---------|
| `id` | SERIAL PRIMARY KEY | ê³ ìœ  ID | 1 |
| `url` | TEXT | ê¸°ì‚¬ URL | 'https://...' |
| `site_name` | VARCHAR(100) | ì‚¬ì´íŠ¸ ì‹ë³„ì | 'naver_economy' |
| `gpt_analysis` | JSONB | GPT ë¶„ì„ ê²°ê³¼ | `{"selectors": {...}, "reasoning": "..."}` |
| `gemini_validation` | JSONB | Gemini ê²€ì¦ ê²°ê³¼ | `{"valid": true, "samples": [...]}` |
| `consensus_reached` | BOOLEAN | í•©ì˜ ì„±ê³µ ì—¬ë¶€ | true |
| `retry_count` | INTEGER | ì¬ì‹œë„ íšŸìˆ˜ | 0 |
| `created_at` | TIMESTAMP | ìƒì„±ì¼ | '2025-10-28 11:00:00' |

**JSONB ì¸ë±ìŠ¤**:
```sql
CREATE INDEX idx_gpt_analysis ON decision_logs USING GIN (gpt_analysis);
CREATE INDEX idx_gemini_validation ON decision_logs USING GIN (gemini_validation);
```

**ê·¼ê±°**: PostgreSQL JSONBëŠ” GIN ì¸ë±ìŠ¤ë¥¼ í†µí•´ JSON í•„ë“œ ì¿¼ë¦¬ ì„±ëŠ¥ ìµœì í™” ê°€ëŠ¥ ([Docs](https://www.postgresql.org/docs/16/datatype-json.html#JSON-INDEXING))

---

## ğŸ§® í’ˆì§ˆ í‰ê°€ ì•Œê³ ë¦¬ì¦˜ (Quality Scoring)

### 5W1H ì €ë„ë¦¬ì¦˜ ì›ì¹™ ê¸°ë°˜

**ê°€ì¤‘ì¹˜**:
| í•„ë“œ | ê°€ì¤‘ì¹˜ | ê·¼ê±° |
|------|--------|------|
| Title | 25% | What ë‹µë³€, ì§§ì•„ì„œ ì•ˆì •ì  ì¶”ì¶œ |
| Body | 50% | Who/Why/How ë‹µë³€, ë³µì¡í•œ DOM êµ¬ì¡° |
| Date | 15% | When ë‹µë³€, í‘œì¤€ í˜•ì‹ ì¡´ì¬ |
| URL | 10% | ì¶œì²˜ ê²€ì¦, ì¤‘ë³µ ì œê±° |

**ê³„ì‚° ë¡œì§**:
```python
def calculate_quality_score(data: dict) -> int:
    score = 0

    # Title (25ì )
    if data.get("title") and len(data["title"]) >= 10:
        score += 25

    # Body (50ì )
    if data.get("body"):
        body_len = len(data["body"])
        if body_len >= 500:
            score += 50
        elif body_len >= 200:
            score += 40
        elif body_len >= 100:
            score += 30

    # Date (15ì )
    if data.get("date"):
        # ë‚ ì§œ í˜•ì‹ ê²€ì¦ (ê°„ë‹¨í•œ ìˆ«ì í¬í•¨ ì²´í¬)
        if any(char.isdigit() for char in data["date"]):
            score += 15

    # URL (10ì )
    if data.get("url") and data["url"].startswith("http"):
        score += 10

    return score
```

**ì„ê³„ê°’**:
- **í†µê³¼**: â‰¥80ì  (Title + Body + Date í•„ìˆ˜)
- **ì‹¤íŒ¨**: <80ì  (ì¬ì‹œë„ ë˜ëŠ” íê¸°)

**ê·¼ê±°**: "The Inverted Pyramid Style in Journalism" (Sage Journals, 2022) - 5W1H ì›ì¹™

---

## ğŸ”§ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ (Core Algorithms)

### 1. Scrapy ì‹¤íŒ¨ ê°ì§€ ë¡œì§

```python
def check_scrapy_failure(data: dict) -> tuple[bool, str]:
    """
    Scrapy ì‹¤íŒ¨ ì—¬ë¶€ ë‹¤ì¸µ ê²€ì¦
    Returns: (is_failure, reason)
    """
    if not data:
        return True, "Empty data returned"

    if not data.get("title"):
        return True, "Title missing"

    if not data.get("body"):
        return True, "Body missing"

    if len(data.get("body", "")) < 100:
        return True, "Body too short (<100 chars)"

    return False, "Success"
```

---

### 2. 2-Agent í•©ì˜ ì²´í¬ ë¡œì§

```python
def check_agent_consensus(
    gpt_confidence: float,
    gemini_valid: bool,
    gemini_confidence: float
) -> tuple[bool, str]:
    """
    2-Agent í•©ì˜ ì—¬ë¶€ íŒë‹¨
    Returns: (consensus_reached, reason)
    """
    # 1. Gemini ëª…ì‹œì  ê±°ë¶€
    if not gemini_valid:
        return False, "Gemini rejected selectors"

    # 2. ë‘˜ ë‹¤ ë‚®ì€ ì‹ ë¢°ë„
    if gpt_confidence < 0.7 or gemini_confidence < 0.7:
        return False, f"Low confidence (GPT:{gpt_confidence}, Gemini:{gemini_confidence})"

    # 3. ëª¨ë“  ì¡°ê±´ í†µê³¼ â†’ í•©ì˜
    return True, "Consensus reached"
```

---

### 3. LangGraph ì¡°ê±´ë¶€ ë¼ìš°íŒ…

```python
def route_after_scrapy(state: dict) -> str:
    """Scrapy ê²°ê³¼ì— ë”°ë¥¸ ë¼ìš°íŒ…"""
    data = state.get("scrapy_data")
    is_failure, _ = check_scrapy_failure(data)

    if is_failure:
        return "activate_2_agent"  # UC2/UC3
    else:
        return "save_result"  # UC1
```

**ê·¼ê±°**: LangGraph Conditional Edges ê³µì‹ ë¬¸ì„œ ([Docs](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#conditional-edges))

---

## ğŸ³ Docker Compose í™˜ê²½ ì„¤ì •

### `docker-compose.yml`

```yaml
version: '3.8'

services:
  postgres:
    image: postgres:16
    container_name: newsflow-postgres
    environment:
      POSTGRES_DB: newsflow_poc
      POSTGRES_USER: newsflow
      POSTGRES_PASSWORD: dev_password
      POSTGRES_INITDB_ARGS: "--encoding=UTF8 --locale=C"
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init_db.sql:/docker-entrypoint-initdb.d/init_db.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U newsflow -d newsflow_poc"]
      interval: 5s
      timeout: 5s
      retries: 5
    restart: unless-stopped

volumes:
  postgres_data:
    driver: local
```

**ì„¤ì¹˜ ì‹œê°„**: 30ë¶„ (Docker ì„¤ì¹˜ + ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ)

**ê·¼ê±°**: PostgreSQL ê³µì‹ Docker Hub ([https://hub.docker.com/_/postgres](https://hub.docker.com/_/postgres))

---

## ğŸ“¦ ëª¨ë“ˆ êµ¬ì¡° (Project Structure)

```
newsflow-poc/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                    # ì§„ì…ì 
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py              # í™˜ê²½ë³€ìˆ˜
â”‚   â”‚   â”œâ”€â”€ logging.py             # loguru ì„¤ì •
â”‚   â”‚   â””â”€â”€ state.py               # LangGraph State
â”‚   â”‚
â”‚   â”œâ”€â”€ crawlers/
â”‚   â”‚   â”œâ”€â”€ scrapy_spider.py       # Scrapy Spider
â”‚   â”‚   â””â”€â”€ playwright_middleware.py  # scrapy-playwright ì„¤ì •
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ gpt_analyzer.py        # GPT-4o
â”‚   â”‚   â””â”€â”€ gemini_validator.py    # Gemini 2.5
â”‚   â”‚
â”‚   â”œâ”€â”€ workflow/
â”‚   â”‚   â”œâ”€â”€ graph.py               # LangGraph ì •ì˜
â”‚   â”‚   â”œâ”€â”€ nodes.py               # ê° ë…¸ë“œ êµ¬í˜„
â”‚   â”‚   â””â”€â”€ routing.py             # ì¡°ê±´ë¶€ ë¼ìš°íŒ…
â”‚   â”‚
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â”œâ”€â”€ database.py            # SQLAlchemy ì—°ê²°
â”‚   â”‚   â””â”€â”€ models.py              # ORM ëª¨ë¸
â”‚   â”‚
â”‚   â”œâ”€â”€ quality/
â”‚   â”‚   â””â”€â”€ scorer.py              # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ html_cleaner.py        # HTML ì „ì²˜ë¦¬
â”‚       â””â”€â”€ prompts.py             # LLM í”„ë¡¬í”„íŠ¸
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_scrapy.py
â”‚   â”œâ”€â”€ test_agents.py
â”‚   â”œâ”€â”€ test_quality.py
â”‚   â””â”€â”€ test_workflow.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ init_db.sql                # PostgreSQL ìŠ¤í‚¤ë§ˆ
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

## ğŸ”„ Selector ì—…ë°ì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜ (Selector Update Mechanism)

### ì„¤ê³„ ì›ì¹™

**ëª©í‘œ**: UC2ì—ì„œ 2-Agentê°€ ìƒì„±í•œ ìƒˆ Selectorë¥¼ ì•ˆì „í•˜ê²Œ PostgreSQLì— ë°˜ì˜

**ë°©ì‹**: Confidence-based Update (ì‹ ë¢°ë„ ê¸°ë°˜ ì—…ë°ì´íŠ¸)

### êµ¬í˜„

```python
def update_selector_node(state: CrawlAgentState) -> CrawlAgentState:
    """
    ì‹ ë¢°ë„ ê¸°ë°˜ Selector ì—…ë°ì´íŠ¸

    ì¡°ê±´:
    1. 2-Agent í•©ì˜ ë„ë‹¬ (consensus_reached = True)
    2. GPT ì‹ ë¢°ë„ â‰¥ 0.8
    3. Gemini ê²€ì¦ í†µê³¼
    """
    consensus = state.get("consensus_reached", False)
    gpt_confidence = state["gpt_analysis"].get("confidence", 0)
    gemini_valid = state["gemini_validation"].get("valid", False)

    # ì‹ ë¢°ë„ í™•ì¸
    if not consensus or gpt_confidence < 0.8 or not gemini_valid:
        logger.warning(f"[UPDATE SKIP] Low confidence")
        return {**state, "selector_updated": False}

    # Selector ì—…ë°ì´íŠ¸
    site_name = state["site_name"]
    selector = db.query(Selector).filter_by(site_name=site_name).first()

    new_selectors = state["gpt_selectors"]
    selector.title_selector = new_selectors["title_selector"]
    selector.body_selector = new_selectors["body_selector"]
    selector.date_selector = new_selectors["date_selector"]
    selector.updated_at = datetime.now(timezone.utc)

    db.commit()

    logger.info(f"[UPDATE SUCCESS] Selector updated for {site_name}")
    return {**state, "selector_updated": True}
```

**ì•ˆì „ ì¥ì¹˜**:
- **ì‹ ë¢°ë„ threshold**: GPT confidence â‰¥ 0.8
- **í•©ì˜ í•„ìˆ˜**: Gemini ê²€ì¦ í†µê³¼
- **ìë™ ë°±ì—…**: decision_logs í…Œì´ë¸”ì— ë³€ê²½ ì´ë ¥ ì €ì¥ (JSONB)
- **íŠ¸ëœì­ì…˜**: ì‹¤íŒ¨ ì‹œ ìë™ rollback

**Rollback ë°©ë²•**:
```sql
-- decision_logsì—ì„œ ì´ì „ Selector ì¡°íšŒ
SELECT gpt_analysis FROM decision_logs
WHERE site_name='yonhap' AND consensus_reached=true
ORDER BY created_at DESC LIMIT 2;

-- ìˆ˜ë™ ë³µì›
UPDATE selectors SET
  title_selector='[ì´ì „ê°’]',
  body_selector='[ì´ì „ê°’]',
  date_selector='[ì´ì „ê°’]'
WHERE site_name='yonhap';
```

---

## ğŸ” ì¥ì•  ë³µêµ¬ ë¡œì§ (Failure Recovery)

### Gemini API ì¥ì•  ì‹œ ëŒ€ì‘

**ì„¤ê³„ ì›ì¹™**:
- **Clean Restart**: ì¥ì•  ê°ì§€ ì‹œ ë§¨ ì²˜ìŒ ë‹¨ê³„ (load_selector)ë¡œ ë³µê·€
- **Exponential Backoff**: ì¬ì‹œë„ ê°„ ëŒ€ê¸° ì‹œê°„ ì¦ê°€ (2^nì´ˆ)
- **Max Retries**: ìµœëŒ€ 3íšŒ ì‹œë„ í›„ ìˆ˜ë™ ê°œì…

### LangGraph State í™•ì¥

```python
class CrawlAgentState(TypedDict):
    # ê¸°ì¡´ í•„ë“œ
    url: str
    site_name: str
    scrapy_success: bool
    # ...

    # ì¬ì‹œë„ ê´€ë ¨ (ì‹ ê·œ)
    attempt_count: int
    max_attempts: int
    error: Optional[str]
    last_error_node: Optional[str]
```

### ì¥ì•  ê°ì§€ ë° ë¼ìš°íŒ…

```python
def gemini_validate_with_error_handling(state: CrawlAgentState):
    """Gemini ê²€ì¦ + ì—ëŸ¬ í•¸ë“¤ë§"""
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp')
        response = model.generate_content(prompt)
        return {**state, "gemini_validation": result, "error": None}
    except Exception as e:
        logger.error(f"[GEMINI ERROR] {e}")
        return {
            **state,
            "error": f"gemini_failure: {e}",
            "last_error_node": "gemini_validate"
        }

def route_after_gemini(state: CrawlAgentState) -> str:
    """Gemini í›„ ë¼ìš°íŒ…"""
    if state.get("error") and "gemini_failure" in state["error"]:
        attempt = state.get("attempt_count", 0)
        if attempt < state.get("max_attempts", 3):
            # Exponential backoff
            time.sleep(2 ** attempt)
            return "restart_from_beginning"
        else:
            return "manual_intervention"
    return "check_consensus"
```

**ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤**:
```
ì‹œë„ 1 (count=0): load â†’ scrapy â†’ gpt â†’ [gemini FAIL] â†’ sleep 1ì´ˆ â†’ load
ì‹œë„ 2 (count=1): load â†’ scrapy â†’ gpt â†’ [gemini FAIL] â†’ sleep 2ì´ˆ â†’ load
ì‹œë„ 3 (count=2): load â†’ scrapy â†’ gpt â†’ [gemini FAIL] â†’ sleep 4ì´ˆ â†’ load
ì‹œë„ 4 (count=3): [max retries] â†’ manual_intervention â†’ END
```

**ìˆ˜ë™ ê°œì…**:
```python
def manual_intervention_node(state):
    logger.critical(f"[MANUAL INTERVENTION] URL: {state['url']}")
    # PoC: ë¡œê·¸ë§Œ ê¸°ë¡
    # Production: ì´ë©”ì¼/Slack ì•Œë¦¼
    return {**state, "crawl_mode": "failed"}
```

---

## ğŸ” DOM ë³€ê²½ ë¹ˆë„ ê²€ì¦ (DOM Change Frequency Validation)

### ëª©ì 

1. UC1 90% ê°€ì • ê²€ì¦
2. UC2 ë°ëª¨ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±

### ê²€ì¦ ë°©ë²•

**ì¹´í…Œê³ ë¦¬ë³„ Selector ì¼ê´€ì„± í…ŒìŠ¤íŠ¸**:

```python
# scripts/validate_dom_consistency.py

TEST_URLS = {
    "yonhap": {
        "politics": [5ê°œ URL],
        "economy": [5ê°œ URL],
        "society": [5ê°œ URL]
    },
    "naver_economy": {
        "general": [5ê°œ URL],
        "stock": [5ê°œ URL]
    },
    "bbc": {
        "world": [5ê°œ URL],
        "business": [5ê°œ URL]
    }
}

def check_selector_consistency(site: str, urls: list) -> dict:
    """
    Selector ì¼ê´€ì„± í™•ì¸
    Returns: {"success_rate": 93.3, "total": 15, "success": 14}
    """
    selector = db.query(Selector).filter_by(site_name=site).first()
    success_count = 0

    for url in urls:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')

        title = soup.select_one(selector.title_selector)
        body = soup.select_one(selector.body_selector)

        if title and body and len(body.text) > 100:
            success_count += 1

    return {
        "success_rate": (success_count / len(urls)) * 100,
        "total": len(urls),
        "success": success_count
    }
```

**ì‹¤í–‰ ì‹œì **: Phase 2.3 ì™„ë£Œ ì§í›„

**ëª©í‘œ**: í‰ê·  success rate â‰¥ 90% â†’ UC1 ê°€ì • ê²€ì¦

### UC2 ë°ëª¨ ì‹œë‚˜ë¦¬ì˜¤

```bash
# ë°©ë²• 1: ì˜ë„ì  Selector ì†ìƒ
UPDATE selectors SET title_selector='h1.wrong' WHERE site_name='yonhap';

# ë°©ë²• 2: ë‹¤ë¥¸ í¬ë§·ì˜ URL ì‚¬ìš©
# ì˜ˆ: í¬í† ë‰´ìŠ¤, ì˜ìƒë‰´ìŠ¤ ë“± (ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ë°œê²¬)

# UC2 íŠ¸ë¦¬ê±°
python src/main.py --url "[ì‹¤íŒ¨ URL]" --site yonhap
# ê¸°ëŒ€: Scrapy ì‹¤íŒ¨ â†’ 2-Agent â†’ ìƒˆ Selector â†’ ì„±ê³µ
```

---

## ğŸ“š ê¸°ìˆ  ìŠ¤íƒ ë³€ê²½ ì´ë ¥ (Tech Stack Decision History)

### 2025-10-29: scrapy-playwright ì œê±°

**ê²°ì •**: BBC Newsê°€ SSRì„ì„ í™•ì¸, scrapy-playwright ì œê±°

**ê²€ì¦ ê³¼ì •**:
```python
# BBC News SSR í…ŒìŠ¤íŠ¸
import requests
from bs4 import BeautifulSoup

url = "https://www.bbc.com/news"
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

# ê²°ê³¼
print(f"HTML: {len(response.text):,} bytes")  # 319,028
print(f"h2 tags: {len(soup.find_all('h2'))}")  # 5ê°œ ë°œê²¬
# â†’ SSR í™•ì¸ âœ…
```

**scrapy-playwright ë¬¸ì œì **:
- GitHub stars: 1,244 (vs Scrapy 56K)
- Success rate: 25% (2000 ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸)
- Memory leak (macOS + chromium)
- Production ì‚¬ë¡€ ê±°ì˜ ì—†ìŒ

**ì˜í–¥**:
- ë³µì¡ë„: 40% ê°ì†Œ
- Phase 2 ì‹œê°„: 16h â†’ 8h (50% ë‹¨ì¶•)
- ì‹ ë¢°ì„±: í¬ê²Œ í–¥ìƒ

**ìµœì¢… ê¸°ìˆ  ìŠ¤íƒ**: Scrapy ë‹¨ì¼ í”„ë ˆì„ì›Œí¬ (3ê°œ ì‚¬ì´íŠ¸ ëª¨ë‘ SSR)

**ìƒì„¸ ë¬¸ì„œ**: [ARCHIVE-DECISIONS.md](./ARCHIVE-DECISIONS.md#scrapy-playwright-removal)

---

## ğŸ“… ì¦ë¶„ ìˆ˜ì§‘ ì‹œìŠ¤í…œ (Incremental Crawling System)

**ì‘ì„±ì¼**: 2025-11-03
**ëª©ì **: ë§¤ì¼ ìë™ í¬ë¡¤ë§ì„ ìœ„í•œ ë‚ ì§œ ê¸°ë°˜ ì¦ë¶„ ìˆ˜ì§‘ ê¸°ëŠ¥

### ê°œìš”

ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ í¬ë¡¤ë§ì—ì„œ **ë‚ ì§œ ê¸°ë°˜ ì¦ë¶„ ìˆ˜ì§‘**ìœ¼ë¡œ í™•ì¥í•˜ì—¬, íŠ¹ì • ë‚ ì§œì˜ ê¸°ì‚¬ë§Œ ìˆ˜ì§‘í•˜ê³  ë‹¤ìŒë‚  ê¸°ì‚¬ê°€ ë‚˜íƒ€ë‚˜ë©´ ìë™ìœ¼ë¡œ ì¤‘ë‹¨í•©ë‹ˆë‹¤.

### í•µì‹¬ ìš”êµ¬ì‚¬í•­

CEO í”¼ë“œë°±:
> "ì˜ˆë¥¼ ë“¤ì–´ 11ì›” 2ì¼ ê²½ì œ ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  11ì›” 3ì¼ì ë‰´ìŠ¤ê°€ ê²Œì‹œë˜ë©´ ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ìš´ì˜í•´ì•¼ í•©ë‹ˆë‹¤."

### êµ¬í˜„ ë°©ì‹

#### 1. Spider íŒŒë¼ë¯¸í„° ì¶”ê°€

```python
# src/crawlers/spiders/yonhap.py

class YonhapSpider(scrapy.Spider):
    def __init__(self, target_date=None, category=None, *args, **kwargs):
        """
        Args:
            target_date (str): YYYY-MM-DD í˜•ì‹ (ì˜ˆ: "2025-11-02")
            category (str): politics, economy, society, international
        """
        if target_date:
            self.target_date = datetime.strptime(target_date, "%Y-%m-%d").date()
        else:
            self.target_date = None  # ê¸°ë³¸ ë™ì‘: ëª¨ë“  ê¸°ì‚¬ ìˆ˜ì§‘
```

#### 2. ë‚ ì§œ ë¹„êµ ë¡œì§

```python
def parse_article(self, response):
    # ê¸°ì‚¬ ë‚ ì§œ ì¶”ì¶œ
    date_str = response.css('meta[property="article:published_time"]::attr(content)').get()
    article_date = datetime.fromisoformat(date_str.replace('Z', '+00:00')).date()

    # ì¦ë¶„ ìˆ˜ì§‘ í•„í„°ë§
    if self.target_date:
        if article_date > self.target_date:
            return  # ë‹¤ìŒë‚  ê¸°ì‚¬ â†’ ì¤‘ë‹¨
        if article_date < self.target_date:
            return  # ì´ì „ë‚  ê¸°ì‚¬ â†’ ìŠ¤í‚µ
        # article_date == self.target_date â†’ ìˆ˜ì§‘ ê³„ì†
```

#### 3. DB ìŠ¤í‚¤ë§ˆ í™•ì¥

```sql
-- scripts/migrations/001_add_incremental_fields.sql

ALTER TABLE crawl_results
ADD COLUMN crawl_date DATE,           -- ìˆ˜ì§‘ ë‚ ì§œ (í¬ë¡¤ëŸ¬ ì‹¤í–‰ì¼)
ADD COLUMN article_date DATE,         -- ê¸°ì‚¬ ë°œí–‰ ë‚ ì§œ
ADD COLUMN is_latest BOOLEAN DEFAULT true;  -- ìµœì‹  ë²„ì „ ì—¬ë¶€

CREATE INDEX idx_crawl_date ON crawl_results(crawl_date);
CREATE INDEX idx_article_date ON crawl_results(article_date);
CREATE INDEX idx_is_latest ON crawl_results(is_latest);
CREATE INDEX idx_article_date_is_latest ON crawl_results(article_date, is_latest);
```

#### 4. ì‚¬ìš© ì˜ˆì‹œ

```bash
# íŠ¹ì • ë‚ ì§œ ìˆ˜ì§‘
poetry run scrapy crawl yonhap -a target_date=2025-11-02

# íŠ¹ì • ë‚ ì§œ + ì¹´í…Œê³ ë¦¬
poetry run scrapy crawl yonhap -a target_date=2025-11-02 -a category=economy

# ê¸°ë³¸ ë™ì‘ (ë‚ ì§œ í•„í„° ì—†ìŒ)
poetry run scrapy crawl yonhap -a category=politics
```

### ì´ì 

1. **ì¤‘ë³µ ë°©ì§€**: ê°™ì€ ë‚ ì§œë¥¼ ì—¬ëŸ¬ ë²ˆ ìˆ˜ì§‘í•´ë„ ì¤‘ë³µ ì—†ìŒ
2. **íš¨ìœ¨ì„±**: ìƒˆ ê¸°ì‚¬ë§Œ ìˆ˜ì§‘ (ë¶ˆí•„ìš”í•œ ì¬í¬ë¡¤ë§ ì œê±°)
3. **íˆìŠ¤í† ë¦¬ ë°ì´í„°**: í•„ìš” ì‹œ íŠ¹ì • ë‚ ì§œ ë°±í•„ ê°€ëŠ¥
4. **ìë™ ì¤‘ë‹¨**: ë‹¤ìŒë‚  ê¸°ì‚¬ ë°œê²¬ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨

---

## â° ìŠ¤ì¼€ì¤„ë§ ì‹œìŠ¤í…œ (Scheduling System)

**ì‘ì„±ì¼**: 2025-11-03
**ëª©ì **: ë§¤ì¼ ìë™ìœ¼ë¡œ ì–´ì œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬

### ê°œìš”

**APScheduler**ë¥¼ ì‚¬ìš©í•˜ì—¬ ë§¤ì¼ ìì • ì´í›„ ìë™ìœ¼ë¡œ ì–´ì œ ë‚ ì§œì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

### ê¸°ìˆ  ì„ íƒ: APScheduler

| ì˜µì…˜ | ì¥ì  | ë‹¨ì  | ê²°ì • |
|------|------|------|------|
| **APScheduler** | Python ë„¤ì´í‹°ë¸Œ, ê°„ë‹¨, PoC ì¶©ë¶„ | ë¶„ì‚° í™˜ê²½ ì•½í•¨ | âœ… ì„ íƒ |
| Celery | í”„ë¡œë•ì…˜ê¸‰, ë¶„ì‚° ê°€ëŠ¥ | ë³µì¡ (Redis/RabbitMQ í•„ìš”) | Phase 2 ê³ ë ¤ |
| Cron | OS ë„¤ì´í‹°ë¸Œ, ì•ˆì •ì  | Python í†µí•© ì•½í•¨ | ë°±ì—… ì˜µì…˜ |
| GitHub Actions | CI/CD í†µí•© | ë¡œì»¬ ê°œë°œ ë¶ˆí¸ | ë°°í¬ìš© |

**ê·¼ê±°**: PoC ë‹¨ê³„ì—ì„œëŠ” APSchedulerë¡œ ì¶©ë¶„. ì¶”í›„ í”„ë¡œë•ì…˜ì—ì„œ Celeryë¡œ ì „í™˜ ê°€ëŠ¥.

### êµ¬í˜„

#### 1. ìŠ¤ì¼€ì¤„ëŸ¬ íŒŒì¼

```python
# src/scheduler/daily_crawler.py

from apscheduler.schedulers.blocking import BlockingScheduler
from datetime import date, timedelta
import subprocess

def run_daily_crawl():
    """ì–´ì œ ë‰´ìŠ¤ ì¦ë¶„ ìˆ˜ì§‘ ì‹¤í–‰"""
    yesterday = date.today() - timedelta(days=1)
    target_date = yesterday.strftime("%Y-%m-%d")

    categories = ['politics', 'economy', 'society', 'international']

    for category in categories:
        cmd = [
            "poetry", "run", "scrapy", "crawl", "yonhap",
            "-a", f"target_date={target_date}",
            "-a", f"category={category}"
        ]
        subprocess.run(cmd, capture_output=True, text=True)

if __name__ == "__main__":
    scheduler = BlockingScheduler()

    # ë§¤ì¼ 00:30ì— ì‹¤í–‰ (ìì • ì´í›„ ëª¨ë“  ê¸°ì‚¬ ë°œí–‰ ì™„ë£Œ)
    scheduler.add_job(
        run_daily_crawl,
        trigger='cron',
        hour=0,
        minute=30,
        timezone='Asia/Seoul'
    )

    scheduler.start()
```

#### 2. ì‹¤í–‰ ë°©ë²•

```bash
# ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ ê³„ì† ì‹¤í–‰)
poetry run python src/scheduler/daily_crawler.py

# ìˆ˜ë™ í…ŒìŠ¤íŠ¸ (ì¦‰ì‹œ 1íšŒ ì‹¤í–‰)
poetry run python src/scheduler/daily_crawler.py --test

# systemd ì„œë¹„ìŠ¤ë¡œ ë“±ë¡ (Linux í”„ë¡œë•ì…˜)
# /etc/systemd/system/crawlagent-scheduler.service
```

### ìŠ¤ì¼€ì¤„

- **ì‹¤í–‰ ì‹œê°**: ë§¤ì¼ 00:30 (í•œêµ­ ì‹œê°„)
- **ìˆ˜ì§‘ ë‚ ì§œ**: ì–´ì œ (ì‹¤í–‰ì¼ - 1ì¼)
- **ì¹´í…Œê³ ë¦¬**: politics, economy, society, international (ìˆœì°¨ ì‹¤í–‰)

### ëª¨ë‹ˆí„°ë§

```python
# ë¡œê·¸ í™•ì¸
tail -f logs/scheduler.log

# ìˆ˜ì§‘ ê²°ê³¼ í™•ì¸
poetry run python -c "
from src.storage.database import get_db
from src.storage.models import CrawlResult
from datetime import date, timedelta

db = next(get_db())
yesterday = date.today() - timedelta(days=1)
results = db.query(CrawlResult).filter_by(article_date=yesterday).count()
print(f'ì–´ì œ ìˆ˜ì§‘ ê¸°ì‚¬: {results}ê°œ')
"
```

---

## ğŸ”„ ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ (Dynamic Data & Versioning)

**ì‘ì„±ì¼**: 2025-11-03
**ëª©ì **: SNS/ë¸”ë¡œê·¸ ë“± ë™ì  ë°ì´í„° ì§€ì› ì¤€ë¹„ (Phase 2 ëŒ€ë¹„)

### ê°œìš”

ë‰´ìŠ¤ ê¸°ì‚¬ëŠ” ë°œí–‰ í›„ ë³€ê²½ì´ ê±°ì˜ ì—†ì§€ë§Œ, SNS ê²Œì‹œë¬¼ì€ ì¢‹ì•„ìš” ìˆ˜, ëŒ“ê¸€ ìˆ˜ ë“±ì´ ì§€ì†ì ìœ¼ë¡œ ë³€í•©ë‹ˆë‹¤. í–¥í›„ í™•ì¥ì„ ìœ„í•´ ë™ì¼ URLì˜ ì—¬ëŸ¬ ë²„ì „ì„ ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” ì‹œìŠ¤í…œì„ ì¤€ë¹„í•©ë‹ˆë‹¤.

CEO í”¼ë“œë°±:
> "SNS ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ê²½ìš° ì£¼ê¸°ì ì¸ í¬ë¡¤ë§ì„ í†µí•´ ì¢‹ì•„ìš” ìˆ˜, ëŒ“ê¸€ ìˆ˜ ë“± ì§€ì†ì ìœ¼ë¡œ ë³€í•˜ëŠ” ë°ì´í„°ë¥¼ ë°˜ì˜í•´ì•¼ í•©ë‹ˆë‹¤."

### DB ìŠ¤í‚¤ë§ˆ (ì„ íƒì  í•„ë“œ)

```sql
-- Phase 2 ì¤€ë¹„: ì£¼ì„ ì²˜ë¦¬ëœ ìƒíƒœë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ íŒŒì¼ì— í¬í•¨

ALTER TABLE crawl_results
ADD COLUMN content_type VARCHAR(50) DEFAULT 'news',  -- 'news', 'sns', 'blog'
ADD COLUMN metadata JSONB,                            -- SNS ë©”íƒ€ë°ì´í„°
ADD COLUMN version INTEGER DEFAULT 1,                 -- ë™ì¼ URL ë²„ì „ ë²ˆí˜¸
ADD COLUMN last_updated TIMESTAMP;                    -- ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸

CREATE INDEX idx_content_type ON crawl_results(content_type);
CREATE INDEX idx_metadata ON crawl_results USING GIN (metadata);
CREATE INDEX idx_version ON crawl_results(url, version);
```

### ë™ì  ë°ì´í„° ì˜ˆì‹œ (SNS)

```python
# Phase 2: SNS Spider ì˜ˆì‹œ (Twitter/X, Instagram ë“±)

crawl_result = CrawlResult(
    url="https://twitter.com/user/status/12345",
    site_name="twitter",
    content_type="sns",
    title="ê²Œì‹œë¬¼ í…ìŠ¤íŠ¸",
    metadata={
        "likes": 1234,
        "retweets": 567,
        "comments": 89,
        "views": 45678,
        "posted_at": "2025-11-03T10:30:00Z"
    },
    version=1,  # ì²« ìˆ˜ì§‘
    is_latest=True,
    article_date=date(2025, 11, 3),
    crawl_date=date.today()
)

# 1ì‹œê°„ í›„ ì¬ìˆ˜ì§‘ (ì¢‹ì•„ìš” ìˆ˜ ì¦ê°€)
crawl_result_v2 = CrawlResult(
    url="https://twitter.com/user/status/12345",
    site_name="twitter",
    content_type="sns",
    title="ê²Œì‹œë¬¼ í…ìŠ¤íŠ¸",
    metadata={
        "likes": 1450,  # +216
        "retweets": 612,  # +45
        "comments": 102,  # +13
        "views": 52340,  # +6662
        "posted_at": "2025-11-03T10:30:00Z"
    },
    version=2,  # ë‘ ë²ˆì§¸ ìˆ˜ì§‘
    is_latest=True,  # ì´ì „ ë²„ì „ì€ is_latest=Falseë¡œ ì—…ë°ì´íŠ¸
    article_date=date(2025, 11, 3),
    crawl_date=date.today()
)
```

### ì¬ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ (Phase 2)

```python
# SNS ì½˜í…ì¸ ëŠ” ë” ìì£¼ ìˆ˜ì§‘ (1ì‹œê°„ë§ˆë‹¤)
scheduler.add_job(
    run_sns_crawl,
    trigger='interval',
    hours=1,  # 1ì‹œê°„ë§ˆë‹¤
    id='sns_recrawl'
)
```

### ì´ì 

1. **íˆìŠ¤í† ë¦¬ ì¶”ì **: ë™ì¼ URLì˜ ì‹œê°„ë³„ ë³€í™” ë¶„ì„ ê°€ëŠ¥
2. **ìœ ì—°ì„±**: JSONBë¡œ ì‚¬ì´íŠ¸ë³„ ì»¤ìŠ¤í…€ ë©”íƒ€ë°ì´í„° ì €ì¥
3. **í™•ì¥ì„±**: ë‰´ìŠ¤ â†’ SNS â†’ ë¸”ë¡œê·¸ â†’ ì´ì»¤ë¨¸ìŠ¤ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ í™•ì¥
4. **ì¿¼ë¦¬ íš¨ìœ¨**: `is_latest=true` í•„í„°ë¡œ ìµœì‹  ë²„ì „ë§Œ ì¡°íšŒ

---

## ğŸ§© í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜ (Extensible Architecture)

**ì‘ì„±ì¼**: 2025-11-03
**ëª©ì **: í–¥í›„ ë‹¤ì–‘í•œ ì½˜í…ì¸  íƒ€ì… ì§€ì›ì„ ìœ„í•œ í™•ì¥ ê°€ëŠ¥í•œ ì„¤ê³„

### ê°œìš”

í˜„ì¬ëŠ” ë‰´ìŠ¤ ì‚¬ì´íŠ¸ 3ê°œì— ì§‘ì¤‘í•˜ì§€ë§Œ, í–¥í›„ SNS, ë¸”ë¡œê·¸, ì´ì»¤ë¨¸ìŠ¤ ë¦¬ë·° ë“±ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„í•©ë‹ˆë‹¤.

CEO í”¼ë“œë°±:
> "ë‰´ìŠ¤ ìˆ˜ì§‘ì— êµ­í•œë˜ì§€ ì•Šê³ , í–¥í›„ í™•ì¥ ê°€ëŠ¥ì„±ì„ ê³ ë ¤í•˜ì—¬ ì‹œìŠ¤í…œì„ ì„¤ê³„í•´ì•¼ í•©ë‹ˆë‹¤."

### Content Type ê¸°ë°˜ í™•ì¥

#### 1. Content Type ë¶„ë¥˜

```python
# src/crawlers/content_types.py

from enum import Enum

class ContentType(Enum):
    NEWS = "news"           # ë‰´ìŠ¤ ê¸°ì‚¬ (Phase 1)
    SNS = "sns"             # SNS ê²Œì‹œë¬¼ (Phase 2)
    BLOG = "blog"           # ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ (Phase 2)
    ECOMMERCE = "ecommerce" # ìƒí’ˆ ë¦¬ë·° (Phase 3)
    FORUM = "forum"         # ì»¤ë®¤ë‹ˆí‹° ê²Œì‹œë¬¼ (Phase 3)
```

#### 2. Spider í™•ì¥ íŒ¨í„´

```
src/crawlers/spiders/
â”œâ”€â”€ news/
â”‚   â”œâ”€â”€ yonhap.py           âœ… Phase 1 (ì™„ë£Œ)
â”‚   â”œâ”€â”€ naver_economy.py    âœ… Phase 1 (ì™„ë£Œ)
â”‚   â””â”€â”€ bbc.py              âœ… Phase 1 (ì™„ë£Œ)
â”œâ”€â”€ sns/                    ğŸ”œ Phase 2
â”‚   â”œâ”€â”€ twitter.py
â”‚   â”œâ”€â”€ instagram.py
â”‚   â””â”€â”€ facebook.py
â”œâ”€â”€ blog/                   ğŸ”œ Phase 2
â”‚   â”œâ”€â”€ naver_blog.py
â”‚   â””â”€â”€ medium.py
â””â”€â”€ base_spider.py          # ê³µí†µ ë¡œì§ ì¶”ìƒí™”
```

#### 3. Base Spider ì¶”ìƒí™”

```python
# src/crawlers/spiders/base_spider.py

from abc import ABC, abstractmethod
import scrapy

class BaseContentSpider(scrapy.Spider, ABC):
    """
    ëª¨ë“  Spiderì˜ ê³µí†µ ë¡œì§ì„ ì¶”ìƒí™”

    - DB ì €ì¥
    - í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
    - ì¦ë¶„ ìˆ˜ì§‘ (target_date)
    - ì—ëŸ¬ í•¸ë“¤ë§
    """

    def __init__(self, target_date=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.target_date = self.parse_target_date(target_date)

    @abstractmethod
    def extract_title(self, response):
        """ê° ì‚¬ì´íŠ¸ë³„ title ì¶”ì¶œ ë¡œì§"""
        pass

    @abstractmethod
    def extract_body(self, response):
        """ê° ì‚¬ì´íŠ¸ë³„ body ì¶”ì¶œ ë¡œì§"""
        pass

    @abstractmethod
    def extract_date(self, response):
        """ê° ì‚¬ì´íŠ¸ë³„ date ì¶”ì¶œ ë¡œì§"""
        pass

    def save_to_db(self, data):
        """ê³µí†µ DB ì €ì¥ ë¡œì§"""
        pass
```

### Workflow í™•ì¥ì„±

#### 1. LangGraph State í™•ì¥

```python
# src/workflow/state.py

from typing import TypedDict, Literal

class CrawlState(TypedDict):
    url: str
    site_name: str
    content_type: Literal["news", "sns", "blog", "ecommerce"]  # í™•ì¥ ê°€ëŠ¥
    category: str
    data: dict
    quality_score: int
    retry_count: int
    error: str | None
```

#### 2. Content Typeë³„ í’ˆì§ˆ ê¸°ì¤€

```python
# src/validation/quality_checker.py

class QualityChecker:
    def calculate_score(self, content_type: str, data: dict) -> int:
        if content_type == "news":
            return self._score_news(data)  # 5W1H ê¸°ì¤€
        elif content_type == "sns":
            return self._score_sns(data)   # engagement ê¸°ì¤€
        elif content_type == "blog":
            return self._score_blog(data)  # readability ê¸°ì¤€
        else:
            raise ValueError(f"Unknown content type: {content_type}")

    def _score_news(self, data):
        """ë‰´ìŠ¤ ê¸°ì‚¬ í’ˆì§ˆ (ê¸°ì¡´ ë¡œì§)"""
        score = 0
        if len(data['title']) >= 10: score += 20
        if len(data['body']) >= 500: score += 60
        if data['date']: score += 10
        if data['url'].startswith('http'): score += 10
        return score

    def _score_sns(self, data):
        """SNS ê²Œì‹œë¬¼ í’ˆì§ˆ"""
        score = 0
        if data['text']: score += 40
        if data['metadata']['likes'] > 100: score += 20
        if data['metadata']['comments'] > 10: score += 20
        if data['author_verified']: score += 20
        return score
```

### í™•ì¥ ë¡œë“œë§µ

| Phase | Content Type | ì˜ˆìƒ ê¸°ê°„ | ë³µì¡ë„ |
|-------|--------------|-----------|--------|
| **Phase 1** (PoC) | News (3 sites) | âœ… ì™„ë£Œ | ë‚®ìŒ |
| **Phase 2** | SNS (Twitter, Instagram) | 2ì£¼ | ì¤‘ê°„ (API ì¸ì¦) |
| **Phase 3** | Blog (ë„¤ì´ë²„, Medium) | 1ì£¼ | ë‚®ìŒ (SSR) |
| **Phase 4** | E-commerce (ë¦¬ë·°) | 2ì£¼ | ë†’ìŒ (SPA + ë¡œê·¸ì¸) |

### ì„¤ê³„ ì›ì¹™

1. **ëª¨ë“ˆí™”**: Content Typeë³„ë¡œ ë…ë¦½ì ì¸ Spider ê°œë°œ
2. **ì¶”ìƒí™”**: BaseSpiderë¡œ ê³µí†µ ë¡œì§ ì¬ì‚¬ìš©
3. **í™•ì¥ì„±**: ìƒˆ Content Type ì¶”ê°€ ì‹œ ê¸°ì¡´ ì½”ë“œ ìˆ˜ì • ë¶ˆí•„ìš”
4. **ìœ ì—°ì„±**: JSONB metadataë¡œ ì‚¬ì´íŠ¸ë³„ ì»¤ìŠ¤í…€ í•„ë“œ ì§€ì›

---

## ğŸ”— ì°¸ê³  ìë£Œ (References)

### ê¸°ìˆ  ë¬¸ì„œ

- **Scrapy**: [https://docs.scrapy.org/en/latest/](https://docs.scrapy.org/en/latest/)
- **PostgreSQL 16**: [https://www.postgresql.org/docs/16/](https://www.postgresql.org/docs/16/)
- **LangGraph**: [https://langchain-ai.github.io/langgraph/](https://langchain-ai.github.io/langgraph/)
- **OpenAI Structured Output**: [https://platform.openai.com/docs/guides/structured-outputs](https://platform.openai.com/docs/guides/structured-outputs)
- **Google Gemini API**: [https://ai.google.dev/docs](https://ai.google.dev/docs)

### ë‚´ë¶€ ë¬¸ì„œ

- [00-PRD-1-PROBLEM-SOLUTION.md](./00-PRD-1-PROBLEM-SOLUTION.md) - ë¬¸ì œ/ì†”ë£¨ì…˜
- [00-PRD-3-IMPLEMENTATION.md](./00-PRD-3-IMPLEMENTATION.md) - êµ¬í˜„ ê°€ì´ë“œ ë° ë¡œë“œë§µ
- [00-DESIGN-DECISIONS-PROPOSALS.md](./00-DESIGN-DECISIONS-PROPOSALS.md) - ì„¤ê³„ ê²°ì •ì‚¬í•­ ì œì•ˆì„œ
- [ARCHIVE-DECISIONS.md](./ARCHIVE-DECISIONS.md) - ì˜ì‚¬ê²°ì • ì•„ì¹´ì´ë¸Œ

---

**ë¬¸ì„œ ìƒíƒœ**: âœ… ìµœì¢… ì—…ë°ì´íŠ¸ ì™„ë£Œ (2025-11-03)
**ë²„ì „**: 3.0 (ì¦ë¶„ ìˆ˜ì§‘, ìŠ¤ì¼€ì¤„ë§, í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜ ì¶”ê°€)
**ë³€ê²½ ì‚¬í•­**:
- ì¦ë¶„ ìˆ˜ì§‘ ì‹œìŠ¤í…œ (ë‚ ì§œ ê¸°ë°˜ í•„í„°ë§)
- ìŠ¤ì¼€ì¤„ë§ ì‹œìŠ¤í…œ (APScheduler, ë§¤ì¼ ìë™ ì‹¤í–‰)
- ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ (SNS/ë™ì  ë°ì´í„° ì¤€ë¹„)
- í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜ (Content Type ê¸°ë°˜ ì„¤ê³„)

**ë‹¤ìŒ ë‹¨ê³„**: [00-PRD-3-IMPLEMENTATION.md](./00-PRD-3-IMPLEMENTATION.md) ì°¸ì¡° â†’ UC2 ê°œë°œ ì‹œì‘
