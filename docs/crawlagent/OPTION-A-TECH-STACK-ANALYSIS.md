# CrawlAgent ì˜µì…˜ A ê°œë°œ ìµœì¢… ê¸°ìˆ  ìŠ¤íƒ & ì¤€ë¹„ ìƒíƒœ ë³´ê³ ì„œ

**ì‘ì„±ì¼**: 2025-11-03
**ë²„ì „**: 1.0
**ëª©ì **: UC2 ê°œë°œ + ì¦ë¶„ ìˆ˜ì§‘(ì˜µì…˜ A) ê¸°ìˆ  ìŠ¤íƒ í™•ì • ë° ê°œë°œ ë ˆë”” ìƒíƒœ ê²€ì¦
**ì‘ì—… ë””ë ‰í† ë¦¬**: `/Users/charlee/Desktop/Intern/crawlagent`

---

## ğŸ¯ ìµœì¢… í™•ì • ê¸°ìˆ  ìŠ¤íƒ

### Core Stack (ë³€ê²½ ì—†ìŒ)

| ë ˆì´ì–´ | ê¸°ìˆ  | ë²„ì „ | ìƒíƒœ | ìš©ë„ |
|--------|------|------|------|------|
| **ì–¸ì–´** | Python | 3.9.6 | âœ… ì„¤ì¹˜ë¨ | ì „ì²´ ì‹œìŠ¤í…œ |
| **íŒ¨í‚¤ì§€ ê´€ë¦¬** | Poetry | 2.2.1 | âœ… ì„¤ì¹˜ë¨ | ì˜ì¡´ì„± ê´€ë¦¬ |
| **í¬ë¡¤ë§** | Scrapy | 2.11.0+ | âœ… ì„¤ì¹˜ë¨ | SSR í¬ë¡¤ë§ |
| **ì½˜í…ì¸  ì¶”ì¶œ** | Trafilatura | 1.12.0+ | âœ… ì„¤ì¹˜ë¨ | ê´‘ê³  ì œê±° |
| **ë°ì´í„°ë² ì´ìŠ¤** | PostgreSQL | 16 | âœ… Docker ì‹¤í–‰ ì¤‘ | ë°ì´í„° ì €ì¥ |
| **ORM** | SQLAlchemy | 2.0.0+ | âœ… ì„¤ì¹˜ë¨ | DB ì¶”ìƒí™” |
| **ë§ˆì´ê·¸ë ˆì´ì…˜** | Alembic | 1.13.0+ | âœ… ì„¤ì¹˜ë¨ | DB ìŠ¤í‚¤ë§ˆ ê´€ë¦¬ |
| **ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜** | LangGraph | 0.2.0+ | âœ… ì„¤ì¹˜ë¨ | Agent ì›Œí¬í”Œë¡œìš° |
| **LLM (Analyzer)** | GPT-4o | 2024-08-06 | âœ… API Key ì„¤ì •ë¨ | CSS Selector ìƒì„± |
| **LLM (Validator)** | Gemini 2.5 Flash | 2025-01 | âœ… API Key ì„¤ì •ë¨ | Selector ê²€ì¦ |
| **ë¡œê¹…** | Loguru | 0.7.0+ | âœ… ì„¤ì¹˜ë¨ | êµ¬ì¡°í™”ëœ ë¡œê¹… |
| **UI** | Gradio | 4.0.0+ | âœ… ì„¤ì¹˜ë¨ | ë°ëª¨ ì¸í„°í˜ì´ìŠ¤ |
| **ì»¨í…Œì´ë„ˆ** | Docker Compose | 2.24+ | âœ… ì‹¤í–‰ ì¤‘ | PostgreSQL í™˜ê²½ |

### ì¶”ê°€ Stack (ì˜µì…˜ Aìš© - ì¦ë¶„ ìˆ˜ì§‘ & ìŠ¤ì¼€ì¤„ë§)

| ë ˆì´ì–´ | ê¸°ìˆ  | ë²„ì „ | ìƒíƒœ | ìš©ë„ | ì„¤ì¹˜ ëª…ë ¹ |
|--------|------|------|------|------|-----------|
| **ìŠ¤ì¼€ì¤„ëŸ¬** | APScheduler | 3.10+ | âŒ **ì„¤ì¹˜ í•„ìš”** | ì¼ì¼ í¬ë¡¤ë§ ìë™í™” | `poetry add apscheduler` |
| **ë‚ ì§œ ì²˜ë¦¬** | python-dateutil | 2.9.0 | âœ… ì„¤ì¹˜ë¨ | ë‚ ì§œ íŒŒì‹±/ë¹„êµ | (ì´ë¯¸ ì„¤ì¹˜) |
| **HTTP í´ë¼ì´ì–¸íŠ¸** | httpx | 0.27.0+ | âœ… ì„¤ì¹˜ë¨ | ë¹„ë™ê¸° HTTP | (ì´ë¯¸ ì„¤ì¹˜) |
| **HTML íŒŒì‹±** | BeautifulSoup4 | 4.12.0+ | âœ… ì„¤ì¹˜ë¨ | HTML ì „ì²˜ë¦¬ | (ì´ë¯¸ ì„¤ì¹˜) |

**ì„¤ì¹˜ í•„ìš” í•­ëª©**: APSchedulerë§Œ ì¶”ê°€ ì„¤ì¹˜ í•„ìš” (1ê°œ)

---

## ğŸ“Š Claude Skills & MCP ì„œë²„ ë¶„ì„

### Part 1: Claude Skills ìƒì„± ì™„ë£Œ âœ…

**ì €ì¥ ìœ„ì¹˜**: `/Users/charlee/Desktop/Intern/crawlagent/.claude/skills/`

#### ìƒì„±ëœ Skills (3ê°œ)

| Skill íŒŒì¼ | ëª©ì  | ì£¼ìš” ë‚´ìš© | í¬ê¸° |
|-----------|------|----------|------|
| **`uc2-development.md`** | UC2 ê°œë°œ ì „ìš© ì»¨í…ìŠ¤íŠ¸ | - UC2 ì›Œí¬í”Œë¡œìš° ë‹¤ì´ì–´ê·¸ë¨<br>- GPT/Gemini êµ¬í˜„ íŒ¨í„´<br>- LangGraph StateGraph ì˜ˆì œ<br>- í…ŒìŠ¤íŠ¸ ì „ëµ | 7.2 KB |
| **`incremental-crawling.md`** | ì¦ë¶„ ìˆ˜ì§‘ êµ¬í˜„ ê°€ì´ë“œ | - ë‚ ì§œ ê¸°ë°˜ Spider ìˆ˜ì •<br>- DB ìŠ¤í‚¤ë§ˆ í™•ì¥<br>- ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸<br>- í…ŒìŠ¤íŠ¸ ë°©ë²• | 5.8 KB |
| **`scheduler.md`** | ìŠ¤ì¼€ì¤„ëŸ¬ êµ¬í˜„ ê°€ì´ë“œ | - APScheduler íŒ¨í„´<br>- Cron vs Celery ë¹„êµ<br>- Gradio UI í†µí•©<br>- ì—ëŸ¬ í•¸ë“¤ë§ | 6.4 KB |

**ì‚¬ìš© ë°©ë²•**:
```bash
# Claude Code CLIì—ì„œ ìë™ ë¡œë“œë¨ (ì¬ì‹œì‘ ë¶ˆí•„ìš”)
# Skill ì»¨í…ìŠ¤íŠ¸ëŠ” ëŒ€í™” ì‹œì‘ ì‹œ ìë™ ì œê³µë¨
```

**íš¨ê³¼**:
- UC2 ê°œë°œ ì‹œ í•µì‹¬ íŒ¨í„´ì„ ì¦‰ì‹œ ì°¸ì¡° ê°€ëŠ¥
- ì¦ë¶„ ìˆ˜ì§‘ êµ¬í˜„ ì‹œ ë‹¨ê³„ë³„ ê°€ì´ë“œ ì œê³µ
- ìŠ¤ì¼€ì¤„ëŸ¬ ì„ íƒ ì‹œ ë¹„êµí‘œë¡œ ë¹ ë¥¸ ì˜ì‚¬ê²°ì •

### Part 2: MCP ì„œë²„ ë¶„ì„

**í˜„ì¬ ìƒíƒœ**: MCP ì„œë²„ ì„¤ì • íŒŒì¼ ì—†ìŒ (`~/.config/claude/claude_desktop_config.json` ë¯¸ì¡´ì¬)

#### ìœ ìš©í•œ MCP ì„œë²„ í›„ë³´ (ì„¤ì¹˜ ê¶Œì¥í•˜ì§€ ì•ŠìŒ)

| MCP ì„œë²„ | ìš©ë„ | í‰ê°€ | ê¶Œì¥ ì—¬ë¶€ |
|----------|------|------|-----------|
| `@modelcontextprotocol/server-postgres` | PostgreSQL ì¿¼ë¦¬ ìë™í™” | PoC ë‹¨ê³„ì—ì„œ SQLAlchemy ORMìœ¼ë¡œ ì¶©ë¶„ | âŒ ë¶ˆí•„ìš” |
| `@modelcontextprotocol/server-filesystem` | íŒŒì¼ ì‘ì—… ìë™í™” | Claude Code ê¸°ë³¸ íŒŒì¼ ë„êµ¬ë¡œ ì¶©ë¶„ | âŒ ë¶ˆí•„ìš” |
| `@modelcontextprotocol/server-fetch` | HTTP ìš”ì²­ | Scrapyë¡œ ì´ë¯¸ ì²˜ë¦¬ë¨ | âŒ ë¶ˆí•„ìš” |
| `@modelcontextprotocol/server-brave-search` | ì›¹ ê²€ìƒ‰ | ì‹ ê·œ ì‚¬ì´íŠ¸ ë°œê²¬ìš© (UC3 ë‹¨ê³„) | â¸ï¸ Phase 2 |

**ê²°ë¡ **: í˜„ì¬ PoC ë‹¨ê³„ì—ì„œëŠ” **MCP ì„œë²„ ë¶ˆí•„ìš”**
- ê¸°ì¡´ ë„êµ¬(Scrapy, SQLAlchemy, Claude Code)ë¡œ ì¶©ë¶„
- ë³µì¡ë„ ì¦ê°€ ëŒ€ë¹„ íš¨ê³¼ ë¯¸ë¯¸
- UC3 (ì‹ ê·œ ì‚¬ì´íŠ¸ ë°œê²¬) ë‹¨ê³„ì—ì„œ ì¬ê²€í† 

---

## ğŸ› ï¸ ìŠ¤ì¼€ì¤„ëŸ¬ ê¸°ìˆ  ì„ ì • (Ultra-thorough ë¶„ì„)

### ë¹„êµ ë¶„ì„í‘œ

| í•­ëª© | APScheduler | Celery Beat | Cron | GitHub Actions |
|------|-------------|-------------|------|----------------|
| **ì„¤ì¹˜ ì‹œê°„** | 5ë¶„ | 30ë¶„+ | 10ë¶„ | 15ë¶„ |
| **ë³µì¡ë„** | â­ (Low) | â­â­â­â­ (High) | â­â­ (Medium) | â­â­â­ (Medium) |
| **ì™¸ë¶€ ì˜ì¡´ì„±** | ì—†ìŒ | Redis/RabbitMQ í•„ìˆ˜ | ì—†ìŒ | GitHub repo í•„ìš” |
| **Python í†µí•©** | Excellent | Excellent | Poor (subprocess) | N/A |
| **Gradio UI í†µí•©** | ì‰¬ì›€ (BackgroundScheduler) | ì–´ë ¤ì›€ | ë¶ˆê°€ëŠ¥ | ë¶ˆê°€ëŠ¥ |
| **ì¬ì‹œë„ ë¡œì§** | ìˆ˜ë™ êµ¬í˜„ | ë‚´ì¥ | ìˆ˜ë™ êµ¬í˜„ | ë‚´ì¥ |
| **ëª¨ë‹ˆí„°ë§** | ë¡œê·¸ + UI | Flower (ë³„ë„ ì„¤ì¹˜) | ë¡œê·¸ë§Œ | GitHub Actions UI |
| **í”„ë¡œë•ì…˜ ë ˆë²¨** | âœ… ì¶©ë¶„ | âœ… ìµœê³  | âœ… ì¶©ë¶„ | âŒ ì œí•œì  |
| **ê°œë°œ í¸ì˜ì„±** | â­â­â­â­â­ | â­â­ | â­â­â­ | â­â­â­ |
| **ë¹„ìš©** | ë¬´ë£Œ | ë¬´ë£Œ (self-host) | ë¬´ë£Œ | Public repoë§Œ ë¬´ë£Œ |
| **PoC ì í•©ì„±** | â­â­â­â­â­ | â­â­ | â­â­â­ | â­ |

### Option 1: APScheduler â­ **ê¶Œì¥**

**ì¥ì **:
- Python native, ì™¸ë¶€ ì„œë¹„ìŠ¤ ë¶ˆí•„ìš”
- Gradio UIì™€ ì‰½ê²Œ í†µí•© (`BackgroundScheduler`)
- 5ë¶„ ë‚´ êµ¬í˜„ ê°€ëŠ¥
- SQLite ë°±ì—”ë“œë¡œ ì‘ì—… ì§€ì†ì„± ì§€ì›
- ê°œë°œ/í…ŒìŠ¤íŠ¸ê°€ ë§¤ìš° ì‰¬ì›€

**ë‹¨ì **:
- í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œ ìŠ¤ì¼€ì¤„ ì¤‘ë‹¨ (â†’ systemd/supervisorë¡œ í•´ê²°)
- ë¶„ì‚° ì‘ì—… í ë¯¸ì§€ì› (â†’ PoCì—ì„œ ë¶ˆí•„ìš”)

**êµ¬í˜„ ì˜ˆì‹œ**:
```python
from apscheduler.schedulers.blocking import BlockingScheduler
from datetime import date, timedelta
import subprocess

def crawl_yesterday():
    yesterday = (date.today() - timedelta(days=1)).strftime("%Y-%m-%d")
    subprocess.run([
        "poetry", "run", "scrapy", "crawl", "yonhap",
        "-a", f"target_date={yesterday}"
    ])

scheduler = BlockingScheduler()
scheduler.add_job(crawl_yesterday, 'cron', hour=0, minute=30)
scheduler.start()
```

**ì‚¬ìš© ì‚¬ë¡€**: PoC ë°ëª¨, ì†Œê·œëª¨ í”„ë¡œë•ì…˜ (í•˜ë£¨ 1-10íšŒ ì‹¤í–‰)

### Option 2: Celery Beat

**ì¥ì **:
- ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ì•ˆì •ì„±
- ë¶„ì‚° ì‘ì—… í ì§€ì›
- ì¬ì‹œë„, ìš°ì„ ìˆœìœ„, ì²´ì´ë‹ ê¸°ëŠ¥
- Flower ëŒ€ì‹œë³´ë“œë¡œ ëª¨ë‹ˆí„°ë§

**ë‹¨ì **:
- Redis ë˜ëŠ” RabbitMQ í•„ìˆ˜ (ë³µì¡ë„ â†‘â†‘)
- ì„¤ì • íŒŒì¼ ë³µì¡ (celeryconfig.py, beat-schedule.db)
- PoC ë‹¨ê³„ì—ì„œ ê³¼ë„í•œ ì„¤ì •

**êµ¬í˜„ ì˜ˆì‹œ**:
```python
from celery import Celery
from celery.schedules import crontab

app = Celery('crawlagent', broker='redis://localhost:6379')

@app.task
def crawl_yesterday():
    # ... í¬ë¡¤ë§ ë¡œì§ ...

app.conf.beat_schedule = {
    'crawl-every-day': {
        'task': 'crawl_yesterday',
        'schedule': crontab(hour=0, minute=30)
    }
}
```

**ì‚¬ìš© ì‚¬ë¡€**: ëŒ€ê·œëª¨ í”„ë¡œë•ì…˜ (í•˜ë£¨ 100+ ì‘ì—…), ë¶„ì‚° í™˜ê²½

### Option 3: Cron (ì‹œìŠ¤í…œ ë ˆë²¨)

**ì¥ì **:
- OS ë ˆë²¨ ì•ˆì •ì„± (ê°€ì¥ ì‹ ë¢°ì„± ë†’ìŒ)
- ì¬ë¶€íŒ… í›„ ìë™ ì¬ì‹œì‘
- ë¡œê·¸ ë¡œí…Œì´ì…˜ ì§€ì› (logrotate)
- ì¶”ê°€ Python í”„ë¡œì„¸ìŠ¤ ë¶ˆí•„ìš”

**ë‹¨ì **:
- Pythonê³¼ í†µí•© ì–´ë ¤ì›€ (subprocessë§Œ ê°€ëŠ¥)
- Gradio UIì—ì„œ ì œì–´ ë¶ˆê°€
- ë¡œì»¬ ê°œë°œ í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸ ì–´ë ¤ì›€
- macOSì—ì„œ crontab ê¶Œí•œ ë¬¸ì œ ê°€ëŠ¥

**êµ¬í˜„ ì˜ˆì‹œ**:
```bash
# crontab -e
30 0 * * * cd /Users/charlee/Desktop/Intern/crawlagent && poetry run python src/scheduler/run_daily.py >> /var/log/crawler.log 2>&1
```

**ì‚¬ìš© ì‚¬ë¡€**: í”„ë¡œë•ì…˜ ì„œë²„ (Linux), UI ì œì–´ ë¶ˆí•„ìš”

### Option 4: GitHub Actions

**ì¥ì **:
- ì„œë²„ ë¶ˆí•„ìš” (GitHub ì¸í”„ë¼ ì‚¬ìš©)
- YAML ì„¤ì •ìœ¼ë¡œ ê°„ë‹¨
- ë¬´ë£Œ (Public repo)

**ë‹¨ì **:
- Self-hosted runner í•„ìš” (Private repo ë˜ëŠ” ë¡œì»¬ DB ì ‘ê·¼)
- Rate limit (ì›” 2000ë¶„ Free tier)
- API Key ë…¸ì¶œ ìœ„í—˜ (Secrets í•„ìš”)
- ì‹¤ì‹œê°„ ì œì–´ ì–´ë ¤ì›€

**êµ¬í˜„ ì˜ˆì‹œ**:
```yaml
# .github/workflows/daily-crawl.yml
on:
  schedule:
    - cron: '30 0 * * *'  # 00:30 UTC
jobs:
  crawl:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - run: poetry install
      - run: poetry run scrapy crawl yonhap -a target_date=$(date -d yesterday +%Y-%m-%d)
```

**ì‚¬ìš© ì‚¬ë¡€**: í´ë¼ìš°ë“œ ì „ìš©, Public repo

### ìµœì¢… ê¶Œì¥ ìˆœìœ„

| ìˆœìœ„ | ê¸°ìˆ  | ìš©ë„ | ì„ íƒ ê·¼ê±° |
|------|------|------|-----------|
| **1ìœ„** | **APScheduler** | **PoC ë°ëª¨, UC2 ê°œë°œ** | âœ… ì¦‰ì‹œ ì‹œì‘ ê°€ëŠ¥<br>âœ… Gradio í†µí•© ì‰¬ì›€<br>âœ… ë³µì¡ë„ ìµœì†Œ |
| 2ìœ„ | Cron | í”„ë¡œë•ì…˜ ì„œë²„ | ì¥ê¸° ìš´ì˜ ì‹œ ê°€ì¥ ì•ˆì •ì  |
| 3ìœ„ | Celery Beat | ëŒ€ê·œëª¨ í™•ì¥ | ë¶„ì‚° í™˜ê²½ í•„ìš” ì‹œ |
| 4ìœ„ | GitHub Actions | í´ë¼ìš°ë“œ ì „ìš© | Self-hosted runner ë³µì¡ |

**ê²°ì •**: **APScheduler 3.10+ ì‚¬ìš©** (PoC â†’ í”„ë¡œë•ì…˜ ì „í™˜ ì‹œ Cronìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜)

---

## ğŸ—„ï¸ DB ìŠ¤í‚¤ë§ˆ í™•ì¥ ì „ëµ

### í˜„ì¬ ìŠ¤í‚¤ë§ˆ ë¶„ì„

**í…Œì´ë¸” êµ¬ì¡°** (`scripts/init_db.sql`):

```sql
CREATE TABLE crawl_results (
    id SERIAL PRIMARY KEY,
    url TEXT UNIQUE NOT NULL,
    site_name VARCHAR(100) NOT NULL,
    category VARCHAR(50),           -- âœ… ì´ë¯¸ ì¡´ì¬ (ì¶”ê°€ë¨)
    category_kr VARCHAR(50),         -- âœ… ì´ë¯¸ ì¡´ì¬ (ì¶”ê°€ë¨)
    title TEXT,
    body TEXT,
    date TEXT,
    quality_score INTEGER,
    crawl_mode VARCHAR(20),
    crawl_duration_seconds FLOAT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

**ê¸°ì¡´ ì¸ë±ìŠ¤**:
- `idx_crawl_results_site_name` ON `site_name`
- `idx_crawl_results_quality_score` ON `quality_score`
- `idx_crawl_results_crawl_mode` ON `crawl_mode`

### í™•ì¥ í•„ìš” í•„ë“œ (ì¦ë¶„ ìˆ˜ì§‘ìš©)

#### í•„ìˆ˜ ì¶”ê°€ (High Priority)

```sql
-- Migration: scripts/migrations/001_add_incremental_fields.sql

ALTER TABLE crawl_results
ADD COLUMN crawl_date DATE,           -- í¬ë¡¤ë§ ìˆ˜í–‰ ë‚ ì§œ (2025-11-03)
ADD COLUMN article_date DATE,         -- ê¸°ì‚¬ ë°œí–‰ ë‚ ì§œ (2025-11-02)
ADD COLUMN is_latest BOOLEAN DEFAULT true;  -- ìµœì‹  ë²„ì „ ì—¬ë¶€

-- ì¸ë±ìŠ¤ ì¶”ê°€ (ì„±ëŠ¥ ìµœì í™”)
CREATE INDEX idx_crawl_date ON crawl_results(crawl_date);
CREATE INDEX idx_article_date ON crawl_results(article_date);
CREATE INDEX idx_is_latest ON crawl_results(is_latest);

-- ë³µí•© ì¸ë±ìŠ¤ (ìì£¼ ì‚¬ìš©ë˜ëŠ” ì¿¼ë¦¬ íŒ¨í„´)
CREATE INDEX idx_article_site_date ON crawl_results(site_name, article_date);
```

**í•„ë“œ ì„¤ëª…**:
- `crawl_date`: ì‹¤ì œë¡œ í¬ë¡¤ë§ì„ ìˆ˜í–‰í•œ ë‚ ì§œ (ì‹œìŠ¤í…œ ë‚ ì§œ)
- `article_date`: ê¸°ì‚¬ê°€ ë°œí–‰ëœ ë‚ ì§œ (ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶”ì¶œ)
- `is_latest`: ë™ì¼ URLì˜ ì—¬ëŸ¬ ë²„ì „ ì¤‘ ìµœì‹  ë²„ì „ ì—¬ë¶€

**ì‚¬ìš© ì‚¬ë¡€**:
```sql
-- ì–´ì œ ìˆ˜ì§‘í•œ ëª¨ë“  ê¸°ì‚¬
SELECT * FROM crawl_results WHERE crawl_date = '2025-11-02';

-- ì–´ì œ ë°œí–‰ëœ ëª¨ë“  ê¸°ì‚¬ (ìˆ˜ì§‘ ë‚ ì§œ ë¬´ê´€)
SELECT * FROM crawl_results WHERE article_date = '2025-11-02';

-- ìµœì‹  ë²„ì „ë§Œ ì¡°íšŒ (ì¤‘ë³µ ì œê±°)
SELECT * FROM crawl_results WHERE is_latest = true;
```

#### ì„ íƒ ì¶”ê°€ (Future Extensions - Phase 2)

SNS/ë™ì  ì½˜í…ì¸  í™•ì¥ ëŒ€ë¹„:

```sql
-- Migration: scripts/migrations/002_add_metadata_fields.sql (ë‚˜ì¤‘ì—)

ALTER TABLE crawl_results
ADD COLUMN content_type VARCHAR(50) DEFAULT 'news',  -- 'news', 'sns', 'blog', 'video'
ADD COLUMN metadata JSONB,                            -- ìœ ì—°í•œ ë©”íƒ€ë°ì´í„° ì €ì¥
ADD COLUMN version INTEGER DEFAULT 1,                 -- ë™ì¼ URLì˜ ë²„ì „ ë²ˆí˜¸
ADD COLUMN last_updated TIMESTAMP;                    -- ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°

-- JSONB GIN ì¸ë±ìŠ¤ (ë¹ ë¥¸ JSON ì¿¼ë¦¬)
CREATE INDEX idx_content_type ON crawl_results(content_type);
CREATE INDEX idx_metadata ON crawl_results USING GIN (metadata);
```

**metadata JSONB í™œìš© ì˜ˆ**:
```json
{
  "author": "í™ê¸¸ë™ ê¸°ì",
  "tags": ["ê²½ì œ", "ì£¼ì‹", "ì‚¼ì„±ì „ì"],
  "comments_count": 42,
  "shares": 128,
  "reactions": {"like": 85, "wow": 12}
}
```

### Migration ì „ëµ

#### Option A: ALTER TABLE (ë®ì–´ì“°ê¸°) â­ **ê¶Œì¥**

**ì¥ì **:
- êµ¬í˜„ ê°„ë‹¨ (SQL 1ê°œ íŒŒì¼)
- ê¸°ì¡´ ë°ì´í„° ë³´ì¡´
- Downtime ìµœì†Œ (PostgreSQLì€ ALTER TABLEì´ ë¹ ë¦„)

**ë‹¨ì **:
- ë¡¤ë°± ì–´ë ¤ì›€ (ìˆ˜ë™ DROP COLUMN í•„ìš”)

**ì‹¤í–‰ ë°©ë²•**:
```bash
# 1. Migration íŒŒì¼ ìƒì„±
cat > scripts/migrations/001_add_incremental_fields.sql << 'EOF'
-- Add incremental crawling fields
ALTER TABLE crawl_results
ADD COLUMN crawl_date DATE,
ADD COLUMN article_date DATE,
ADD COLUMN is_latest BOOLEAN DEFAULT true;

CREATE INDEX idx_crawl_date ON crawl_results(crawl_date);
CREATE INDEX idx_article_date ON crawl_results(article_date);
CREATE INDEX idx_is_latest ON crawl_results(is_latest);

-- Backfill existing data (optional)
UPDATE crawl_results
SET crawl_date = created_at::date,
    article_date = created_at::date,
    is_latest = true
WHERE crawl_date IS NULL;
EOF

# 2. ì ìš©
docker exec -i crawlagent-postgres psql -U crawlagent -d crawlagent < scripts/migrations/001_add_incremental_fields.sql

# 3. í™•ì¸
docker exec -it crawlagent-postgres psql -U crawlagent -d crawlagent -c "\d crawl_results"
```

#### Option B: ìƒˆ í…Œì´ë¸” ìƒì„± + ë°ì´í„° ë³µì‚¬

**ì¥ì **:
- ë¡¤ë°± ì‰¬ì›€ (ì›ë³¸ í…Œì´ë¸” ë³´ì¡´)
- Zero-downtime (ìƒˆ í…Œì´ë¸”ë¡œ ì „í™˜)

**ë‹¨ì **:
- ë””ìŠ¤í¬ ê³µê°„ 2ë°° í•„ìš”
- ì½”ë“œ ìˆ˜ì • í•„ìš” (í…Œì´ë¸”ëª… ë³€ê²½)

**ê¶Œì¥í•˜ì§€ ì•ŠìŒ** (í˜„ì¬ ë°ì´í„° ì—†ìœ¼ë¯€ë¡œ ë¶ˆí•„ìš”)

### SQLAlchemy Model ì—…ë°ì´íŠ¸

```python
# src/storage/models.py

from sqlalchemy import Date, Boolean

class CrawlResult(Base):
    __tablename__ = "crawl_results"

    # ... ê¸°ì¡´ í•„ë“œ ...

    # ì¦ë¶„ ìˆ˜ì§‘ í•„ë“œ (ì¶”ê°€)
    crawl_date = Column(Date, nullable=True, index=True, comment="í¬ë¡¤ë§ ìˆ˜í–‰ ë‚ ì§œ")
    article_date = Column(Date, nullable=True, index=True, comment="ê¸°ì‚¬ ë°œí–‰ ë‚ ì§œ")
    is_latest = Column(Boolean, default=True, nullable=False, index=True, comment="ìµœì‹  ë²„ì „ ì—¬ë¶€")

    # ì„ íƒ í•„ë“œ (Phase 2)
    # content_type = Column(String(50), default='news', index=True)
    # metadata = Column(JSONB, nullable=True)
    # version = Column(Integer, default=1)
    # last_updated = Column(TIMESTAMP, nullable=True)
```

### ë§ˆì´ê·¸ë ˆì´ì…˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Migration SQL íŒŒì¼ ì‘ì„± (`scripts/migrations/001_add_incremental_fields.sql`)
- [ ] Migration ì‹¤í–‰ (Docker PostgreSQL)
- [ ] SQLAlchemy Model ì—…ë°ì´íŠ¸ (`src/storage/models.py`)
- [ ] Spider ì½”ë“œ ìˆ˜ì • (ë‚ ì§œ í•„ë“œ ì €ì¥)
- [ ] í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì‹ ê·œ í•„ë“œ ê²€ì¦)

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 30ë¶„

---

## ğŸ“ˆ Gradio UI í™•ì¥ ì œì•ˆ

### í˜„ì¬ UI êµ¬ì¡° (4 Tabs)

1. **Tab 1: ğŸš€ ì‹¤ì‹œê°„ í¬ë¡¤ë§** - ë‹¨ì¼/ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ìˆ˜ì§‘
2. **Tab 2: ğŸ“Š ë°ì´í„° ì¡°íšŒ** - ê²€ìƒ‰, í•„í„°ë§, CSV ë‹¤ìš´ë¡œë“œ
3. **Tab 3: ğŸ§  LangGraph Agent** - UC1/UC2 ì„¤ëª… (ì½ê¸° ì „ìš©)
4. **Tab 4: ğŸ“ˆ í†µê³„** - ì‚¬ì´íŠ¸ë³„ í†µê³„, í’ˆì§ˆ ë¶„í¬

### ì¶”ê°€ ì œì•ˆ Tabs (ì˜µì…˜ Aìš©)

#### Option 1: Tab 5 ì¶”ê°€ - "â° ìŠ¤ì¼€ì¤„ëŸ¬ ì œì–´" (ê¶Œì¥)

**ëª©ì **: APScheduler BackgroundScheduler ì œì–´

**UI êµ¬ì„±**:
```python
with gr.Tab("â° ìŠ¤ì¼€ì¤„ëŸ¬"):
    gr.Markdown("## ì¼ì¼ ìë™ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬")

    # í˜„ì¬ ìƒíƒœ í‘œì‹œ
    scheduler_status = gr.Textbox(
        label="ìƒíƒœ",
        value="ì‹¤í–‰ ì¤‘ âœ…" if scheduler.running else "ì¤‘ì§€ë¨ â¸ï¸",
        interactive=False
    )

    next_run = gr.Textbox(
        label="ë‹¤ìŒ ì‹¤í–‰ ì‹œê°",
        value=str(scheduler.get_next_run_time()),
        interactive=False
    )

    # ì œì–´ ë²„íŠ¼
    with gr.Row():
        start_btn = gr.Button("â–¶ï¸ ì‹œì‘", variant="primary")
        pause_btn = gr.Button("â¸ï¸ ì¼ì‹œì •ì§€", variant="secondary")
        stop_btn = gr.Button("â¹ï¸ ì¤‘ì§€", variant="stop")

    # ìˆ˜ë™ ì‹¤í–‰
    gr.Markdown("### ìˆ˜ë™ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©)")
    test_date = gr.Textbox(label="í…ŒìŠ¤íŠ¸ ë‚ ì§œ", value="2025-11-02")
    run_now_btn = gr.Button("ì¦‰ì‹œ ì‹¤í–‰", variant="secondary")

    result_log = gr.Textbox(label="ì‹¤í–‰ ë¡œê·¸", lines=10, max_lines=20)

    # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
    start_btn.click(fn=start_scheduler, outputs=[scheduler_status, next_run])
    pause_btn.click(fn=pause_scheduler, outputs=[scheduler_status])
    run_now_btn.click(fn=run_manual_crawl, inputs=[test_date], outputs=[result_log])
```

**êµ¬í˜„ ì‹œê°„**: 1-2ì‹œê°„

#### Option 2: Tab 2 í™•ì¥ - "ì¦ë¶„ ìˆ˜ì§‘ í•„í„°" (ìµœì†Œ ë³€ê²½)

ê¸°ì¡´ "ë°ì´í„° ì¡°íšŒ" íƒ­ì— ë‚ ì§œ í•„í„° ì¶”ê°€:

```python
with gr.Tab("ğŸ“Š ë°ì´í„° ì¡°íšŒ"):
    # ê¸°ì¡´ í•„í„° (ì‚¬ì´íŠ¸, ê¸°ê°„, ì ìˆ˜, í‚¤ì›Œë“œ)
    # ...

    # ì‹ ê·œ í•„í„° ì¶”ê°€
    crawl_date_filter = gr.Dropdown(
        label="ğŸ“… í¬ë¡¤ë§ ë‚ ì§œ",
        choices=["ì „ì²´", "ì˜¤ëŠ˜", "ì–´ì œ", "ì§€ë‚œ 7ì¼"],
        value="ì „ì²´"
    )

    article_date_filter = gr.Dropdown(
        label="ğŸ“° ê¸°ì‚¬ ë°œí–‰ì¼",
        choices=["ì „ì²´", "ì˜¤ëŠ˜", "ì–´ì œ", "ì§€ë‚œ 7ì¼"],
        value="ì „ì²´"
    )

    latest_only = gr.Checkbox(label="ìµœì‹  ë²„ì „ë§Œ í‘œì‹œ", value=True)
```

**êµ¬í˜„ ì‹œê°„**: 30ë¶„

### ê¶Œì¥ ìˆœì„œ

1. **Phase 1 (UC2 ê°œë°œ ì¤‘)**: UI í™•ì¥ ë³´ë¥˜
   - UC2 ê°œë°œì— ì§‘ì¤‘
   - ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” CLIë¡œ ìˆ˜ë™ ì‹¤í–‰ (`poetry run python src/scheduler/daily_crawler.py`)

2. **Phase 2 (UC2 ì™„ë£Œ í›„)**: UI í™•ì¥
   - Tab 5 (ìŠ¤ì¼€ì¤„ëŸ¬ ì œì–´) ì¶”ê°€
   - Tab 2 (ì¦ë¶„ í•„í„°) í™•ì¥

---

## ğŸš€ ê°œë°œ ë ˆë”” ìµœì¢… ìƒíƒœ ì ê²€

### âœ… Ready (ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥)

| í•­ëª© | ìƒíƒœ | ê·¼ê±° |
|------|------|------|
| **í™˜ê²½** | âœ… | Docker PostgreSQL 16 ì‹¤í–‰ ì¤‘ (ì»¨í…Œì´ë„ˆ: `crawlagent-postgres`) |
| **ì˜ì¡´ì„±** | âœ… | Poetry 2.2.1, Python 3.9.6, ëª¨ë“  íŒ¨í‚¤ì§€ ì„¤ì¹˜ë¨ |
| **API Keys** | âœ… | `.env` íŒŒì¼ ì¡´ì¬, OpenAI + Google API Key ì„¤ì •ë¨ |
| **Spider** | âœ… | `yonhap.py` êµ¬í˜„ ì™„ë£Œ (2-stage crawling, ì¹´í…Œê³ ë¦¬ ì§€ì›) |
| **UC1 Workflow** | âœ… | `src/workflow/uc1_validation.py` êµ¬í˜„ ì™„ë£Œ (5W1H í’ˆì§ˆ ê²€ì¦) |
| **DB Schema** | âœ… | 3 tables (selectors, crawl_results, decision_logs) + ì¸ë±ìŠ¤ |
| **Gradio UI** | âœ… | 4-Tab êµ¬ì¡° ì™„ì„± (í¬ë¡¤ë§, ì¡°íšŒ, Agent ì„¤ëª…, í†µê³„) |
| **PRD ë¬¸ì„œ** | âœ… | PRD-1/2/3 + UC2 Masterplan + Quick Start ì™„ë¹„ |
| **Claude Skills** | âœ… | 3ê°œ Skills ìƒì„±ë¨ (uc2-development, incremental-crawling, scheduler) |

### âš ï¸ Preparation Needed (1-2ì‹œê°„ ì‘ì—… í•„ìš”)

| í•­ëª© | ì‘ì—… ë‚´ìš© | ì†Œìš” ì‹œê°„ | íŒŒì¼ ê²½ë¡œ |
|------|----------|----------|----------|
| **APScheduler ì„¤ì¹˜** | `poetry add apscheduler` | 2ë¶„ | `pyproject.toml` |
| **DB ìŠ¤í‚¤ë§ˆ í™•ì¥** | Migration ì‹¤í–‰ (crawl_date, article_date, is_latest) | 30ë¶„ | `scripts/migrations/001_add_incremental_fields.sql` |
| **Spider ìˆ˜ì •** | `target_date` íŒŒë¼ë¯¸í„° + ë‚ ì§œ ë¹„êµ ë¡œì§ ì¶”ê°€ | 30ë¶„ | `src/crawlers/spiders/yonhap.py` |
| **Models ì—…ë°ì´íŠ¸** | SQLAlchemy ëª¨ë¸ì— ì‹ ê·œ í•„ë“œ ì¶”ê°€ | 10ë¶„ | `src/storage/models.py` |
| **Scheduler ìƒì„±** | `daily_crawler.py` ì‘ì„± (APScheduler BlockingScheduler) | 20ë¶„ | `src/scheduler/daily_crawler.py` |

**ì´ ì˜ˆìƒ ì†Œìš” ì‹œê°„**: **1.5ì‹œê°„**

### âŒ Not Ready (Blocker - UC2 ê°œë°œ í•„ìš”)

| í•­ëª© | í˜„ì¬ ìƒíƒœ | í•´ê²° ë°©ë²• | ì†Œìš” ì‹œê°„ |
|------|----------|----------|----------|
| **GPT-4o Analyzer** | âŒ ë¯¸êµ¬í˜„ | `src/agents/gpt_analyzer.py` ìƒì„± | 3ì‹œê°„ |
| **Gemini Validator** | âŒ ë¯¸êµ¬í˜„ | `src/agents/gemini_validator.py` ìƒì„± | 2ì‹œê°„ |
| **UC2 Workflow** | âŒ ë¯¸êµ¬í˜„ | `src/workflow/uc2_recovery.py` ìƒì„± (LangGraph) | 3ì‹œê°„ |
| **Consensus Logic** | âŒ ë¯¸êµ¬í˜„ | Conditional routing êµ¬í˜„ | 1ì‹œê°„ |
| **HITL Interface** | âŒ ë¯¸êµ¬í˜„ | Gradio Tab 5 ì¶”ê°€ (ìˆ˜ë™ ê²€í† ) | 1ì‹œê°„ |

**ì´ ì˜ˆìƒ ì†Œìš” ì‹œê°„**: **10ì‹œê°„** (UC2 ì™„ì „ êµ¬í˜„)

---

## ğŸ› ï¸ ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš” í•­ëª© (ìš°ì„ ìˆœìœ„ë³„)

### High Priority (ì˜¤ëŠ˜ ì™„ë£Œ - UC2 ê°œë°œ ì¤€ë¹„)

#### 1. APScheduler ì„¤ì¹˜
**ì‘ì—…**: Poetry íŒ¨í‚¤ì§€ ì¶”ê°€
**ì†Œìš”**: 2ë¶„
**ëª…ë ¹ì–´**:
```bash
cd /Users/charlee/Desktop/Intern/crawlagent
poetry add apscheduler
```

#### 2. DB ìŠ¤í‚¤ë§ˆ í™•ì¥
**ì‘ì—…**: Migration ì‹¤í–‰
**ì†Œìš”**: 30ë¶„
**íŒŒì¼**: `scripts/migrations/001_add_incremental_fields.sql`
**ëª…ë ¹ì–´**:
```bash
# 1. Migration íŒŒì¼ ìƒì„±
cat > scripts/migrations/001_add_incremental_fields.sql << 'EOF'
ALTER TABLE crawl_results
ADD COLUMN crawl_date DATE,
ADD COLUMN article_date DATE,
ADD COLUMN is_latest BOOLEAN DEFAULT true;

CREATE INDEX idx_crawl_date ON crawl_results(crawl_date);
CREATE INDEX idx_article_date ON crawl_results(article_date);
CREATE INDEX idx_is_latest ON crawl_results(is_latest);
EOF

# 2. ì‹¤í–‰
docker exec -i crawlagent-postgres psql -U crawlagent -d crawlagent < scripts/migrations/001_add_incremental_fields.sql

# 3. ê²€ì¦
docker exec -it crawlagent-postgres psql -U crawlagent -d crawlagent -c "\d crawl_results"
```

#### 3. SQLAlchemy Model ì—…ë°ì´íŠ¸
**ì‘ì—…**: `CrawlResult` í´ë˜ìŠ¤ì— í•„ë“œ 3ê°œ ì¶”ê°€
**ì†Œìš”**: 10ë¶„
**íŒŒì¼**: `src/storage/models.py`
**ìˆ˜ì • ë‚´ìš©**:
```python
# Line 110 ì´í›„ ì¶”ê°€
crawl_date = Column(Date, nullable=True, index=True)
article_date = Column(Date, nullable=True, index=True)
is_latest = Column(Boolean, default=True, nullable=False, index=True)
```

#### 4. Spider ìˆ˜ì • (ë‚ ì§œ í•„í„° ì¶”ê°€)
**ì‘ì—…**: `target_date` íŒŒë¼ë¯¸í„° ì¶”ê°€ + ë‚ ì§œ ë¹„êµ ë¡œì§
**ì†Œìš”**: 30ë¶„
**íŒŒì¼**: `src/crawlers/spiders/yonhap.py`
**ìˆ˜ì • ìœ„ì¹˜**:
- `__init__`: Line 46-84 (íŒŒë¼ë¯¸í„° ì¶”ê°€)
- `parse_article`: Line 175-308 (ë‚ ì§œ ë¹„êµ ë¡œì§)

### Medium Priority (ë‚´ì¼ ì™„ë£Œ - í¸ì˜ ê¸°ëŠ¥)

#### 5. Scheduler ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
**ì‘ì—…**: `daily_crawler.py` ì‘ì„±
**ì†Œìš”**: 20ë¶„
**íŒŒì¼**: `src/scheduler/daily_crawler.py` (ì‹ ê·œ ìƒì„±)
**í…œí”Œë¦¿**: `.claude/skills/scheduler.md` ì°¸ì¡°

#### 6. PRD ì—…ë°ì´íŠ¸
**ì‘ì—…**: ì¦ë¶„ ìˆ˜ì§‘ ì„¹ì…˜ ì¶”ê°€
**ì†Œìš”**: 15ë¶„
**íŒŒì¼**: `docs/crawlagent/PRD-3-IMPLEMENTATION.md`
**ì¶”ê°€ ì„¹ì…˜**:
- ì¦ë¶„ ìˆ˜ì§‘ ìš”êµ¬ì‚¬í•­
- ìŠ¤ì¼€ì¤„ë§ ì „ëµ
- DB ìŠ¤í‚¤ë§ˆ ë³€ê²½ì‚¬í•­

### Low Priority (UC2 ì™„ë£Œ í›„)

#### 7. Gradio UI í™•ì¥
**ì‘ì—…**: Tab 5 "ìŠ¤ì¼€ì¤„ëŸ¬ ì œì–´" ì¶”ê°€
**ì†Œìš”**: 1-2ì‹œê°„
**íŒŒì¼**: `src/ui/app.py`

#### 8. ë™ì  ë°ì´í„° ì„¹ì…˜ ì¶”ê°€
**ì‘ì—…**: PRDì— SPA/ë™ì  ì‚¬ì´íŠ¸ ì „ëµ ë¬¸ì„œí™”
**ì†Œìš”**: 30ë¶„
**íŒŒì¼**: `docs/crawlagent/PRD-4-DYNAMIC-SITES.md` (ì‹ ê·œ)

---

## ğŸ“… ê°œë°œ ì‹œì‘ ë¡œë“œë§µ

### Phase 1: ì¤€ë¹„ (ì˜¤ëŠ˜, 3-4ì‹œê°„)

**ëª©í‘œ**: UC2 ê°œë°œ í™˜ê²½ ì™„ì „ ì¤€ë¹„

| ë‹¨ê³„ | ì‘ì—… | ì†Œìš” | ì²´í¬ |
|------|------|------|------|
| 1.1 | APScheduler ì„¤ì¹˜ | 2ë¶„ | â¬œ |
| 1.2 | DB Migration ì‹¤í–‰ | 30ë¶„ | â¬œ |
| 1.3 | Models ì—…ë°ì´íŠ¸ | 10ë¶„ | â¬œ |
| 1.4 | Spider ìˆ˜ì • (ë‚ ì§œ í•„í„°) | 30ë¶„ | â¬œ |
| 1.5 | ì¦ë¶„ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ | 20ë¶„ | â¬œ |
| 1.6 | Scheduler ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± | 20ë¶„ | â¬œ |
| 1.7 | Scheduler í…ŒìŠ¤íŠ¸ | 15ë¶„ | â¬œ |
| 1.8 | PRD ì—…ë°ì´íŠ¸ | 15ë¶„ | â¬œ |

**ì™„ë£Œ ì¡°ê±´**: `target_date` íŒŒë¼ë¯¸í„°ë¡œ íŠ¹ì • ë‚ ì§œ ìˆ˜ì§‘ ì„±ê³µ

**í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´**:
```bash
poetry run scrapy crawl yonhap -a target_date=2025-11-02 -a category=politics
```

### Phase 2: UC2 ê°œë°œ (ë‚´ì¼~, 7-8ì‹œê°„)

**ëª©í‘œ**: 2-Agent Consensus System êµ¬í˜„

| ë‹¨ê³„ | ì‘ì—… | ì†Œìš” | íŒŒì¼ | ì²´í¬ |
|------|------|------|------|------|
| 2.1 | UC2 State ì„¤ê³„ | 1ì‹œê°„ | `src/workflow/uc2_recovery.py` | â¬œ |
| 2.2 | GPT-4o Analyzer | 3ì‹œê°„ | `src/agents/gpt_analyzer.py` | â¬œ |
| 2.3 | Gemini Validator | 2ì‹œê°„ | `src/agents/gemini_validator.py` | â¬œ |
| 2.4 | Consensus Logic | 1ì‹œê°„ | `src/workflow/uc2_recovery.py` | â¬œ |
| 2.5 | DB Integration | 30ë¶„ | `src/workflow/uc2_recovery.py` | â¬œ |
| 2.6 | í†µí•© í…ŒìŠ¤íŠ¸ | 1ì‹œê°„ | `tests/test_uc2.py` | â¬œ |

**ì™„ë£Œ ì¡°ê±´**: ê³ ì˜ë¡œ ì†ìƒëœ Selectorë¥¼ UC2ê°€ ìë™ ë³µêµ¬

**í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤**:
```python
# 1. ì—°í•©ë‰´ìŠ¤ Selector ê³ ì˜ ì†ìƒ
# 2. UC1 ì‹¤í–‰ â†’ quality_score < 80
# 3. UC2 ìë™ ì‹¤í–‰ â†’ ìƒˆ Selector ìƒì„±
# 4. DB ì—…ë°ì´íŠ¸ â†’ ì¬í¬ë¡¤ë§ â†’ quality_score â‰¥ 80
```

### Phase 3: í†µí•© & ë°ëª¨ ì¤€ë¹„ (ëª¨ë ˆ~, 3-4ì‹œê°„)

**ëª©í‘œ**: ì™„ì „ ìë™í™” ì‹œìŠ¤í…œ ê²€ì¦

| ë‹¨ê³„ | ì‘ì—… | ì†Œìš” | íŒŒì¼ | ì²´í¬ |
|------|------|------|------|------|
| 3.1 | Scheduler + UC2 í†µí•© | 1ì‹œê°„ | `src/scheduler/daily_crawler.py` | â¬œ |
| 3.2 | Gradio UI í™•ì¥ | 2ì‹œê°„ | `src/ui/app.py` | â¬œ |
| 3.3 | ì¢…ë‹¨ê°„ í…ŒìŠ¤íŠ¸ | 1ì‹œê°„ | - | â¬œ |
| 3.4 | ë°ëª¨ ì‹œë‚˜ë¦¬ì˜¤ ì‘ì„± | 30ë¶„ | `docs/DEMO-SCRIPT.md` | â¬œ |

**ì™„ë£Œ ì¡°ê±´**: ìŠ¤ì¼€ì¤„ëŸ¬ â†’ í¬ë¡¤ë§ â†’ UC1 â†’ UC2 (í•„ìš” ì‹œ) â†’ ì €ì¥ ì „ì²´ í”Œë¡œìš° ì„±ê³µ

**ë°ëª¨ ì‹œë‚˜ë¦¬ì˜¤**:
1. Gradio ì‹¤í–‰
2. Tab 1: ë‹¨ì¼ ê¸°ì‚¬ ìˆ˜ì§‘ (ì •ìƒ)
3. Tab 1: ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ìˆ˜ì§‘ (ì •ìƒ)
4. Tab 5: ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (ìë™)
5. Selector ì†ìƒ â†’ UC2 ìë™ ë³µêµ¬ (Self-Healing)

---

## ğŸ“ ìµœì¢… ê¶Œì¥ì‚¬í•­ & ì‹¤í–‰ ê³„íš

### í•µì‹¬ ê²°ë¡ 

1. **ê¸°ìˆ  ìŠ¤íƒ**: ê²€ì¦ ì™„ë£Œ âœ…
   - ëª¨ë“  í•„ìˆ˜ ê¸°ìˆ  ì„¤ì¹˜ë¨
   - APSchedulerë§Œ ì¶”ê°€ ì„¤ì¹˜ í•„ìš” (2ë¶„)

2. **Claude Skills**: ìƒì„± ì™„ë£Œ âœ…
   - 3ê°œ Skillsë¡œ ê°œë°œ ê°€ì´ë“œ ì œê³µ
   - MCP ì„œë²„ëŠ” í˜„ì¬ ë¶ˆí•„ìš”

3. **ìŠ¤ì¼€ì¤„ëŸ¬**: APScheduler ì„ íƒ âœ…
   - PoC ë‹¨ê³„ì— ìµœì 
   - Gradio UI í†µí•© ì‰¬ì›€
   - í”„ë¡œë•ì…˜ ì „í™˜ ì‹œ Cron ê³ ë ¤

4. **DB í™•ì¥**: ì„¤ê³„ ì™„ë£Œ âœ…
   - 3ê°œ í•„ë“œ ì¶”ê°€ (crawl_date, article_date, is_latest)
   - Migration SQL ì¤€ë¹„ë¨
   - 30ë¶„ ë‚´ ì ìš© ê°€ëŠ¥

5. **ê°œë°œ ì¤€ë¹„ë„**: 95% âœ…
   - í™˜ê²½/ì˜ì¡´ì„±/Spider/UC1 ì™„ë£Œ
   - 1.5ì‹œê°„ ì¤€ë¹„ ì‘ì—… í›„ UC2 ê°œë°œ ì‹œì‘ ê°€ëŠ¥

### ì¦‰ì‹œ ì‹œì‘ ê°€ëŠ¥í•œ ì‘ì—… ìˆœì„œ

#### Step 1: í™˜ê²½ ì¤€ë¹„ (15ë¶„)
```bash
# 1. APScheduler ì„¤ì¹˜
cd /Users/charlee/Desktop/Intern/crawlagent
poetry add apscheduler

# 2. Migration ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p scripts/migrations

# 3. Git ìƒíƒœ í™•ì¸ (ë³€ê²½ì‚¬í•­ ì»¤ë°‹ ì „)
git status
```

#### Step 2: DB ë§ˆì´ê·¸ë ˆì´ì…˜ (30ë¶„)
```bash
# 1. Migration íŒŒì¼ ìƒì„± (ìœ„ì˜ High Priority 2ë²ˆ ì°¸ì¡°)
# 2. Migration ì‹¤í–‰
# 3. ê²€ì¦ (í…Œì´ë¸” êµ¬ì¡° í™•ì¸)
```

#### Step 3: ì½”ë“œ ìˆ˜ì • (40ë¶„)
- `src/storage/models.py`: í•„ë“œ 3ê°œ ì¶”ê°€
- `src/crawlers/spiders/yonhap.py`: target_date ë¡œì§ ì¶”ê°€

#### Step 4: í…ŒìŠ¤íŠ¸ (20ë¶„)
```bash
# íŠ¹ì • ë‚ ì§œ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸
poetry run scrapy crawl yonhap -a target_date=2025-11-02 -a category=politics

# DB í™•ì¸
docker exec -it crawlagent-postgres psql -U crawlagent -d crawlagent -c "SELECT crawl_date, article_date, title FROM crawl_results ORDER BY created_at DESC LIMIT 5;"
```

#### Step 5: Scheduler êµ¬í˜„ (20ë¶„)
- `src/scheduler/daily_crawler.py` ìƒì„± (`.claude/skills/scheduler.md` í…œí”Œë¦¿ ì‚¬ìš©)

#### Step 6: UC2 ê°œë°œ ì‹œì‘ (7-8ì‹œê°„)
- `.claude/skills/uc2-development.md` ì°¸ì¡°
- `docs/crawlagent/UC2-DEVELOPMENT-MASTERPLAN.md` ë”°ë¼ êµ¬í˜„

### ì„±ê³µ ì§€í‘œ

| ë‹¨ê³„ | ì„±ê³µ ì¡°ê±´ | ê²€ì¦ ë°©ë²• |
|------|----------|----------|
| **Phase 1 ì™„ë£Œ** | íŠ¹ì • ë‚ ì§œ ìˆ˜ì§‘ ì„±ê³µ | `target_date=2025-11-02` í¬ë¡¤ë§ â†’ DB ì €ì¥ í™•ì¸ |
| **Phase 2 ì™„ë£Œ** | UC2 ìë™ ë³µêµ¬ ì„±ê³µ | Selector ì†ìƒ â†’ UC2 ì‹¤í–‰ â†’ ìƒˆ Selector ìƒì„± â†’ ì¬í¬ë¡¤ë§ ì„±ê³µ |
| **Phase 3 ì™„ë£Œ** | ì™„ì „ ìë™í™” ê²€ì¦ | ìŠ¤ì¼€ì¤„ëŸ¬ â†’ í¬ë¡¤ë§ â†’ UC1/UC2 â†’ ì €ì¥ ì „ì²´ í”Œë¡œìš° 0-error |

### ë¦¬ìŠ¤í¬ & ì™„í™” ì „ëµ

| ë¦¬ìŠ¤í¬ | í™•ë¥  | ì˜í–¥ | ì™„í™” ë°©ë²• |
|--------|------|------|-----------|
| GPT-4o Selector ìƒì„± ì‹¤íŒ¨ | ë‚®ìŒ | ë†’ìŒ | ì¬ì‹œë„ ë¡œì§ (max 3íšŒ) + HITL ê°œì… |
| Gemini ê²€ì¦ False Negative | ì¤‘ê°„ | ì¤‘ê°„ | ê·œì¹™ ê¸°ë°˜ ê²€ì¦ ì¶”ê°€ (80% + íŒ¨í„´ ì²´í¬) |
| Migration ì‹¤íŒ¨ | ë‚®ìŒ | ë†’ìŒ | í…ŒìŠ¤íŠ¸ DBì—ì„œ ë¨¼ì € ì‹¤í–‰ + ë°±ì—… |
| ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜ | ì¤‘ê°„ | ë‚®ìŒ | try/except + ë¡œê¹…, ì‹¤íŒ¨ ì‹œ ì „ì²´ ìˆ˜ì§‘ |

### ë‹¤ìŒ ë‹¨ê³„

**ì§€ê¸ˆ ë°”ë¡œ ì‹œì‘**:
```bash
# í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰
cd /Users/charlee/Desktop/Intern/crawlagent
poetry add apscheduler

# ê·¸ ë‹¤ìŒ, ìœ„ì˜ Step 2 (DB Migration) ì§„í–‰
```

**ì§ˆë¬¸ì´ ìˆë‹¤ë©´**:
- `.claude/skills/` ë””ë ‰í† ë¦¬ì˜ 3ê°œ Skills ì°¸ì¡°
- `docs/crawlagent/UC2-DEVELOPMENT-MASTERPLAN.md` ìƒì„¸ ê°€ì´ë“œ
- PRD-2-TECHNICAL-SPEC.md (Lines 121-151) UC2 ìŠ¤í™

---

## ğŸ“ ì°¸ê³  ìë£Œ

### í”„ë¡œì íŠ¸ ë¬¸ì„œ
- **PRD-1**: ë¬¸ì œ ì •ì˜ & ì†”ë£¨ì…˜
- **PRD-2**: ê¸°ìˆ  ìŠ¤í™ (Lines 121-151: UC2 ì›Œí¬í”Œë¡œìš°)
- **PRD-3**: êµ¬í˜„ ê³„íš
- **UC2 Masterplan**: ì™„ì „í•œ ê°œë°œ ê°€ì´ë“œ (HITL í¬ì¸íŠ¸ í¬í•¨)
- **UC2 Quick Start**: ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

### Claude Skills (ë°©ê¸ˆ ìƒì„±ë¨)
- **uc2-development.md**: UC2 ê°œë°œ íŒ¨í„´ & ì˜ˆì œ
- **incremental-crawling.md**: ì¦ë¶„ ìˆ˜ì§‘ êµ¬í˜„ ê°€ì´ë“œ
- **scheduler.md**: ìŠ¤ì¼€ì¤„ëŸ¬ ë¹„êµ & êµ¬í˜„

### ì™¸ë¶€ ë ˆí¼ëŸ°ìŠ¤
- APScheduler: https://apscheduler.readthedocs.io/
- LangGraph: https://langchain-ai.github.io/langgraph/
- OpenAI Structured Outputs: https://platform.openai.com/docs/guides/structured-outputs
- PostgreSQL Date/Time: https://www.postgresql.org/docs/16/datatype-datetime.html

---

**ë³´ê³ ì„œ ì¢…ë£Œ**

**ì‘ì„±ì**: Claude (Anthropic)
**ì‘ì„±ì¼**: 2025-11-03
**ë²„ì „**: 1.0
**ë‹¤ìŒ ë‹¨ê³„**: Phase 1 ì¤€ë¹„ ì‘ì—… ì‹œì‘ (1.5ì‹œê°„)
