# CrawlAgent PoC - PRD Part 3: Development Roadmap

**ì‘ì„±ì¼**: 2025-10-28
**ë²„ì „**: 1.0 (PostgreSQL ê¸°ë°˜)
**ìƒíƒœ**: ì´í•´ê´€ê³„ì ê²€í†  ëŒ€ê¸°
**ì´ ê¸°ê°„**: 10ì¼ (2ì£¼)

---

## ğŸ“… ê°œë°œ ì¼ì • (Development Schedule)

### Week 1: Infrastructure & UC1 (Day 1-5)

| Day | Phase | ì‘ì—… ë‚´ìš© | ì‚°ì¶œë¬¼ | ì†Œìš” ì‹œê°„ |
|-----|-------|----------|--------|----------|
| **1** | Phase 0 | í™˜ê²½ ì„¤ì • | PostgreSQL (Docker), Python 3.11 venv, ì˜ì¡´ì„± ì„¤ì¹˜ | 4h |
| **2** | Phase 1 | PostgreSQL ìŠ¤í‚¤ë§ˆ | 3ê°œ í…Œì´ë¸” ìƒì„±, SQLAlchemy ëª¨ë¸ ì •ì˜ | 6h |
| **3** | Phase 2.1 | Scrapy ì´ˆê¸°í™” | 3ê°œ Spider ê³¨ê²© (ì—°í•©ë‰´ìŠ¤, ë„¤ì´ë²„, BBC) | 6h |
| **4** | Phase 2.2 | Scrapy êµ¬í˜„ | SSR Spider ì™„ì„± (ì—°í•©ë‰´ìŠ¤, ë„¤ì´ë²„) | 6h |
| **5** | Phase 2.3 | scrapy-playwright | BBC News SPA Spider ì™„ì„±, UC1 ê²€ì¦ | 6h |

**Week 1 ëª©í‘œ**: UC1 (ì •ìƒ í¬ë¡¤ë§) ì‘ë™ í™•ì¸ - 3-Site ê° 5ê°œ ê¸°ì‚¬ ìˆ˜ì§‘

---

### Week 2: 2-Agent System & Integration (Day 6-10)

| Day | Phase | ì‘ì—… ë‚´ìš© | ì‚°ì¶œë¬¼ | ì†Œìš” ì‹œê°„ |
|-----|-------|----------|--------|----------|
| **6** | Phase 3 | LangGraph Workflow | UC1 ê²½ë¡œ êµ¬í˜„, Scrapy ì‹¤íŒ¨ ê°ì§€ ë¡œì§ | 6h |
| **7** | Phase 4.1 | GPT Analyzer | GPT-4o Structured Output, CSS Selector ìƒì„± | 6h |
| **8** | Phase 4.2 | Gemini Validator | Gemini 2.5 ë…ë¦½ ê²€ì¦, í•©ì˜ ì²´í¬ ë¡œì§ | 6h |
| **9** | Phase 5 | í†µí•© í…ŒìŠ¤íŠ¸ | UC1/UC2/UC3 ê° 10íšŒ í…ŒìŠ¤íŠ¸, í’ˆì§ˆ ê²€ì¦ | 6h |
| **10** | Phase 6 | ë¬¸ì„œí™” & ë°œí‘œ | README, ë°œí‘œ ìë£Œ, ë°ëª¨ ì¤€ë¹„ | 6h |

**Week 2 ëª©í‘œ**: UC2/UC3 ì‘ë™ í™•ì¸ - 30ê°œ ê¸°ì‚¬ â‰¥90% í’ˆì§ˆ ë‹¬ì„±

---

## âœ… Phase 0: í™˜ê²½ ì„¤ì • (Day 1)

### Task 0.1: Docker Composeë¡œ PostgreSQL ì‹œì‘

- [x] **0.1.1**: `docker-compose.yml` íŒŒì¼ í™•ì¸
- [x] **0.1.2**: PostgreSQL ì‹œì‘
  ```bash
  cd newsflow-poc
  docker-compose up -d
  ```
- [x] **0.1.3**: ì—°ê²° í…ŒìŠ¤íŠ¸
  ```bash
  psql -h localhost -U newsflow -d newsflow_poc -c "SELECT version();"
  ```

**ì˜ˆìƒ ì‹œê°„**: 30ë¶„
**ì™„ë£Œ ê¸°ì¤€**: PostgreSQL 16 ë²„ì „ ì¶œë ¥ í™•ì¸

---

### Task 0.2: Python í™˜ê²½ ì„¤ì •

- [x] **0.2.1**: Python 3.11 venv ìƒì„±
  ```bash
  python -m venv .venv
  .venv\Scripts\activate  # Windows
  ```
- [x] **0.2.2**: ì˜ì¡´ì„± ì„¤ì¹˜
  ```bash
  pip install -e .  # pyproject.toml ê¸°ë°˜ ìë™ ì„¤ì¹˜
  ```

**ì˜ˆìƒ ì‹œê°„**: 15ë¶„
**ì™„ë£Œ ê¸°ì¤€**: `scrapy version` ëª…ë ¹ ì„±ê³µ

---

### Task 0.3: í™˜ê²½ë³€ìˆ˜ ì„¤ì •

- [x] **0.3.1**: `.env` íŒŒì¼ ìƒì„±
  ```bash
  OPENAI_API_KEY=sk-...
  GOOGLE_API_KEY=...
  DATABASE_URL=postgresql://newsflow:dev_password@localhost:5432/newsflow_poc
  LOG_LEVEL=INFO
  ```

**ì˜ˆìƒ ì‹œê°„**: 10ë¶„
**ì™„ë£Œ ê¸°ì¤€**: API í‚¤ ì…ë ¥ ì™„ë£Œ

---

## âœ… Phase 1: PostgreSQL ìŠ¤í‚¤ë§ˆ (Day 2)

### Task 1.1: ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ìƒì„±

- [x] **1.1.1**: `scripts/init_db.sql` ì‘ì„±
  ```sql
  CREATE TABLE IF NOT EXISTS selectors (
      id SERIAL PRIMARY KEY,
      site_name VARCHAR(100) UNIQUE NOT NULL,
      title_selector TEXT NOT NULL,
      body_selector TEXT NOT NULL,
      date_selector TEXT NOT NULL,
      site_type VARCHAR(20) DEFAULT 'ssr',
      created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
      updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
      success_count INTEGER DEFAULT 0,
      failure_count INTEGER DEFAULT 0
  );

  CREATE TABLE IF NOT EXISTS crawl_results (
      id SERIAL PRIMARY KEY,
      url TEXT UNIQUE NOT NULL,
      site_name VARCHAR(100) NOT NULL,
      title TEXT,
      body TEXT,
      date TEXT,
      quality_score INTEGER,
      crawl_mode VARCHAR(20),
      crawl_duration_seconds FLOAT,
      created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
  );

  CREATE TABLE IF NOT EXISTS decision_logs (
      id SERIAL PRIMARY KEY,
      url TEXT NOT NULL,
      site_name VARCHAR(100) NOT NULL,
      gpt_analysis JSONB,
      gemini_validation JSONB,
      consensus_reached BOOLEAN,
      retry_count INTEGER DEFAULT 0,
      created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
  );

  CREATE INDEX idx_selectors_site_name ON selectors(site_name);
  CREATE INDEX idx_crawl_results_site_name ON crawl_results(site_name);
  CREATE INDEX idx_decision_logs_url ON decision_logs(url);
  ```
- [x] **1.1.2**: ìŠ¤í‚¤ë§ˆ ì‹¤í–‰
  ```bash
  docker exec -i newsflow-postgres psql -U newsflow -d newsflow_poc < scripts/init_db.sql
  ```

**ì˜ˆìƒ ì‹œê°„**: 1ì‹œê°„
**ì™„ë£Œ ê¸°ì¤€**: `\dt` ëª…ë ¹ìœ¼ë¡œ 3ê°œ í…Œì´ë¸” í™•ì¸

---

### Task 1.2: SQLAlchemy ORM ëª¨ë¸

- [x] **1.2.1**: `src/storage/models.py` ì‘ì„±
  ```python
  from sqlalchemy import Column, Integer, String, Text, Boolean, Float, TIMESTAMP, JSON
  from sqlalchemy.ext.declarative import declarative_base

  Base = declarative_base()

  class Selector(Base):
      __tablename__ = 'selectors'
      id = Column(Integer, primary_key=True)
      site_name = Column(String(100), unique=True, nullable=False)
      title_selector = Column(Text, nullable=False)
      body_selector = Column(Text, nullable=False)
      date_selector = Column(Text, nullable=False)
      site_type = Column(String(20), default='ssr')
      success_count = Column(Integer, default=0)
      failure_count = Column(Integer, default=0)

  class CrawlResult(Base):
      __tablename__ = 'crawl_results'
      id = Column(Integer, primary_key=True)
      url = Column(Text, unique=True, nullable=False)
      site_name = Column(String(100), nullable=False)
      title = Column(Text)
      body = Column(Text)
      date = Column(Text)
      quality_score = Column(Integer)
      crawl_mode = Column(String(20))
  ```

**ì˜ˆìƒ ì‹œê°„**: 1ì‹œê°„
**ì™„ë£Œ ê¸°ì¤€**: `python -c "from src.storage.models import Base; print(Base)"` ì„±ê³µ

---

### Task 1.3: ì´ˆê¸° Selector ë°ì´í„° ì‚½ì…

- [x] **1.3.1**: ì—°í•©ë‰´ìŠ¤ Selector ì‚½ì…
  ```sql
  INSERT INTO selectors (site_name, title_selector, body_selector, date_selector, site_type)
  VALUES ('yonhap', 'article h1.tit', 'article div.article-txt', 'article time', 'ssr');
  ```
- [x] **1.3.2**: ë„¤ì´ë²„ ê²½ì œ Selector (init_db.sqlì— í¬í•¨ë¨)
- [x] **1.3.3**: BBC News Selector (init_db.sqlì— í¬í•¨ë¨)

**ì˜ˆìƒ ì‹œê°„**: 2ì‹œê°„
**ì™„ë£Œ ê¸°ì¤€**: `SELECT * FROM selectors;` 3ê°œ ì‚¬ì´íŠ¸ í™•ì¸

---

## âœ… Phase 2: Scrapy êµ¬í˜„ (Day 3-5)

### Task 2.1: Scrapy í”„ë¡œì íŠ¸ ì´ˆê¸°í™” (Day 3)

- [x] **2.1.1**: Scrapy í”„ë¡œì íŠ¸ ìƒì„±
  ```bash
  # src/crawlers/ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
  # scrapy.cfg, settings.py ì‘ì„± ì™„ë£Œ
  ```
- [x] **2.1.2**: Spider 3ê°œ ìƒì„±
  ```bash
  # yonhap.py, naver.py, bbc.py ê³¨ê²© ì‘ì„± ì™„ë£Œ
  scrapy list  # â†’ bbc, naver, yonhap
  ```

**ì˜ˆìƒ ì‹œê°„**: 1ì‹œê°„

---

### Task 2.2: SSR Spider êµ¬í˜„ (Day 4) âœ…

- [x] **2.2.1**: `yonhap_spider.py` ì‘ì„±
  - PostgreSQLì—ì„œ Selector ë™ì  ë¡œë“œ
  - CSS Selectorë¡œ title, body, date ì¶”ì¶œ
  - PostgreSQL `crawl_results` í…Œì´ë¸”ì— ì €ì¥
  - ì‹¤ì œ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì„±ê³µ (2ê°œ ê¸°ì‚¬)

- [x] **2.2.2**: HTML êµ¬ì¡° ë¶„ì„
  - ì—°í•©ë‰´ìŠ¤: `h1.tit01`, `article.article-wrap01`, `meta[property="article:published_time"]`
  - ë„¤ì´ë²„: `meta[property="og:title"]`, `article.go_trans._article_content`
  - PostgreSQL selectors í…Œì´ë¸” ì—…ë°ì´íŠ¸

- [x] **2.2.3**: ìˆ˜ë™ í…ŒìŠ¤íŠ¸ ì„±ê³µ
  ```bash
  scrapy crawl yonhap -a url="https://www.yna.co.kr/view/AKR20251028095752073"
  # [SUCCESS] Saved to PostgreSQL: Æ®ï¿½ï¿½ï¿½ï¿½, ï¿½Ïºï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½...
  ```

**ì‹¤ì œ ì†Œìš” ì‹œê°„**: 4ì‹œê°„
**ì™„ë£Œ ê¸°ì¤€**: âœ… ì—°í•©ë‰´ìŠ¤ ê¸°ì‚¬ 2ê°œ PostgreSQL ì €ì¥ ì„±ê³µ

---

### Task 2.3: ë„¤ì´ë²„ + BBC Spider êµ¬í˜„ (Day 5) âœ… ë‹¨ìˆœí™”!

- [ ] **2.3.1**: `naver.py` Spider ì‘ì„±
  - yonhap Spider íŒ¨í„´ ë³µì‚¬ (SSR)
  - site_name="naver_economy"ë¡œ ë³€ê²½
  - PostgreSQLì—ì„œ Selector ë¡œë“œ

- [ ] **2.3.2**: `bbc.py` Spider ì‘ì„±
  - **BBC News SSR í™•ì¸!** (2025-10-29 ê²€ì¦)
  - yonhap Spider íŒ¨í„´ ë³µì‚¬ (SSR)
  - site_name="bbc"ë¡œ ë³€ê²½

- [ ] **2.3.3**: í…ŒìŠ¤íŠ¸
  - ë„¤ì´ë²„ ê¸°ì‚¬ 1ê°œ í¬ë¡¤ë§ ì„±ê³µ
  - BBC ê¸°ì‚¬ 1ê°œ í¬ë¡¤ë§ ì„±ê³µ

**ì˜ˆìƒ ì‹œê°„**: 2ì‹œê°„ (ì›ë˜ 6ì‹œê°„ â†’ **4ì‹œê°„ ë‹¨ì¶•!**)
**ì™„ë£Œ ê¸°ì¤€**: ë„¤ì´ë²„ + BBC ê° 1ê°œ ê¸°ì‚¬ PostgreSQL ì €ì¥
**ë³€ê²½ ì‚¬í•­**: scrapy-playwright ì œê±° (BBCë„ SSR, ë¶ˆí•„ìš”)

---

## âœ… Phase 3: LangGraph Workflow (Day 6)

### Task 3.1: LangGraph State ì •ì˜

- [ ] **3.1.1**: `src/workflow/state.py` ì‘ì„±
  ```python
  from typing import TypedDict, Literal, Optional

  class CrawlAgentState(TypedDict):
      url: str
      site_name: Literal["yonhap", "naver_economy", "bbc"]
      scrapy_success: bool
      scrapy_data: Optional[dict]
      gpt_selectors: Optional[dict]
      gemini_valid: bool
      final_data: Optional[dict]
      quality_score: int
  ```

**ì˜ˆìƒ ì‹œê°„**: 30ë¶„

---

### Task 3.2: LangGraph ë…¸ë“œ êµ¬í˜„

- [ ] **3.2.1**: `src/workflow/nodes.py` ì‘ì„±
  ```python
  def load_selector(state: CrawlAgentState) -> CrawlAgentState:
      # PostgreSQLì—ì„œ Selector ì¡°íšŒ
      pass

  def run_scrapy(state: CrawlAgentState) -> CrawlAgentState:
      # Scrapy Spider ì‹¤í–‰
      pass

  def check_scrapy_success(state: CrawlAgentState) -> CrawlAgentState:
      # title, body ê²€ì¦
      pass
  ```

**ì˜ˆìƒ ì‹œê°„**: 2ì‹œê°„

---

### Task 3.3: ì¡°ê±´ë¶€ ë¼ìš°íŒ…

- [ ] **3.3.1**: `src/workflow/routing.py` ì‘ì„±
  ```python
  def route_after_scrapy(state: CrawlAgentState) -> str:
      if state["scrapy_success"]:
          return "save_result"
      else:
          return "activate_2_agent"
  ```

**ì˜ˆìƒ ì‹œê°„**: 1ì‹œê°„

---

### Task 3.4: LangGraph ë¹Œë“œ

- [ ] **3.4.1**: `src/workflow/graph.py` ì‘ì„±
  ```python
  from langgraph.graph import StateGraph, END

  def build_newsflow_graph():
      workflow = StateGraph(CrawlAgentState)
      workflow.add_node("load_selector", load_selector)
      workflow.add_node("run_scrapy", run_scrapy)
      workflow.add_conditional_edges(
          "run_scrapy",
          route_after_scrapy,
          {"save_result": END, "activate_2_agent": "gpt_analyze"}
      )
      return workflow.compile()
  ```

**ì˜ˆìƒ ì‹œê°„**: 2ì‹œê°„
**ì™„ë£Œ ê¸°ì¤€**: UC1 ê²½ë¡œ (Scrapy ì„±ê³µ) ì‘ë™ í™•ì¸

---

## âœ… Phase 4: 2-Agent System (Day 7-8)

### Task 4.1: GPT-4o Analyzer (Day 7)

- [ ] **4.1.1**: `src/utils/prompts.py` ì‘ì„±
  ```python
  GPT_SYSTEM_PROMPT = """
  ë‹¹ì‹ ì€ HTML êµ¬ì¡° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
  ì£¼ì–´ì§„ HTMLì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ title, body, dateë¥¼ ì¶”ì¶œí•  CSS Selectorë¥¼ ìƒì„±í•˜ì„¸ìš”.

  **ì¶œë ¥ í˜•ì‹** (JSON):
  {
    "title_selector": "CSS Selector",
    "body_selector": "CSS Selector",
    "date_selector": "CSS Selector",
    "confidence": 0.85
  }
  """
  ```
- [ ] **4.1.2**: `src/agents/gpt_analyzer.py` ì‘ì„±
  ```python
  from openai import OpenAI

  def analyze_html(html: str) -> dict:
      client = OpenAI()
      response = client.chat.completions.create(
          model="gpt-4o-2024-08-06",
          messages=[
              {"role": "system", "content": GPT_SYSTEM_PROMPT},
              {"role": "user", "content": html}
          ],
          response_format={"type": "json_object"}
      )
      return response.choices[0].message.content
  ```

**ì˜ˆìƒ ì‹œê°„**: 3ì‹œê°„
**ì™„ë£Œ ê¸°ì¤€**: í…ŒìŠ¤íŠ¸ HTMLë¡œ Selector ìƒì„± í™•ì¸

---

### Task 4.2: Gemini 2.5 Validator (Day 8)

- [ ] **4.2.1**: `src/agents/gemini_validator.py` ì‘ì„±
  ```python
  import google.generativeai as genai

  def validate_selectors(html: str, selectors: dict) -> dict:
      model = genai.GenerativeModel('gemini-2.0-flash-exp')
      prompt = f"ë‹¤ìŒ CSS Selectorê°€ ì˜¬ë°”ë¥¸ì§€ 10ê°œ ìƒ˜í”Œì„ ì¶”ì¶œí•˜ì—¬ ê²€ì¦í•˜ì„¸ìš”: {selectors}"
      response = model.generate_content([html, prompt])
      return {"valid": True, "samples": [...]}
  ```

**ì˜ˆìƒ ì‹œê°„**: 3ì‹œê°„
**ì™„ë£Œ ê¸°ì¤€**: GPT Selector ê²€ì¦ ì„±ê³µ

---

### Task 4.3: í•©ì˜ ë¡œì§ í†µí•©

- [ ] **4.3.1**: `src/workflow/nodes.py`ì— 2-Agent ë…¸ë“œ ì¶”ê°€
  ```python
  def gpt_analyze_node(state: CrawlAgentState) -> CrawlAgentState:
      # GPT ë¶„ì„
      pass

  def gemini_validate_node(state: CrawlAgentState) -> CrawlAgentState:
      # Gemini ê²€ì¦
      pass

  def check_consensus_node(state: CrawlAgentState) -> CrawlAgentState:
      # í•©ì˜ ì²´í¬
      pass
  ```

**ì˜ˆìƒ ì‹œê°„**: 2ì‹œê°„
**ì™„ë£Œ ê¸°ì¤€**: UC2 ê²½ë¡œ (2-Agent ë³µêµ¬) ì‘ë™ í™•ì¸

---

## âœ… Phase 5: í†µí•© í…ŒìŠ¤íŠ¸ (Day 9)

### Task 5.1: 3-Site í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸

- [ ] **5.1.1**: ì—°í•©ë‰´ìŠ¤ 10ê°œ URL ìˆ˜ì§‘
- [ ] **5.1.2**: LangGraph ì‹¤í–‰
  ```bash
  python src/main.py --site yonhap --urls urls_yonhap.txt
  ```
- [ ] **5.1.3**: í’ˆì§ˆ ì ìˆ˜ í™•ì¸
  ```sql
  SELECT site_name, AVG(quality_score) FROM crawl_results GROUP BY site_name;
  ```

**ëª©í‘œ**: ì—°í•©ë‰´ìŠ¤ 9/10 ì´ìƒ â‰¥80ì 

- [ ] **5.1.4**: ë„¤ì´ë²„ ê²½ì œ 10ê°œ í…ŒìŠ¤íŠ¸
- [ ] **5.1.5**: BBC News 10ê°œ í…ŒìŠ¤íŠ¸

**ì˜ˆìƒ ì‹œê°„**: 4ì‹œê°„
**ì™„ë£Œ ê¸°ì¤€**: 30ê°œ ì¤‘ 27ê°œ ì´ìƒ â‰¥80ì 

---

### Task 5.2: UC2 ì‹œì—°

- [ ] **5.2.1**: ì—°í•©ë‰´ìŠ¤ Selector ì˜ë„ì ìœ¼ë¡œ ë³€ê²½ (ì˜ëª»ëœ Selector ì…ë ¥)
- [ ] **5.2.2**: Scrapy ì‹¤íŒ¨ ê°ì§€ í™•ì¸
- [ ] **5.2.3**: 2-Agent í™œì„±í™” í™•ì¸
- [ ] **5.2.4**: ìƒˆ Selector ìƒì„± ë° ì¬í¬ë¡¤ë§ ì„±ê³µ í™•ì¸

**ì˜ˆìƒ ì‹œê°„**: 1ì‹œê°„

---

### Task 5.3: Decision Log ê²€ì¦

- [ ] **5.3.1**: PostgreSQL ì¿¼ë¦¬
  ```sql
  SELECT * FROM decision_logs WHERE consensus_reached = true LIMIT 5;
  ```
- [ ] **5.3.2**: JSONB ë°ì´í„° í™•ì¸ (GPT reasoning, Gemini samples)

**ì˜ˆìƒ ì‹œê°„**: 30ë¶„

---

## âœ… Phase 6: ë¬¸ì„œí™” & ë°œí‘œ (Day 10)

### Task 6.1: README ì—…ë°ì´íŠ¸

- [ ] **6.1.1**: ì„¤ì¹˜ ê°€ì´ë“œ ì‘ì„±
- [ ] **6.1.2**: ì‹¤í–‰ ë°©ë²• ì‘ì„±
- [ ] **6.1.3**: ì˜ˆì‹œ ì¶œë ¥ ìŠ¤í¬ë¦°ìƒ· ì¶”ê°€

**ì˜ˆìƒ ì‹œê°„**: 1ì‹œê°„

---

### Task 6.2: ë°œí‘œ ìë£Œ ì‘ì„±

- [ ] **6.2.1**: ìŠ¬ë¼ì´ë“œ ì‘ì„± (ë¬¸ì œ/ì†”ë£¨ì…˜/ê²°ê³¼)
- [ ] **6.2.2**: ë°ëª¨ ì‹œë‚˜ë¦¬ì˜¤ ì¤€ë¹„ (UC1/UC2/UC3)
- [ ] **6.2.3**: í’ˆì§ˆ í†µê³„ ì •ë¦¬
  - ì´ 30ê°œ ê¸°ì‚¬ ìˆ˜ì§‘
  - â‰¥80ì  ë‹¬ì„±: 28ê°œ (93%)
  - í‰ê·  í’ˆì§ˆ: 87ì 
  - 2-Agent í™œì„±í™”: 3íšŒ (UC2 2íšŒ, UC3 1íšŒ)

**ì˜ˆìƒ ì‹œê°„**: 2ì‹œê°„

---

### Task 6.3: ì½”ë“œ ì •ë¦¬

- [ ] **6.3.1**: ë¯¸ì‚¬ìš© import ì œê±°
- [ ] **6.3.2**: Type hints ì¶”ê°€
- [ ] **6.3.3**: Docstrings ì‘ì„±

**ì˜ˆìƒ ì‹œê°„**: 1ì‹œê°„

---

## ğŸ“Š ì§„í–‰ í˜„í™© ëŒ€ì‹œë³´ë“œ

### Week 1 ì²´í¬í¬ì¸íŠ¸ (Day 5 ì¢…ë£Œ ì‹œ)

- [ ] PostgreSQL 3ê°œ í…Œì´ë¸” ìƒì„± ì™„ë£Œ
- [ ] Scrapy 3ê°œ Spider ì‘ë™ (ì—°í•©ë‰´ìŠ¤, ë„¤ì´ë²„, BBC)
- [ ] UC1 ê²½ë¡œ ì‘ë™ (ê° ì‚¬ì´íŠ¸ 5ê°œ ê¸°ì‚¬ ìˆ˜ì§‘)
- [ ] í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ë¡œì§ ì‘ë™

---

### Week 2 ì²´í¬í¬ì¸íŠ¸ (Day 10 ì¢…ë£Œ ì‹œ)

- [ ] LangGraph Workflow ì™„ì„± (UC1/UC2/UC3)
- [ ] 2-Agent ì‹œìŠ¤í…œ ì‘ë™ (GPT + Gemini)
- [ ] 30ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ (â‰¥90% í’ˆì§ˆ)
- [ ] Decision Log PostgreSQL ì €ì¥ í™•ì¸
- [ ] ë°œí‘œ ìë£Œ ì™„ì„±

---

## ğŸš¨ ë¦¬ìŠ¤í¬ ë° ëŒ€ì‘ ê³„íš

### Risk 1: scrapy-playwright ì„¤ì¹˜ ì‹¤íŒ¨

**ëŒ€ì‘**: Playwright ìˆ˜ë™ ì„¤ì¹˜ ê°€ì´ë“œ ì¤€ë¹„
```bash
playwright install --with-deps chromium
```

---

### Risk 2: API ë¹„ìš© ì´ˆê³¼

**ëŒ€ì‘**: ì¼ì¼ í˜¸ì¶œ ì œí•œ (10íšŒ/ì¼)
```python
if daily_api_calls > 10:
    raise Exception("Daily API limit exceeded")
```

---

### Risk 3: PostgreSQL ì—°ê²° ì‹¤íŒ¨

**ëŒ€ì‘**: Docker ì¬ì‹œì‘, í¬íŠ¸ ì¶©ëŒ í™•ì¸
```bash
docker-compose down && docker-compose up -d
```

---

## ğŸ“ˆ ì˜ˆìƒ ì„±ê³¼ (Expected Outcomes)

### ì •ëŸ‰ì  ì„±ê³¼

- **í¬ë¡¤ë§ ì„±ê³µë¥ **: â‰¥90% (27/30)
- **í’ˆì§ˆ ì ìˆ˜**: í‰ê·  85ì  ì´ìƒ
- **ìë™ ë³µêµ¬ ì‹œê°„**: 30-60ì´ˆ
- **ë¹„ìš©**: PoC $0.06, ì—°ê°„ $2.00

---

### ì •ì„±ì  ì„±ê³¼

- PostgreSQL ê¸°ë°˜ í”„ë¡œë•ì…˜ ì¤€ë¹„ ì™„ë£Œ (ë§ˆì´ê·¸ë ˆì´ì…˜ ë¶ˆí•„ìš”)
- 2-Agent ì‹œìŠ¤í…œ ê²€ì¦ (í¸í–¥ ë°©ì§€)
- 3ê°€ì§€ ìœ ìŠ¤ì¼€ì´ìŠ¤ ëª…í™•í™” (UC1/UC2/UC3)
- LangGraph ì¡°ê±´ë¶€ ë¼ìš°íŒ… ì‹¤ì „ ì ìš©

---

## ğŸ”— ì°¸ê³  ë¬¸ì„œ

- [00-PRD-1-PROBLEM-SOLUTION.md](./00-PRD-1-PROBLEM-SOLUTION.md) - ë¬¸ì œ/ì†”ë£¨ì…˜
- [00-PRD-2-TECHNICAL-SPEC.md](./00-PRD-2-TECHNICAL-SPEC.md) - ê¸°ìˆ  ëª…ì„¸
- [README.md](./README.md) - í”„ë¡œì íŠ¸ ê°œìš”

---

**ë¬¸ì„œ ìƒíƒœ**: âœ… ê²€ì¦ ì™„ë£Œ (10ì¼ ê°œë°œ ê³„íš)
**ê°œë°œ ì‹œì‘ ì¡°ê±´**: ì´í•´ê´€ê³„ì ìŠ¹ì¸ í›„ ì¦‰ì‹œ ì°©ìˆ˜ ê°€ëŠ¥
