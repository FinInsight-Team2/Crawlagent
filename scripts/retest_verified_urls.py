"""
CrawlAgent - DB ê²€ì¦ëœ URL ì¬í…ŒìŠ¤íŠ¸
Created: 2025-11-15

DBì—ì„œ Quality 80+ URL ì¶”ì¶œí•˜ì—¬ ì¬í…ŒìŠ¤íŠ¸:
- 10ê°œ ìƒ˜í”Œ URL ì¬í¬ë¡¤ë§
- Master Workflow ì‹¤í–‰
- ê²°ê³¼ ë¹„êµ (ê¸°ì¡´ vs ì¬í…ŒìŠ¤íŠ¸)
"""

import sys
from pathlib import Path
from typing import Dict, List

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from loguru import logger

from src.storage.database import get_db
from src.storage.models import CrawlResult
from src.workflow.master_crawl_workflow import MasterCrawlState, build_master_graph

# Disable verbose logging for cleaner output
logger.remove()
logger.add(sys.stderr, level="WARNING")


def test_single_url(url: str, site_name: str, master_app) -> Dict:
    """
    ë‹¨ì¼ URL ì¬í…ŒìŠ¤íŠ¸ ì‹¤í–‰

    Returns:
        {
            "url": str,
            "site_name": str,
            "success": bool,
            "uc": str,
            "quality_score": int,
            "title": str,
            "body_length": int,
            "date": str,
            "error": str
        }
    """
    try:
        initial_state: MasterCrawlState = {
            "url": url,
            "site_name": site_name,
            "html_content": None,
            "current_uc": None,
            "next_action": None,
            "failure_count": 0,
            "quality_passed": None,
            "extracted_title": None,
            "extracted_body": None,
            "extracted_date": None,
            "uc1_validation_result": None,
            "uc2_consensus_result": None,
            "uc3_discovery_result": None,
            "final_result": None,
            "error_message": None,
            "workflow_history": [],
            "supervisor_reasoning": None,
            "supervisor_confidence": None,
            "routing_context": None,
        }

        final_state = master_app.invoke(initial_state)

        # ê²°ê³¼ ë¶„ì„
        workflow_path = " â†’ ".join(
            [h.split("(")[0].strip() for h in final_state.get("workflow_history", [])]
        )

        # UC ê²°ì •
        uc_path_lower = workflow_path.lower()
        if "uc3" in uc_path_lower:
            uc = "UC3"
        elif "uc2" in uc_path_lower:
            uc = "UC2"
        elif "uc1" in uc_path_lower:
            uc = "UC1"
        else:
            uc = "Unknown"

        # Quality score
        uc1_result = final_state.get("uc1_validation_result", {})
        quality_score = uc1_result.get("quality_score", 0) if uc1_result else 0

        # Success: Quality 80+ or final_result exists
        success = quality_score >= 80 or final_state.get("final_result") is not None

        # Extract data
        final_result = final_state.get("final_result", {})
        title = final_result.get("title", "") if final_result else ""
        body = final_result.get("body", "") if final_result else ""
        date = final_result.get("date", "") if final_result else ""

        error = final_state.get("error_message", "")

        return {
            "url": url,
            "site_name": site_name,
            "success": success,
            "uc": uc,
            "quality_score": quality_score,
            "title": title[:100] if title else "",
            "body_length": len(body) if body else 0,
            "date": date,
            "error": error,
        }

    except Exception as e:
        return {
            "url": url,
            "site_name": site_name,
            "success": False,
            "uc": "Error",
            "quality_score": 0,
            "title": "",
            "body_length": 0,
            "date": "",
            "error": str(e),
        }


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - DB ê²€ì¦ëœ URL 10ê°œ ì¬í…ŒìŠ¤íŠ¸
    """
    print("=" * 80)
    print("CrawlAgent - DB ê²€ì¦ëœ URL ì¬í…ŒìŠ¤íŠ¸ (2025-11-15)")
    print("=" * 80)
    print()

    # DB ì—°ê²°
    db_gen = get_db()
    db = next(db_gen)

    try:
        # DBì—ì„œ ê²€ì¦ëœ URL 10ê°œ ì¶”ì¶œ (ë‹¤ì–‘í•œ ì‚¬ì´íŠ¸)
        print("ğŸ” DBì—ì„œ ê²€ì¦ëœ URL ì¶”ì¶œ ì¤‘...")

        # ì‚¬ì´íŠ¸ë³„ë¡œ ë‹¤ì–‘í•˜ê²Œ ì¶”ì¶œ
        test_urls = []
        sites = ["yonhap", "naver", "kbs", "bbc", "cnn", "hankyung", "joongang", "donga", "n", "mk"]

        for site in sites:
            result = (
                db.query(CrawlResult)
                .filter(CrawlResult.site_name == site, CrawlResult.quality_score >= 80)
                .order_by(CrawlResult.created_at.desc())
                .first()
            )

            if result:
                test_urls.append(
                    {
                        "url": result.url,
                        "site_name": result.site_name,
                        "original_quality": result.quality_score,
                        "original_title": result.title[:50] if result.title else "",
                        "original_body_length": len(result.body) if result.body else 0,
                        "original_date": result.date,
                        "original_mode": result.crawl_mode,
                    }
                )

                if len(test_urls) >= 10:
                    break

        print(f"âœ… ì´ {len(test_urls)}ê°œ URL ì¶”ì¶œ ì™„ë£Œ")
        print()

        # Master workflow ë¹Œë“œ
        print("ğŸ—ï¸  Master Workflow ë¹Œë“œ ì¤‘...")
        master_app = build_master_graph()
        print("âœ… Master Workflow ì¤€ë¹„ ì™„ë£Œ")
        print()

        # ì¬í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥
        retest_results = []

        # 10 URL ì¬í…ŒìŠ¤íŠ¸
        for i, url_data in enumerate(test_urls, 1):
            print(f"[{i}/{len(test_urls)}] ì¬í…ŒìŠ¤íŠ¸ ì¤‘: {url_data['site_name']}")
            print(f"        URL: {url_data['url'][:70]}...")
            print(f"        ì›ë³¸ Quality: {url_data['original_quality']}")

            result = test_single_url(url_data["url"], url_data["site_name"], master_app)

            # ì›ë³¸ ë°ì´í„° ì¶”ê°€
            result["original_quality"] = url_data["original_quality"]
            result["original_title"] = url_data["original_title"]
            result["original_body_length"] = url_data["original_body_length"]
            result["original_date"] = url_data["original_date"]
            result["original_mode"] = url_data["original_mode"]

            retest_results.append(result)

            status_icon = "âœ…" if result["success"] else "âŒ"
            print(
                f"        {status_icon} ì¬í…ŒìŠ¤íŠ¸ Quality: {result['quality_score']} (UC: {result['uc']})"
            )

            # ë¹„êµ ê²°ê³¼
            quality_diff = result["quality_score"] - url_data["original_quality"]
            if quality_diff != 0:
                diff_icon = "ğŸ“ˆ" if quality_diff > 0 else "ğŸ“‰"
                print(f"        {diff_icon} Quality ì°¨ì´: {quality_diff:+d}")

            if not result["success"] and result["error"]:
                print(f"        âš ï¸  Error: {result['error'][:80]}")

            print()

        # ============================================================================
        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        # ============================================================================
        print("=" * 80)
        print("ğŸ“Š ì¬í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("=" * 80)
        print()

        # ì„±ê³µ/ì‹¤íŒ¨ ì¹´ìš´íŠ¸
        total = len(retest_results)
        success_count = len([r for r in retest_results if r["success"]])
        fail_count = total - success_count

        print(f"âœ“ ì´ ì¬í…ŒìŠ¤íŠ¸: {total}ê°œ")
        print(f"âœ… ì„±ê³µ: {success_count}ê°œ ({success_count/total*100:.1f}%)")
        print(f"âŒ ì‹¤íŒ¨: {fail_count}ê°œ ({fail_count/total*100:.1f}%)")
        print()

        # UCë³„ ì¹´ìš´íŠ¸
        uc1_count = len([r for r in retest_results if r["uc"] == "UC1"])
        uc2_count = len([r for r in retest_results if r["uc"] == "UC2"])
        uc3_count = len([r for r in retest_results if r["uc"] == "UC3"])

        print(f"ğŸ“ UC1 (Quality Validation): {uc1_count}ê°œ")
        print(f"ğŸ“ UC2 (Self-Healing): {uc2_count}ê°œ")
        print(f"ğŸ“ UC3 (Discovery): {uc3_count}ê°œ")
        print()

        # Quality Score ë¹„êµ
        success_results = [r for r in retest_results if r["success"]]
        if success_results:
            avg_original = sum([r["original_quality"] for r in success_results]) / len(
                success_results
            )
            avg_retest = sum([r["quality_score"] for r in success_results]) / len(success_results)
            print(f"ğŸ“ˆ ì›ë³¸ í‰ê·  Quality: {avg_original:.1f}/100")
            print(f"ğŸ“ˆ ì¬í…ŒìŠ¤íŠ¸ í‰ê·  Quality: {avg_retest:.1f}/100")
            print(f"ğŸ“Š Quality ë³€í™”: {avg_retest - avg_original:+.1f}")
        else:
            print(f"ğŸ“ˆ í‰ê·  Quality: N/A (ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ ì—†ìŒ)")

        print()

        # ìƒì„¸ ë¹„êµ í…Œì´ë¸”
        print("=" * 80)
        print("ğŸ“‹ ìƒì„¸ ë¹„êµ ê²°ê³¼")
        print("=" * 80)
        print()

        for i, r in enumerate(retest_results, 1):
            print(f"{i}. {r['site_name']}")
            print(f"   URL: {r['url'][:70]}...")
            print(f"   ì›ë³¸: Quality={r['original_quality']}, Mode={r['original_mode']}")
            print(f"   ì¬í…ŒìŠ¤íŠ¸: Quality={r['quality_score']}, UC={r['uc']}")

            if r["success"]:
                print(f"   ì œëª©: {r['title'][:60]}...")
                print(f"   ë³¸ë¬¸ ê¸¸ì´: {r['body_length']} chars")
                print(f"   ë‚ ì§œ: {r['date']}")
            else:
                error_msg = r["error"] if r["error"] else "Unknown error"
                print(f"   âŒ ì‹¤íŒ¨: {error_msg[:60]}")

            print()

        print("=" * 80)
        print("âœ… ì¬í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        print("=" * 80)

    finally:
        db.close()


if __name__ == "__main__":
    main()
