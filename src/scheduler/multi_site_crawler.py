"""
CrawlAgent - Multi-Site Crawler Module
Created: 2025-11-17

ë‹¤ì¤‘ ì‚¬ì´íŠ¸ ë° ì¹´í…Œê³ ë¦¬ ìë™í™” í¬ë¡¤ë§ ì§€ì›
- ë™ì  ì‚¬ì´íŠ¸/ì¹´í…Œê³ ë¦¬ ì„ íƒ
- Scrapy spider ê²€ì¦
- ë³‘ë ¬ ì‹¤í–‰ ì§€ì›
"""

import subprocess
from datetime import date, timedelta
from pathlib import Path
from typing import Dict, List, Tuple

from loguru import logger

# í”„ë¡œì íŠ¸ ë£¨íŠ¸
PROJECT_ROOT = Path(__file__).parent.parent.parent

# ê²€ì¦ëœ ì‚¬ì´íŠ¸ ë° ì¹´í…Œê³ ë¦¬ ì •ì˜
VERIFIED_SITES = {
    "yonhap": {
        "name": "ì—°í•©ë‰´ìŠ¤",
        "spider": "yonhap",
        "crawl_count": 827,
        "avg_quality": 94.8,
        "rating": "â˜…â˜…â˜…â˜…â˜…",
        "categories": {
            "politics": "ì •ì¹˜",
            "economy": "ê²½ì œ",
            "nk": "ë¶í•œ",
            "international": "êµ­ì œ",
            "society": "ì‚¬íšŒ",
            "culture": "ë¬¸í™”",
            "sports": "ìŠ¤í¬ì¸ "
        }
    },
    "naver": {
        "name": "ë„¤ì´ë²„ ë‰´ìŠ¤",
        "spider": "naver",
        "crawl_count": 10,
        "avg_quality": 96.0,
        "rating": "â˜…â˜…â˜…â˜…â˜†",
        "categories": {
            "politics": "ì •ì¹˜",
            "economy": "ê²½ì œ",
            "society": "ì‚¬íšŒ",
            "culture": "ìƒí™œ/ë¬¸í™”",
            "world": "ì„¸ê³„",
            "it": "IT/ê³¼í•™"
        }
    },
    "bbc": {
        "name": "BBC (ì˜ë¬¸)",
        "spider": "bbc",
        "crawl_count": 2,
        "avg_quality": 90.0,
        "rating": "â˜…â˜…â˜…â˜†â˜†",
        "categories": {
            "uk": "UK",
            "world": "World",
            "business": "Business",
            "politics": "Politics",
            "technology": "Technology",
            "science": "Science",
            "health": "Health",
            "education": "Education"
        }
    }
}


def get_available_sites() -> List[Tuple[str, str]]:
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ì‚¬ì´íŠ¸ ëª©ë¡ ë°˜í™˜ (Gradio CheckboxGroupìš©)

    Returns:
        [(label, value), ...] í˜•ì‹ì˜ ë¦¬ìŠ¤íŠ¸
    """
    sites = []
    for site_key, site_info in VERIFIED_SITES.items():
        label = site_info['name']
        sites.append((label, site_key))

    return sites


def get_site_categories(site_key: str) -> List[Tuple[str, str]]:
    """
    íŠ¹ì • ì‚¬ì´íŠ¸ì˜ ì¹´í…Œê³ ë¦¬ ëª©ë¡ ë°˜í™˜

    Args:
        site_key: ì‚¬ì´íŠ¸ í‚¤ (yonhap, naver, bbc)

    Returns:
        [(label, value), ...] í˜•ì‹ì˜ ë¦¬ìŠ¤íŠ¸
    """
    if site_key not in VERIFIED_SITES:
        return []

    site_info = VERIFIED_SITES[site_key]
    categories = site_info.get("categories", {})

    return [(f"{kor_name} ({eng_key})", eng_key)
            for eng_key, kor_name in categories.items()]


def validate_spider_exists(site_key: str) -> bool:
    """
    Spider íŒŒì¼ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸

    Args:
        site_key: ì‚¬ì´íŠ¸ í‚¤

    Returns:
        True if spider exists
    """
    spider_file = PROJECT_ROOT / "src" / "crawlers" / "spiders" / f"{site_key}.py"
    return spider_file.exists()


def validate_category(site_key: str, category: str) -> bool:
    """
    ì¹´í…Œê³ ë¦¬ê°€ í•´ë‹¹ ì‚¬ì´íŠ¸ì—ì„œ ìœ íš¨í•œì§€ í™•ì¸

    Args:
        site_key: ì‚¬ì´íŠ¸ í‚¤
        category: ì¹´í…Œê³ ë¦¬ í‚¤

    Returns:
        True if valid
    """
    if site_key not in VERIFIED_SITES:
        return False

    site_categories = VERIFIED_SITES[site_key].get("categories", {})

    # BBCëŠ” ì¹´í…Œê³ ë¦¬ ì—†ìŒ (í•­ìƒ ìœ íš¨)
    if not site_categories:
        return True

    return category in site_categories


def run_multi_site_crawl(
    sites: List[str],
    categories_per_site: Dict[str, List[str]],
    target_date: str = None,
    scope: str = "selected"
) -> Tuple[str, str, Dict]:
    """
    ë‹¤ì¤‘ ì‚¬ì´íŠ¸/ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹¤í–‰

    Args:
        sites: ì‹¤í–‰í•  ì‚¬ì´íŠ¸ ë¦¬ìŠ¤íŠ¸ (["yonhap", "naver"])
        categories_per_site: ì‚¬ì´íŠ¸ë³„ ì¹´í…Œê³ ë¦¬ ë§µ {"yonhap": ["politics"], "naver": ["economy"]}
        target_date: ìˆ˜ì§‘ ë‚ ì§œ (YYYY-MM-DD), Noneì´ë©´ ì–´ì œ
        scope: "selected" (ì„ íƒ ì¹´í…Œê³ ë¦¬ë§Œ) or "all" (ì „ì²´ ì¹´í…Œê³ ë¦¬)

    Returns:
        (status_message, log_message, stats_dict)
    """
    try:
        logger.info(f"[Multi-Site Crawler] ì‹œì‘: sites={sites}, scope={scope}")

        # ê¸°ë³¸ê°’: ì–´ì œ ë‚ ì§œ
        if not target_date:
            yesterday = date.today() - timedelta(days=1)
            target_date = yesterday.strftime("%Y-%m-%d")

        # ê²€ì¦
        for site in sites:
            if not validate_spider_exists(site):
                return (
                    "âŒ ì‹¤í–‰ ì‹¤íŒ¨",
                    f"Spider íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {site}",
                    {"success": 0, "failed": 1}
                )

        # ì‹¤í–‰ ê³„íš ìˆ˜ë¦½
        crawl_tasks = []

        for site in sites:
            site_info = VERIFIED_SITES[site]

            # scopeì— ë”°ë¼ ì¹´í…Œê³ ë¦¬ ê²°ì •
            if scope == "all":
                # ì „ì²´ ì¹´í…Œê³ ë¦¬
                categories = list(site_info.get("categories", {}).keys())
            else:
                # ì„ íƒí•œ ì¹´í…Œê³ ë¦¬ë§Œ
                categories = categories_per_site.get(site, [])

            # ëª¨ë“  ì‚¬ì´íŠ¸ ì¹´í…Œê³ ë¦¬ë³„ ì‹¤í–‰ (BBCë„ ë™ì¼)
            if not categories:
                # ì¹´í…Œê³ ë¦¬ê°€ ì—†ìœ¼ë©´ ì „ì²´ í¬ë¡¤ë§ (ê¸°ë³¸ ë™ì‘)
                crawl_tasks.append({
                    "site": site,
                    "category": None,
                    "target_date": target_date
                })
            else:
                for category in categories:
                    # ì¹´í…Œê³ ë¦¬ ê²€ì¦
                    if not validate_category(site, category):
                        logger.warning(f"[Multi-Site] ìœ íš¨í•˜ì§€ ì•Šì€ ì¹´í…Œê³ ë¦¬ ìŠ¤í‚µ: {site}/{category}")
                        continue

                    crawl_tasks.append({
                        "site": site,
                        "category": category,
                        "target_date": target_date
                    })

        if not crawl_tasks:
            return (
                "âš ï¸ ì‹¤í–‰ ì—†ìŒ",
                "ì‹¤í–‰í•  í¬ë¡¤ë§ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´íŠ¸ì™€ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”.",
                {"success": 0, "failed": 0}
            )

        # ì‹¤í–‰ ë¡œê·¸ ìƒì„±
        log_lines = [
            f"ğŸš€ ë‹¤ì¤‘ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹œì‘",
            f"ğŸ“… ìˆ˜ì§‘ ë‚ ì§œ: {target_date}",
            f"ğŸ¯ ì‹¤í–‰ ì‘ì—… ìˆ˜: {len(crawl_tasks)}",
            ""
        ]

        # Scrapy í¬ë¡¤ë§ ì‹¤í–‰
        success_count = 0
        failed_count = 0

        for task in crawl_tasks:
            site = task["site"]
            category = task["category"]

            # ëª…ë ¹ì–´ êµ¬ì„±
            cmd = ["poetry", "run", "scrapy", "crawl", site]

            # target_date íŒŒë¼ë¯¸í„°
            cmd.extend(["-a", f"target_date={target_date}"])

            # category íŒŒë¼ë¯¸í„° (BBC ì œì™¸)
            if category:
                cmd.extend(["-a", f"category={category}"])

            # ë¡œê·¸
            task_name = f"{site}/{category}" if category else site
            log_lines.append(f"â–¶ï¸ {task_name} ì‹¤í–‰ ì¤‘...")
            logger.info(f"[Multi-Site] ì‹¤í–‰: {' '.join(cmd)}")

            # ì‹¤í–‰
            try:
                result = subprocess.run(
                    cmd,
                    cwd=PROJECT_ROOT,
                    timeout=600,  # 10ë¶„ íƒ€ì„ì•„ì›ƒ (ëŒ€ëŸ‰ í¬ë¡¤ë§ ëŒ€ë¹„)
                    capture_output=True,
                    text=True
                )

                if result.returncode == 0:
                    success_count += 1
                    log_lines.append(f"   âœ… {task_name} ì™„ë£Œ")
                else:
                    failed_count += 1
                    log_lines.append(f"   âŒ {task_name} ì‹¤íŒ¨ (ì½”ë“œ: {result.returncode})")
                    logger.error(f"[Multi-Site] {task_name} stderr: {result.stderr}")

            except subprocess.TimeoutExpired:
                failed_count += 1
                log_lines.append(f"   â±ï¸ {task_name} íƒ€ì„ì•„ì›ƒ (10ë¶„ ì´ˆê³¼)")
                logger.error(f"[Multi-Site] {task_name} timeout")

            except Exception as e:
                failed_count += 1
                log_lines.append(f"   âŒ {task_name} ì˜¤ë¥˜: {str(e)}")
                logger.error(f"[Multi-Site] {task_name} error: {e}")

        # ê²°ê³¼ ìš”ì•½
        log_lines.extend([
            "",
            "=" * 50,
            f"âœ… ì„±ê³µ: {success_count}",
            f"âŒ ì‹¤íŒ¨: {failed_count}",
            f"ğŸ“Š ì´ ì‘ì—…: {len(crawl_tasks)}",
            "=" * 50
        ])

        log_message = "\n".join(log_lines)

        if failed_count == 0:
            status = f"âœ… ì™„ë£Œ ({success_count}ê°œ ì„±ê³µ)"
        else:
            status = f"âš ï¸ ë¶€ë¶„ ì™„ë£Œ (ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {failed_count})"

        stats = {
            "success": success_count,
            "failed": failed_count,
            "total": len(crawl_tasks),
            "target_date": target_date
        }

        logger.info(f"[Multi-Site Crawler] ì™„ë£Œ: {stats}")

        return status, log_message, stats

    except Exception as e:
        logger.error(f"[Multi-Site Crawler] ì˜ˆì™¸ ë°œìƒ: {e}")
        return (
            "âŒ ì‹¤í–‰ ì‹¤íŒ¨",
            f"ì˜¤ë¥˜: {str(e)}",
            {"success": 0, "failed": 1}
        )


def get_crawl_plan_summary(
    sites: List[str],
    categories_per_site: Dict[str, List[str]],
    scope: str = "selected"
) -> str:
    """
    ì‹¤í–‰ ê³„íš ìš”ì•½ ìƒì„± (ì‹¤í–‰ ì „ í™•ì¸ìš©)

    Args:
        sites: ì‚¬ì´íŠ¸ ë¦¬ìŠ¤íŠ¸
        categories_per_site: ì‚¬ì´íŠ¸ë³„ ì¹´í…Œê³ ë¦¬
        scope: ìˆ˜ì§‘ ë²”ìœ„

    Returns:
        ìš”ì•½ ë¬¸ìì—´
    """
    lines = ["ğŸ“‹ ì‹¤í–‰ ê³„íš ìš”ì•½", ""]

    for site in sites:
        site_info = VERIFIED_SITES.get(site, {})
        site_name = site_info.get("name", site)

        lines.append(f"ğŸ”¹ {site_name} ({site})")

        if scope == "all":
            categories = list(site_info.get("categories", {}).keys())
            if categories:
                lines.append(f"   â”” ì „ì²´ ì¹´í…Œê³ ë¦¬ ({len(categories)}ê°œ)")
            else:
                lines.append(f"   â”” ì¹´í…Œê³ ë¦¬ ì—†ìŒ (ì „ì²´ ìˆ˜ì§‘)")
        else:
            categories = categories_per_site.get(site, [])
            if categories:
                cat_names = [site_info["categories"].get(c, c) for c in categories]
                lines.append(f"   â”” ì„ íƒ ì¹´í…Œê³ ë¦¬: {', '.join(cat_names)}")
            else:
                lines.append(f"   â”” ì¹´í…Œê³ ë¦¬ ì„ íƒ ì•ˆ ë¨")

        lines.append("")

    return "\n".join(lines)
