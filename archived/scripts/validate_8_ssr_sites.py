"""
8ê°œ SSR ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ì‹¤ì œ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
Created: 2025-11-16

Purpose:
    Phase 1 SSR-only ì‚¬ì´íŠ¸ 8ê°œì— ëŒ€í•œ ì‹¤ì œ í¬ë¡¤ë§ ì„±ê³µë¥  ê²€ì¦
    - DBì— ì €ì¥ëœ ì‹¤ì œ 491ê°œ í¬ë¡¤ë§ ê²°ê³¼ ë¶„ì„
    - ì‚¬ì´íŠ¸ë‹¹ 10ê°œ URL í…ŒìŠ¤íŠ¸ (ê°€ëŠ¥í•œ ê²½ìš°)
    - ê°ê´€ì ì¸ ì„±ê³µë¥ , ì²˜ë¦¬ ì‹œê°„, í’ˆì§ˆ ì ìˆ˜ ì¸¡ì •

Excluded Sites:
    - Bloomberg: Paywall
    - JTBC: SPA ê°€ëŠ¥ì„±
    - Phase 2ë¡œ ì—°ê¸°

Target Sites (SSR-only):
    1. Yonhap (ì—°í•©ë‰´ìŠ¤)
    2. Donga (ë™ì•„ì¼ë³´)
    3. MK (ë§¤ì¼ê²½ì œ)
    4. eDaily (ì´ë°ì¼ë¦¬)
    5. BBC
    6. Reuters
    7. Hankyung (í•œêµ­ê²½ì œ)
    8. CNN

Usage:
    cd /Users/charlee/Desktop/Intern/crawlagent
    poetry run python scripts/validate_8_ssr_sites.py
"""

import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import json
from datetime import datetime
from typing import Dict, List

from loguru import logger
from sqlalchemy import func

from src.storage.models import CrawlResult, Selector
from src.utils.db_utils import get_db_session_no_commit

# 8ê°œ SSR ì‚¬ì´íŠ¸ ì •ì˜
SSR_SITES = [
    "yonhap",
    "donga",
    "mk",
    "edaily",
    "bbc",
    "reuters",
    "hankyung",
    "cnn",
]


def analyze_db_crawl_results() -> Dict[str, Dict]:
    """
    DBì— ì €ì¥ëœ ì‹¤ì œ í¬ë¡¤ë§ ê²°ê³¼ ë¶„ì„

    Returns:
        ì‚¬ì´íŠ¸ë³„ í†µê³„: {
            'site_name': {
                'total_crawls': int,
                'successful_crawls': int,
                'success_rate': float,
                'avg_quality_score': float,
                'avg_processing_time': float,
                'latest_crawl': str
            }
        }
    """
    logger.info("=" * 80)
    logger.info("DB ì‹¤ì œ í¬ë¡¤ë§ ê²°ê³¼ ë¶„ì„ ì‹œì‘")
    logger.info("=" * 80)

    results = {}

    with get_db_session_no_commit() as db:
        for site_name in SSR_SITES:
            # ì‚¬ì´íŠ¸ë³„ í¬ë¡¤ë§ ê²°ê³¼ ì¡°íšŒ
            crawl_results = db.query(CrawlResult).filter_by(site_name=site_name).all()

            if not crawl_results:
                logger.warning(f"âš ï¸  {site_name}: DBì— í¬ë¡¤ë§ ê²°ê³¼ ì—†ìŒ")
                results[site_name] = {
                    "total_crawls": 0,
                    "successful_crawls": 0,
                    "success_rate": 0.0,
                    "avg_quality_score": 0.0,
                    "avg_processing_time": 0.0,
                    "latest_crawl": None,
                }
                continue

            # í†µê³„ ê³„ì‚°
            total_crawls = len(crawl_results)
            successful_crawls = sum(1 for r in crawl_results if r.title and r.body)
            success_rate = (successful_crawls / total_crawls * 100) if total_crawls > 0 else 0.0

            # í’ˆì§ˆ ì ìˆ˜ í‰ê· 
            quality_scores = [r.quality_score for r in crawl_results if r.quality_score is not None]
            avg_quality_score = sum(quality_scores) / len(quality_scores) if quality_scores else 0.0

            # í¬ë¡¤ë§ ì†Œìš” ì‹œê°„ í‰ê·  (ì´ˆ ë‹¨ìœ„)
            crawl_durations = [
                r.crawl_duration_seconds
                for r in crawl_results
                if r.crawl_duration_seconds is not None
            ]
            avg_crawl_duration = sum(crawl_durations) / len(crawl_durations) if crawl_durations else 0.0

            # ìµœê·¼ í¬ë¡¤ë§ ì‹œê°„
            latest_crawl = max(r.created_at for r in crawl_results if r.created_at)

            results[site_name] = {
                "total_crawls": total_crawls,
                "successful_crawls": successful_crawls,
                "success_rate": success_rate,
                "avg_quality_score": avg_quality_score,
                "avg_crawl_duration": avg_crawl_duration,
                "latest_crawl": latest_crawl.strftime("%Y-%m-%d %H:%M:%S"),
            }

            logger.info(f"\nì‚¬ì´íŠ¸: {site_name}")
            logger.info(f"  ì´ í¬ë¡¤ë§: {total_crawls}ê°œ")
            logger.info(f"  ì„±ê³µ: {successful_crawls}ê°œ")
            logger.info(f"  ì„±ê³µë¥ : {success_rate:.1f}%")
            logger.info(f"  í‰ê·  í’ˆì§ˆ ì ìˆ˜: {avg_quality_score:.2f}")
            logger.info(f"  í‰ê·  í¬ë¡¤ë§ ì‹œê°„: {avg_crawl_duration:.2f}ì´ˆ")
            logger.info(f"  ìµœê·¼ í¬ë¡¤ë§: {latest_crawl.strftime('%Y-%m-%d %H:%M:%S')}")

    logger.info("=" * 80)
    return results


def analyze_selector_status() -> Dict[str, Dict]:
    """
    DBì— ì €ì¥ëœ Selector ìƒíƒœ ë¶„ì„

    Returns:
        ì‚¬ì´íŠ¸ë³„ Selector ì •ë³´
    """
    logger.info("=" * 80)
    logger.info("DB Selector ìƒíƒœ ë¶„ì„")
    logger.info("=" * 80)

    results = {}

    with get_db_session_no_commit() as db:
        for site_name in SSR_SITES:
            selector = db.query(Selector).filter_by(site_name=site_name).first()

            if not selector:
                logger.warning(f"âš ï¸  {site_name}: DBì— Selector ì—†ìŒ (UC3 íŠ¸ë¦¬ê±° ê°€ëŠ¥)")
                results[site_name] = {
                    "exists": False,
                    "success_count": 0,
                    "failure_count": 0,
                    "success_rate": 0.0,
                }
                continue

            # Selector ì„±ê³µë¥  ê³„ì‚°
            total_attempts = selector.success_count + selector.failure_count
            success_rate = (
                (selector.success_count / total_attempts * 100) if total_attempts > 0 else 0.0
            )

            results[site_name] = {
                "exists": True,
                "success_count": selector.success_count,
                "failure_count": selector.failure_count,
                "success_rate": success_rate,
                "title_selector": selector.title_selector,
                "body_selector": selector.body_selector,
                "date_selector": selector.date_selector,
                "site_type": selector.site_type,
            }

            logger.info(f"\nì‚¬ì´íŠ¸: {site_name}")
            logger.info(f"  Selector ì¡´ì¬: âœ…")
            logger.info(f"  ì„±ê³µ íšŸìˆ˜: {selector.success_count}")
            logger.info(f"  ì‹¤íŒ¨ íšŸìˆ˜: {selector.failure_count}")
            logger.info(f"  ì„±ê³µë¥ : {success_rate:.1f}%")
            logger.info(f"  title_selector: {selector.title_selector}")
            logger.info(f"  body_selector: {selector.body_selector}")
            logger.info(f"  site_type: {selector.site_type}")

    logger.info("=" * 80)
    return results


def calculate_overall_statistics(crawl_results: Dict, selector_results: Dict) -> Dict:
    """
    ì „ì²´ í†µê³„ ê³„ì‚°

    Args:
        crawl_results: í¬ë¡¤ë§ ê²°ê³¼ í†µê³„
        selector_results: Selector ìƒíƒœ í†µê³„

    Returns:
        ì „ì²´ í†µê³„ ìš”ì•½
    """
    logger.info("=" * 80)
    logger.info("ì „ì²´ í†µê³„ ìš”ì•½")
    logger.info("=" * 80)

    # ì „ì²´ í¬ë¡¤ë§ í†µê³„
    total_crawls = sum(r["total_crawls"] for r in crawl_results.values())
    total_successful = sum(r["successful_crawls"] for r in crawl_results.values())
    overall_success_rate = (total_successful / total_crawls * 100) if total_crawls > 0 else 0.0

    # í‰ê·  í’ˆì§ˆ ì ìˆ˜
    quality_scores = [r["avg_quality_score"] for r in crawl_results.values() if r["avg_quality_score"] > 0]
    avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0.0

    # í‰ê·  í¬ë¡¤ë§ ì‹œê°„
    crawl_durations = [
        r.get("avg_crawl_duration", 0) for r in crawl_results.values() if r.get("avg_crawl_duration", 0) > 0
    ]
    avg_crawl_duration = sum(crawl_durations) / len(crawl_durations) if crawl_durations else 0.0

    # Selector ì¡´ì¬ ì—¬ë¶€
    selectors_exist = sum(1 for r in selector_results.values() if r["exists"])
    selectors_missing = len(SSR_SITES) - selectors_exist

    overall_stats = {
        "total_sites": len(SSR_SITES),
        "total_crawls": total_crawls,
        "successful_crawls": total_successful,
        "overall_success_rate": overall_success_rate,
        "avg_quality_score": avg_quality,
        "avg_crawl_duration": avg_crawl_duration,
        "selectors_exist": selectors_exist,
        "selectors_missing": selectors_missing,
    }

    logger.info(f"\nì´ ì‚¬ì´íŠ¸ ìˆ˜: {len(SSR_SITES)}ê°œ (SSR-only)")
    logger.info(f"ì´ í¬ë¡¤ë§ ìˆ˜: {total_crawls}ê°œ")
    logger.info(f"ì„±ê³µ í¬ë¡¤ë§: {total_successful}ê°œ")
    logger.info(f"ì „ì²´ ì„±ê³µë¥ : {overall_success_rate:.1f}%")
    logger.info(f"í‰ê·  í’ˆì§ˆ ì ìˆ˜: {avg_quality:.2f}")
    logger.info(f"í‰ê·  í¬ë¡¤ë§ ì‹œê°„: {avg_crawl_duration:.2f}ì´ˆ")
    logger.info(f"Selector ì¡´ì¬: {selectors_exist}ê°œ")
    logger.info(f"Selector ëˆ„ë½: {selectors_missing}ê°œ")

    logger.info("=" * 80)

    return overall_stats


def save_validation_report(crawl_results: Dict, selector_results: Dict, overall_stats: Dict):
    """
    ê²€ì¦ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥

    Args:
        crawl_results: í¬ë¡¤ë§ ê²°ê³¼ í†µê³„
        selector_results: Selector ìƒíƒœ í†µê³„
        overall_stats: ì „ì²´ í†µê³„ ìš”ì•½
    """
    report_path = project_root / "docs" / "8_SSR_SITES_VALIDATION.md"
    report_path.parent.mkdir(parents=True, exist_ok=True)

    with open(report_path, "w", encoding="utf-8") as f:
        f.write("# 8ê°œ SSR ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ì‹¤ì œ ê²€ì¦ ê²°ê³¼\n\n")
        f.write(f"ìƒì„± ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        f.write("## 1. ì „ì²´ ìš”ì•½\n\n")
        f.write(f"- **ì´ ì‚¬ì´íŠ¸ ìˆ˜**: {overall_stats['total_sites']}ê°œ (SSR-only)\n")
        f.write(f"- **ì´ í¬ë¡¤ë§ ìˆ˜**: {overall_stats['total_crawls']}ê°œ\n")
        f.write(f"- **ì„±ê³µ í¬ë¡¤ë§**: {overall_stats['successful_crawls']}ê°œ\n")
        f.write(f"- **ì „ì²´ ì„±ê³µë¥ **: {overall_stats['overall_success_rate']:.1f}%\n")
        f.write(f"- **í‰ê·  í’ˆì§ˆ ì ìˆ˜**: {overall_stats['avg_quality_score']:.2f}\n")
        f.write(f"- **í‰ê·  í¬ë¡¤ë§ ì‹œê°„**: {overall_stats['avg_crawl_duration']:.2f}ì´ˆ\n")
        f.write(f"- **Selector ì¡´ì¬**: {overall_stats['selectors_exist']}ê°œ\n")
        f.write(f"- **Selector ëˆ„ë½**: {overall_stats['selectors_missing']}ê°œ\n\n")

        f.write("## 2. ì‚¬ì´íŠ¸ë³„ í¬ë¡¤ë§ ê²°ê³¼\n\n")
        f.write("| ì‚¬ì´íŠ¸ | ì´ í¬ë¡¤ë§ | ì„±ê³µ | ì„±ê³µë¥  | í‰ê·  í’ˆì§ˆ | í‰ê·  ì‹œê°„(ì´ˆ) | ìµœê·¼ í¬ë¡¤ë§ |\n")
        f.write("|--------|----------|------|--------|----------|------------|------------|\n")

        for site_name in SSR_SITES:
            stats = crawl_results[site_name]
            f.write(
                f"| {site_name} | {stats['total_crawls']} | {stats['successful_crawls']} | "
                f"{stats['success_rate']:.1f}% | {stats['avg_quality_score']:.2f} | "
                f"{stats.get('avg_crawl_duration', 0.0):.2f} | {stats['latest_crawl'] or 'N/A'} |\n"
            )

        f.write("\n## 3. ì‚¬ì´íŠ¸ë³„ Selector ìƒíƒœ\n\n")
        f.write("| ì‚¬ì´íŠ¸ | Selector ì¡´ì¬ | ì„±ê³µ íšŸìˆ˜ | ì‹¤íŒ¨ íšŸìˆ˜ | ì„±ê³µë¥  | Site Type |\n")
        f.write("|--------|--------------|----------|----------|--------|----------|\n")

        for site_name in SSR_SITES:
            stats = selector_results[site_name]
            exists = "âœ…" if stats["exists"] else "âŒ"
            f.write(
                f"| {site_name} | {exists} | {stats['success_count']} | "
                f"{stats['failure_count']} | {stats['success_rate']:.1f}% | "
                f"{stats.get('site_type', 'N/A')} |\n"
            )

        f.write("\n## 4. ì œì™¸ëœ ì‚¬ì´íŠ¸ (Phase 2)\n\n")
        f.write("- **Bloomberg**: Paywall ì‚¬ì´íŠ¸\n")
        f.write("- **JTBC**: SPA ê°€ëŠ¥ì„± ìˆìŒ\n\n")
        f.write("â†’ Phase 2ì—ì„œ ë™ì  ë Œë”ë§ ì§€ì› í›„ ì¬ê²€í†  ì˜ˆì •\n\n")

        f.write("## 5. í•œê³„ì  ë° ê°œì„  ì‚¬í•­\n\n")
        f.write("### í˜„ì¬ í•œê³„ì \n\n")
        if overall_stats["selectors_missing"] > 0:
            f.write(f"- {overall_stats['selectors_missing']}ê°œ ì‚¬ì´íŠ¸ Selector ëˆ„ë½\n")
        if overall_stats["overall_success_rate"] < 90:
            f.write(f"- ì „ì²´ ì„±ê³µë¥  {overall_stats['overall_success_rate']:.1f}% (ëª©í‘œ: 90% ì´ìƒ)\n")

        f.write("\n### ê°œì„  ê³„íš\n\n")
        f.write("1. **Ground Truth ê²€ì¦**: 30-50ê°œ ìƒ˜í”Œ ìˆ˜ë™ ê²€ì¦\n")
        f.write("2. **F1-Score ê³„ì‚°**: Precision, Recall ê¸°ë°˜ ê°ê´€ì  í‰ê°€\n")
        f.write("3. **UC3 ìë™ ë°œê²¬**: ëˆ„ë½ëœ Selector ìë™ ìƒì„±\n")
        f.write("4. **UC2 Self-Healing**: ì‹¤íŒ¨ìœ¨ ë†’ì€ Selector ìë™ ìˆ˜ì •\n\n")

        f.write("## 6. ì¬í˜„ ë°©ë²•\n\n")
        f.write("```bash\n")
        f.write("cd /Users/charlee/Desktop/Intern/crawlagent\n")
        f.write("poetry run python scripts/validate_8_ssr_sites.py\n")
        f.write("```\n\n")

        f.write("---\n")
        f.write("*ì´ ê²€ì¦ì€ ì‹¤ì œ DB ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. Mock ë°ì´í„° ì—†ìŒ.*\n")

    logger.success(f"âœ… ê²€ì¦ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {report_path}")


def main():
    logger.info("ğŸš€ 8ê°œ SSR ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ê²€ì¦ ì‹œì‘")

    # 1. DB í¬ë¡¤ë§ ê²°ê³¼ ë¶„ì„
    crawl_results = analyze_db_crawl_results()

    # 2. DB Selector ìƒíƒœ ë¶„ì„
    selector_results = analyze_selector_status()

    # 3. ì „ì²´ í†µê³„ ê³„ì‚°
    overall_stats = calculate_overall_statistics(crawl_results, selector_results)

    # 4. ê²€ì¦ ê²°ê³¼ ì €ì¥
    save_validation_report(crawl_results, selector_results, overall_stats)

    logger.success("ğŸ‰ 8ê°œ SSR ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ê²€ì¦ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
