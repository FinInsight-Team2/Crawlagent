"""
Ground Truth ìµœì†Œ ê²€ì¦ - 30-50 ìƒ˜í”Œ ìˆ˜ë™ ê²€ì¦
Created: 2025-11-16

Purpose:
    ì‹¤ì œ F1-Score ê³„ì‚°ì„ ìœ„í•œ Ground Truth êµ¬ì¶•
    - DBì—ì„œ ëœë¤ 30-50ê°œ ìƒ˜í”Œ ì¶”ì¶œ
    - ìˆ˜ë™ ê²€ì¦ (title/body ì¡´ì¬ ì—¬ë¶€)
    - Precision, Recall, F1-Score ê³„ì‚°

Methodology:
    1. ê° ì‚¬ì´íŠ¸ë‹¹ 5-10ê°œì”© ëœë¤ ìƒ˜í”Œë§ (ì´ 30-50ê°œ)
    2. í„°ë¯¸ë„ì—ì„œ URL + í¬ë¡¤ë§ ê²°ê³¼ ì¶œë ¥
    3. ì‚¬ìš©ìê°€ ìˆ˜ë™ìœ¼ë¡œ Y/N ì…ë ¥
    4. Ground Truth CSV ì €ì¥
    5. F1-Score ê³„ì‚°

Usage:
    cd /Users/charlee/Desktop/Intern/crawlagent
    poetry run python scripts/establish_ground_truth_minimal.py
"""

import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import csv
import random
from typing import Dict, List

from loguru import logger

from src.storage.models import CrawlResult
from src.utils.db_utils import get_db_session_no_commit

# ê²€ì¦í•  ìƒ˜í”Œ ìˆ˜
SAMPLES_PER_SITE = {
    "yonhap": 15,  # ê°€ì¥ ë§ì€ ë°ì´í„°
    "donga": 5,
    "mk": 5,
    "bbc": 5,
    "hankyung": 5,
    "cnn": 5,
}


def extract_random_samples() -> List[Dict]:
    """
    DBì—ì„œ ëœë¤ ìƒ˜í”Œ ì¶”ì¶œ

    Returns:
        ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸: [{url, site_name, title, body, quality_score}, ...]
    """
    logger.info("=" * 80)
    logger.info("DBì—ì„œ ëœë¤ ìƒ˜í”Œ ì¶”ì¶œ ì‹œì‘")
    logger.info("=" * 80)

    samples = []

    with get_db_session_no_commit() as db:
        for site_name, count in SAMPLES_PER_SITE.items():
            # í•´ë‹¹ ì‚¬ì´íŠ¸ì˜ ëª¨ë“  í¬ë¡¤ë§ ê²°ê³¼ ì¡°íšŒ
            crawl_results = db.query(CrawlResult).filter_by(site_name=site_name).all()

            if not crawl_results:
                logger.warning(f"âš ï¸  {site_name}: DBì— í¬ë¡¤ë§ ê²°ê³¼ ì—†ìŒ")
                continue

            # ëœë¤ ìƒ˜í”Œë§
            sample_count = min(count, len(crawl_results))
            random_samples = random.sample(crawl_results, sample_count)

            for result in random_samples:
                samples.append(
                    {
                        "url": result.url,
                        "site_name": result.site_name,
                        "title": result.title,
                        "body": result.body,
                        "quality_score": result.quality_score,
                        "predicted_success": bool(result.title and result.body),
                    }
                )

            logger.info(f"âœ… {site_name}: {sample_count}ê°œ ìƒ˜í”Œ ì¶”ì¶œ")

    logger.info(f"\nì´ {len(samples)}ê°œ ìƒ˜í”Œ ì¶”ì¶œ ì™„ë£Œ")
    logger.info("=" * 80)

    return samples


def manual_validation(samples: List[Dict]) -> List[Dict]:
    """
    ìˆ˜ë™ ê²€ì¦ ìˆ˜í–‰ (í„°ë¯¸ë„ ì¸í„°ë™í‹°ë¸Œ)

    Args:
        samples: ê²€ì¦í•  ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸

    Returns:
        Ground Truth ì¶”ê°€ëœ ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸
    """
    logger.info("=" * 80)
    logger.info("ìˆ˜ë™ ê²€ì¦ ì‹œì‘")
    logger.info("=" * 80)
    logger.info("ê° ìƒ˜í”Œì— ëŒ€í•´ Titleê³¼ Bodyê°€ ì˜¬ë°”ë¥´ê²Œ ì¶”ì¶œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    logger.info("Y: ì •ìƒ, N: ë¹„ì •ìƒ, S: ê±´ë„ˆë›°ê¸°, Q: ì¢…ë£Œ\n")

    ground_truth_samples = []

    for i, sample in enumerate(samples, 1):
        print("\n" + "=" * 80)
        print(f"ìƒ˜í”Œ {i}/{len(samples)}")
        print("=" * 80)
        print(f"ì‚¬ì´íŠ¸: {sample['site_name']}")
        print(f"URL: {sample['url']}")
        print(f"\n--- í¬ë¡¤ë§ ê²°ê³¼ ---")
        print(f"Title: {sample['title'][:100] if sample['title'] else 'âŒ None'}...")
        print(f"Body: {sample['body'][:200] if sample['body'] else 'âŒ None'}...")
        print(f"Quality Score: {sample['quality_score']}")
        print(f"ì˜ˆì¸¡ ê²°ê³¼: {'âœ… ì„±ê³µ' if sample['predicted_success'] else 'âŒ ì‹¤íŒ¨'}")
        print("=" * 80)

        while True:
            answer = input("ì˜¬ë°”ë¥¸ í¬ë¡¤ë§ì¸ê°€ìš”? (Y/N/S/Q): ").strip().upper()

            if answer == "Y":
                sample["ground_truth"] = True
                ground_truth_samples.append(sample)
                print("âœ… ì •ìƒìœ¼ë¡œ ê¸°ë¡")
                break
            elif answer == "N":
                sample["ground_truth"] = False
                ground_truth_samples.append(sample)
                print("âŒ ë¹„ì •ìƒìœ¼ë¡œ ê¸°ë¡")
                break
            elif answer == "S":
                print("â­ï¸  ê±´ë„ˆë›°ê¸°")
                break
            elif answer == "Q":
                logger.info(f"\nì¤‘ë‹¨: {len(ground_truth_samples)}ê°œ ìƒ˜í”Œ ê²€ì¦ ì™„ë£Œ")
                return ground_truth_samples
            else:
                print("âš ï¸  ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. Y/N/S/Q ì¤‘ í•˜ë‚˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

    logger.info(f"\nâœ… ì´ {len(ground_truth_samples)}ê°œ ìƒ˜í”Œ ê²€ì¦ ì™„ë£Œ")
    return ground_truth_samples


def save_ground_truth_csv(samples: List[Dict]):
    """
    Ground Truthë¥¼ CSVë¡œ ì €ì¥

    Args:
        samples: Ground Truth í¬í•¨ëœ ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸
    """
    csv_path = project_root / "docs" / "ground_truth_samples.csv"
    csv_path.parent.mkdir(parents=True, exist_ok=True)

    with open(csv_path, "w", encoding="utf-8", newline="") as f:
        fieldnames = [
            "site_name",
            "url",
            "title",
            "body",
            "quality_score",
            "predicted_success",
            "ground_truth",
        ]
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()

        for sample in samples:
            writer.writerow(
                {
                    "site_name": sample["site_name"],
                    "url": sample["url"],
                    "title": sample["title"][:100] if sample["title"] else "",
                    "body": sample["body"][:200] if sample["body"] else "",
                    "quality_score": sample["quality_score"],
                    "predicted_success": sample["predicted_success"],
                    "ground_truth": sample["ground_truth"],
                }
            )

    logger.success(f"âœ… Ground Truth ì €ì¥ ì™„ë£Œ: {csv_path}")


def calculate_f1_score(samples: List[Dict]) -> Dict:
    """
    F1-Score ê³„ì‚°

    Args:
        samples: Ground Truth í¬í•¨ëœ ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸

    Returns:
        {precision, recall, f1_score, tp, fp, tn, fn}
    """
    logger.info("=" * 80)
    logger.info("F1-Score ê³„ì‚°")
    logger.info("=" * 80)

    # True Positive: ì‹œìŠ¤í…œ ì„±ê³µ + Ground Truth ì„±ê³µ
    tp = sum(1 for s in samples if s["predicted_success"] and s["ground_truth"])

    # False Positive: ì‹œìŠ¤í…œ ì„±ê³µ + Ground Truth ì‹¤íŒ¨
    fp = sum(1 for s in samples if s["predicted_success"] and not s["ground_truth"])

    # True Negative: ì‹œìŠ¤í…œ ì‹¤íŒ¨ + Ground Truth ì‹¤íŒ¨
    tn = sum(1 for s in samples if not s["predicted_success"] and not s["ground_truth"])

    # False Negative: ì‹œìŠ¤í…œ ì‹¤íŒ¨ + Ground Truth ì„±ê³µ
    fn = sum(1 for s in samples if not s["predicted_success"] and s["ground_truth"])

    # Precision = TP / (TP + FP)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0

    # Recall = TP / (TP + FN)
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0

    # F1-Score = 2 * (Precision * Recall) / (Precision + Recall)
    f1_score = (
        2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
    )

    logger.info(f"\n--- Confusion Matrix ---")
    logger.info(f"True Positive (TP): {tp}")
    logger.info(f"False Positive (FP): {fp}")
    logger.info(f"True Negative (TN): {tn}")
    logger.info(f"False Negative (FN): {fn}")
    logger.info(f"\n--- Metrics ---")
    logger.info(f"Precision: {precision:.4f} ({precision*100:.2f}%)")
    logger.info(f"Recall: {recall:.4f} ({recall*100:.2f}%)")
    logger.info(f"F1-Score: {f1_score:.4f} ({f1_score*100:.2f}%)")
    logger.info("=" * 80)

    return {
        "precision": precision,
        "recall": recall,
        "f1_score": f1_score,
        "tp": tp,
        "fp": fp,
        "tn": tn,
        "fn": fn,
        "total_samples": len(samples),
    }


def save_f1_report(metrics: Dict):
    """
    F1-Score ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì €ì¥

    Args:
        metrics: F1-Score ë©”íŠ¸ë¦­
    """
    report_path = project_root / "docs" / "GROUND_TRUTH_F1_SCORE.md"

    with open(report_path, "w", encoding="utf-8") as f:
        f.write("# Ground Truth F1-Score ê²€ì¦ ê²°ê³¼\n\n")
        f.write(f"ìƒì„± ì‹œê°: 2025-11-16\n\n")

        f.write("## 1. ê²€ì¦ ë°©ë²•ë¡ \n\n")
        f.write("### ìƒ˜í”Œë§\n\n")
        f.write("- **ì´ ìƒ˜í”Œ ìˆ˜**: 30-50ê°œ (ì‚¬ì´íŠ¸ë³„ 5-15ê°œ)\n")
        f.write("- **ìƒ˜í”Œë§ ë°©ë²•**: ëœë¤ ìƒ˜í”Œë§ (random.sample)\n")
        f.write("- **ê²€ì¦ ë°©ë²•**: ìˆ˜ë™ ê²€ì¦ (ì‚¬ëŒì´ ì§ì ‘ í™•ì¸)\n\n")

        f.write("### í‰ê°€ ê¸°ì¤€\n\n")
        f.write("- **Predicted Success**: Titleê³¼ Bodyê°€ ëª¨ë‘ ì¶”ì¶œë˜ë©´ ì„±ê³µ\n")
        f.write("- **Ground Truth**: ì‚¬ëŒì´ ìˆ˜ë™ìœ¼ë¡œ í™•ì¸í•œ ì‹¤ì œ ê²°ê³¼\n")
        f.write("- **F1-Score**: Precisionê³¼ Recallì˜ ì¡°í™” í‰ê· \n\n")

        f.write("## 2. Confusion Matrix\n\n")
        f.write("| êµ¬ë¶„ | ê°’ |\n")
        f.write("|------|----|\n")
        f.write(f"| True Positive (TP) | {metrics['tp']} |\n")
        f.write(f"| False Positive (FP) | {metrics['fp']} |\n")
        f.write(f"| True Negative (TN) | {metrics['tn']} |\n")
        f.write(f"| False Negative (FN) | {metrics['fn']} |\n")
        f.write(f"| **Total Samples** | {metrics['total_samples']} |\n\n")

        f.write("## 3. í‰ê°€ ë©”íŠ¸ë¦­\n\n")
        f.write("| ë©”íŠ¸ë¦­ | ê°’ | ë°±ë¶„ìœ¨ |\n")
        f.write("|--------|----|---------|\n")
        f.write(f"| **Precision** | {metrics['precision']:.4f} | {metrics['precision']*100:.2f}% |\n")
        f.write(f"| **Recall** | {metrics['recall']:.4f} | {metrics['recall']*100:.2f}% |\n")
        f.write(f"| **F1-Score** | {metrics['f1_score']:.4f} | {metrics['f1_score']*100:.2f}% |\n\n")

        f.write("## 4. ê³µì‹\n\n")
        f.write("```\n")
        f.write("Precision = TP / (TP + FP)\n")
        f.write("Recall = TP / (TP + FN)\n")
        f.write("F1-Score = 2 * (Precision * Recall) / (Precision + Recall)\n")
        f.write("```\n\n")

        f.write("## 5. ì¬í˜„ ë°©ë²•\n\n")
        f.write("```bash\n")
        f.write("cd /Users/charlee/Desktop/Intern/crawlagent\n")
        f.write("poetry run python scripts/establish_ground_truth_minimal.py\n")
        f.write("```\n\n")

        f.write("---\n")
        f.write("*ì´ ê²€ì¦ì€ ì‹¤ì œ ìˆ˜ë™ ê²€ì¦ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. Mock ë°ì´í„° ì—†ìŒ.*\n")

    logger.success(f"âœ… F1-Score ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {report_path}")


def main():
    logger.info("ğŸš€ Ground Truth ìµœì†Œ ê²€ì¦ ì‹œì‘")

    # 1. ëœë¤ ìƒ˜í”Œ ì¶”ì¶œ
    samples = extract_random_samples()

    if len(samples) == 0:
        logger.error("âŒ ì¶”ì¶œëœ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. ìˆ˜ë™ ê²€ì¦ (ì¸í„°ë™í‹°ë¸Œ)
    logger.info("\nâš ï¸  ì£¼ì˜: ì´ ë‹¨ê³„ëŠ” ì¸í„°ë™í‹°ë¸Œ ì…ë ¥ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    logger.info("ê° ìƒ˜í”Œì„ í™•ì¸í•˜ê³  Y/Nì„ ì…ë ¥í•˜ì„¸ìš”.\n")

    proceed = input("ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Y/N): ").strip().upper()
    if proceed != "Y":
        logger.info("ì¤‘ë‹¨ë¨")
        return

    ground_truth_samples = manual_validation(samples)

    if len(ground_truth_samples) == 0:
        logger.error("âŒ ê²€ì¦ëœ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 3. Ground Truth CSV ì €ì¥
    save_ground_truth_csv(ground_truth_samples)

    # 4. F1-Score ê³„ì‚°
    metrics = calculate_f1_score(ground_truth_samples)

    # 5. F1-Score ê²°ê³¼ ì €ì¥
    save_f1_report(metrics)

    logger.success("ğŸ‰ Ground Truth ê²€ì¦ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
