"""
CrawlAgent - F1-Score ê³„ì‚° ìŠ¤í¬ë¦½íŠ¸
Created: 2025-11-14

Phase 1 PoC ì„±ëŠ¥ í‰ê°€:
- Precision: ì¶”ì¶œí•œ ë°ì´í„°ì˜ ì •í™•ë„
- Recall: í•„ìš”í•œ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì¶”ì¶œí–ˆëŠ”ì§€
- F1-Score: Precisionê³¼ Recallì˜ ì¡°í™” í‰ê· 

í‰ê°€ ê¸°ì¤€:
- Title: 10ì ì´ìƒ â†’ Correct
- Body: 500ì ì´ìƒ â†’ Correct
- Date: ì¡´ì¬ â†’ Correct
- Quality Score: 80+ â†’ Success
"""

import sys
from pathlib import Path
from typing import Dict, List

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from sqlalchemy import func

from src.storage.database import get_db
from src.storage.models import CrawlResult, Selector


def calculate_precision_recall(results: List[CrawlResult]) -> Dict[str, float]:
    """
    Precision & Recall ê³„ì‚°

    Precision = TP / (TP + FP)
    - TP (True Positive): ì¶”ì¶œ ì„±ê³µ & í’ˆì§ˆ 80+
    - FP (False Positive): ì¶”ì¶œí–ˆì§€ë§Œ í’ˆì§ˆ 80 ë¯¸ë§Œ

    Recall = TP / (TP + FN)
    - TP (True Positive): ì¶”ì¶œ ì„±ê³µ & í’ˆì§ˆ 80+
    - FN (False Negative): ì¶”ì¶œ ì‹¤íŒ¨ (í’ˆì§ˆ 80 ë¯¸ë§Œ)

    F1-Score = 2 * (Precision * Recall) / (Precision + Recall)
    """
    total = len(results)

    if total == 0:
        return {
            "total": 0,
            "tp": 0,
            "fp": 0,
            "fn": 0,
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
        }

    # Quality Score ê¸°ì¤€: 80+ = Success
    tp = len([r for r in results if r.quality_score >= 80])  # True Positive
    fp = len(
        [r for r in results if r.quality_score < 80 and r.quality_score >= 50]
    )  # False Positive (ë¶€ë¶„ ì„±ê³µ)
    fn = len([r for r in results if r.quality_score < 50])  # False Negative (ì‹¤íŒ¨)

    # Precision & Recall
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0

    # F1-Score
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

    return {
        "total": total,
        "tp": tp,
        "fp": fp,
        "fn": fn,
        "precision": precision,
        "recall": recall,
        "f1_score": f1_score,
    }


def calculate_field_accuracy(results: List[CrawlResult]) -> Dict[str, float]:
    """
    í•„ë“œë³„ ì •í™•ë„ ê³„ì‚°

    - Title Accuracy: 10ì ì´ìƒ ì¶”ì¶œ ë¹„ìœ¨
    - Body Accuracy: 500ì ì´ìƒ ì¶”ì¶œ ë¹„ìœ¨
    - Date Accuracy: ë‚ ì§œ ì¶”ì¶œ ë¹„ìœ¨
    """
    total = len(results)

    if total == 0:
        return {
            "title_accuracy": 0.0,
            "body_accuracy": 0.0,
            "date_accuracy": 0.0,
            "overall_accuracy": 0.0,
        }

    title_correct = len([r for r in results if r.title and len(r.title) >= 10])
    body_correct = len([r for r in results if r.body and len(r.body) >= 500])
    date_correct = len([r for r in results if r.date])

    title_accuracy = title_correct / total
    body_accuracy = body_correct / total
    date_accuracy = date_correct / total
    overall_accuracy = (title_accuracy + body_accuracy + date_accuracy) / 3

    return {
        "title_accuracy": title_accuracy,
        "body_accuracy": body_accuracy,
        "date_accuracy": date_accuracy,
        "overall_accuracy": overall_accuracy,
    }


def calculate_uc_performance(results: List[CrawlResult]) -> Dict[str, Dict]:
    """
    UCë³„ ì„±ëŠ¥ ê³„ì‚° (UC1, UC2, UC3)
    """
    uc_stats = {"scrapy": [], "2-agent": []}

    for result in results:
        if result.crawl_mode:
            uc_stats[result.crawl_mode].append(result)

    performance = {}

    for mode, mode_results in uc_stats.items():
        if len(mode_results) == 0:
            continue

        metrics = calculate_precision_recall(mode_results)
        field_acc = calculate_field_accuracy(mode_results)

        avg_quality = sum([r.quality_score for r in mode_results if r.quality_score]) / len(
            mode_results
        )

        performance[mode] = {
            "count": len(mode_results),
            "avg_quality": avg_quality,
            "precision": metrics["precision"],
            "recall": metrics["recall"],
            "f1_score": metrics["f1_score"],
            "title_accuracy": field_acc["title_accuracy"],
            "body_accuracy": field_acc["body_accuracy"],
            "date_accuracy": field_acc["date_accuracy"],
        }

    return performance


def calculate_site_performance(results: List[CrawlResult]) -> Dict[str, Dict]:
    """
    ì‚¬ì´íŠ¸ë³„ ì„±ëŠ¥ ê³„ì‚°
    """
    site_stats = {}

    for result in results:
        if result.site_name not in site_stats:
            site_stats[result.site_name] = []
        site_stats[result.site_name].append(result)

    performance = {}

    for site, site_results in site_stats.items():
        metrics = calculate_precision_recall(site_results)
        field_acc = calculate_field_accuracy(site_results)

        avg_quality = sum([r.quality_score for r in site_results if r.quality_score]) / len(
            site_results
        )

        performance[site] = {
            "count": len(site_results),
            "avg_quality": avg_quality,
            "f1_score": metrics["f1_score"],
            "title_accuracy": field_acc["title_accuracy"],
            "body_accuracy": field_acc["body_accuracy"],
            "date_accuracy": field_acc["date_accuracy"],
        }

    return performance


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("=" * 80)
    print("CrawlAgent Phase 1 PoC - F1-Score í‰ê°€")
    print("=" * 80)
    print()

    # DB ì—°ê²°
    db_gen = get_db()
    db = next(db_gen)

    try:
        # ëª¨ë“  í¬ë¡¤ë§ ê²°ê³¼ ì¡°íšŒ
        all_results = db.query(CrawlResult).all()

        print(f"ğŸ“Š ì „ì²´ í¬ë¡¤ë§ ê²°ê³¼: {len(all_results)}ê°œ")
        print()

        # 1. ì „ì²´ ì„±ëŠ¥ í‰ê°€
        print("=" * 80)
        print("1ï¸âƒ£  ì „ì²´ ì„±ëŠ¥ í‰ê°€")
        print("=" * 80)

        overall_metrics = calculate_precision_recall(all_results)
        overall_field_acc = calculate_field_accuracy(all_results)

        print(f"âœ“ Total Results: {overall_metrics['total']}")
        print(f"âœ“ True Positive (Quality 80+): {overall_metrics['tp']}")
        print(f"âœ“ False Positive (Quality 50-79): {overall_metrics['fp']}")
        print(f"âœ“ False Negative (Quality <50): {overall_metrics['fn']}")
        print()
        print(f"ğŸ“ˆ Precision: {overall_metrics['precision']:.2%}")
        print(f"ğŸ“ˆ Recall: {overall_metrics['recall']:.2%}")
        print(f"ğŸ¯ F1-Score: {overall_metrics['f1_score']:.2%}")
        print()
        print(f"âœ“ Title Accuracy (10+ chars): {overall_field_acc['title_accuracy']:.2%}")
        print(f"âœ“ Body Accuracy (500+ chars): {overall_field_acc['body_accuracy']:.2%}")
        print(f"âœ“ Date Accuracy: {overall_field_acc['date_accuracy']:.2%}")
        print(f"âœ“ Overall Accuracy: {overall_field_acc['overall_accuracy']:.2%}")
        print()

        # 2. UCë³„ ì„±ëŠ¥ í‰ê°€
        print("=" * 80)
        print("2ï¸âƒ£  UCë³„ ì„±ëŠ¥ í‰ê°€ (Crawl Mode)")
        print("=" * 80)

        uc_performance = calculate_uc_performance(all_results)

        for mode, perf in uc_performance.items():
            print(f"\n[{mode.upper()}]")
            print(f"  Count: {perf['count']}")
            print(f"  Avg Quality: {perf['avg_quality']:.1f}")
            print(f"  Precision: {perf['precision']:.2%}")
            print(f"  Recall: {perf['recall']:.2%}")
            print(f"  F1-Score: {perf['f1_score']:.2%}")
            print(f"  Title Accuracy: {perf['title_accuracy']:.2%}")
            print(f"  Body Accuracy: {perf['body_accuracy']:.2%}")
            print(f"  Date Accuracy: {perf['date_accuracy']:.2%}")

        print()

        # 3. ì‚¬ì´íŠ¸ë³„ ì„±ëŠ¥ í‰ê°€
        print("=" * 80)
        print("3ï¸âƒ£  ì‚¬ì´íŠ¸ë³„ ì„±ëŠ¥ í‰ê°€ (Top 10)")
        print("=" * 80)

        site_performance = calculate_site_performance(all_results)

        # F1-Score ê¸°ì¤€ ì •ë ¬
        sorted_sites = sorted(
            site_performance.items(), key=lambda x: x[1]["f1_score"], reverse=True
        )

        for i, (site, perf) in enumerate(sorted_sites[:10], 1):
            print(f"\n{i}. {site}")
            print(f"   Count: {perf['count']}")
            print(f"   Avg Quality: {perf['avg_quality']:.1f}")
            print(f"   F1-Score: {perf['f1_score']:.2%}")
            print(
                f"   Title: {perf['title_accuracy']:.2%} | Body: {perf['body_accuracy']:.2%} | Date: {perf['date_accuracy']:.2%}"
            )

        print()

        # 4. Quality Score ë¶„í¬
        print("=" * 80)
        print("4ï¸âƒ£  Quality Score ë¶„í¬")
        print("=" * 80)

        score_ranges = {
            "90-100": len([r for r in all_results if r.quality_score >= 90]),
            "80-89": len([r for r in all_results if 80 <= r.quality_score < 90]),
            "70-79": len([r for r in all_results if 70 <= r.quality_score < 80]),
            "60-69": len([r for r in all_results if 60 <= r.quality_score < 70]),
            "50-59": len([r for r in all_results if 50 <= r.quality_score < 60]),
            "0-49": len([r for r in all_results if r.quality_score < 50]),
        }

        for range_name, count in score_ranges.items():
            percentage = count / len(all_results) * 100 if len(all_results) > 0 else 0
            bar = "â–ˆ" * int(percentage / 2)
            print(f"{range_name}: {count:4d} ({percentage:5.1f}%) {bar}")

        print()

        # 5. ìš”ì•½
        print("=" * 80)
        print("5ï¸âƒ£  Phase 1 PoC í‰ê°€ ìš”ì•½")
        print("=" * 80)

        success_rate = (
            (overall_metrics["tp"] / overall_metrics["total"] * 100)
            if overall_metrics["total"] > 0
            else 0
        )

        print(f"âœ… ì „ì²´ F1-Score: {overall_metrics['f1_score']:.2%}")
        print(f"âœ… ì„±ê³µë¥  (Quality 80+): {success_rate:.1f}%")
        print(f"âœ… í‰ê·  ì •í™•ë„ (3-Field): {overall_field_acc['overall_accuracy']:.2%}")
        print(f"âœ… í…ŒìŠ¤íŠ¸ ì‚¬ì´íŠ¸ ìˆ˜: {len(site_performance)}ê°œ")
        print(f"âœ… ì´ í¬ë¡¤ë§ ê²°ê³¼: {len(all_results)}ê°œ")

        print()
        print("=" * 80)

    finally:
        db.close()


if __name__ == "__main__":
    main()
