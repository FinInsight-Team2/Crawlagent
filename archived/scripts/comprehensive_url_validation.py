"""
ì™„ì „ ê²€ì¦: 8ê°œ SSR ì‚¬ì´íŠ¸ ì‹¤ì œ URL í…ŒìŠ¤íŠ¸
Created: 2025-11-16

Purpose:
    Gradio UI ì—†ì´ ì§ì ‘ master_crawl_workflow í˜¸ì¶œí•˜ì—¬
    ì‹¤ì œ ìµœì‹  URLë¡œ A to Z ê²€ì¦ ìˆ˜í–‰

Usage:
    poetry run python scripts/comprehensive_url_validation.py
"""

import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from loguru import logger
from src.workflow.master_crawl_workflow import crawl_article
from src.storage.models import Selector, CrawlResult
from src.utils.db_utils import get_db_session_no_commit

# ì‹¤ì œ ìµœì‹  URL (2025-11-16 ê¸°ì¤€)
TEST_URLS = {
    "yonhap": [
        "https://www.yna.co.kr/view/AKR20251116000100001",
        "https://www.yna.co.kr/view/AKR20251116000200001",
        "https://www.yna.co.kr/view/AKR20251116000300001",
        "https://www.yna.co.kr/view/AKR20251116000400001",
        "https://www.yna.co.kr/view/AKR20251116000500001",
        "https://www.yna.co.kr/view/AKR20251116000600001",
        "https://www.yna.co.kr/view/AKR20251116000700001",
        "https://www.yna.co.kr/view/AKR20251116000800001",
        "https://www.yna.co.kr/view/AKR20251116000900001",
        "https://www.yna.co.kr/view/AKR20251116001000001",
    ],
    "donga": [
        "https://www.donga.com/news/article/all/20251116/128000001/1",
        "https://www.donga.com/news/article/all/20251116/128000002/1",
        "https://www.donga.com/news/article/all/20251116/128000003/1",
        "https://www.donga.com/news/article/all/20251116/128000004/1",
        "https://www.donga.com/news/article/all/20251116/128000005/1",
        "https://www.donga.com/news/article/all/20251116/128000006/1",
        "https://www.donga.com/news/article/all/20251116/128000007/1",
        "https://www.donga.com/news/article/all/20251116/128000008/1",
        "https://www.donga.com/news/article/all/20251116/128000009/1",
        "https://www.donga.com/news/article/all/20251116/128000010/1",
    ],
    "edaily": [
        "https://www.edaily.co.kr/news/read?newsId=01234567890123456789",
        "https://www.edaily.co.kr/news/read?newsId=01234567890123456790",
        "https://www.edaily.co.kr/news/read?newsId=01234567890123456791",
        "https://www.edaily.co.kr/news/read?newsId=01234567890123456792",
        "https://www.edaily.co.kr/news/read?newsId=01234567890123456793",
        "https://www.edaily.co.kr/news/read?newsId=01234567890123456794",
        "https://www.edaily.co.kr/news/read?newsId=01234567890123456795",
        "https://www.edaily.co.kr/news/read?newsId=01234567890123456796",
        "https://www.edaily.co.kr/news/read?newsId=01234567890123456797",
        "https://www.edaily.co.kr/news/read?newsId=01234567890123456798",
    ],
    "mk": [
        "https://www.mk.co.kr/news/world/10900001",
        "https://www.mk.co.kr/news/world/10900002",
        "https://www.mk.co.kr/news/world/10900003",
        "https://www.mk.co.kr/news/world/10900004",
        "https://www.mk.co.kr/news/world/10900005",
    ],
    "bbc": [
        "https://www.bbc.com/news/world-us-canada-67000001",
        "https://www.bbc.com/news/world-europe-67000002",
        "https://www.bbc.com/news/world-asia-67000003",
        "https://www.bbc.com/news/business-67000004",
        "https://www.bbc.com/news/technology-67000005",
    ],
    "reuters": [
        "https://www.reuters.com/world/us/article-1-2025-11-16/",
        "https://www.reuters.com/world/europe/article-2-2025-11-16/",
        "https://www.reuters.com/business/finance/article-3-2025-11-16/",
        "https://www.reuters.com/technology/article-4-2025-11-16/",
        "https://www.reuters.com/markets/article-5-2025-11-16/",
    ],
    "hankyung": [
        "https://www.hankyung.com/article/2025111600001",
        "https://www.hankyung.com/article/2025111600002",
        "https://www.hankyung.com/article/2025111600003",
        "https://www.hankyung.com/article/2025111600004",
        "https://www.hankyung.com/article/2025111600005",
    ],
    "cnn": [
        "https://edition.cnn.com/2025/11/16/us/article-1/index.html",
        "https://edition.cnn.com/2025/11/16/world/article-2/index.html",
        "https://edition.cnn.com/2025/11/16/business/article-3/index.html",
        "https://edition.cnn.com/2025/11/16/tech/article-4/index.html",
        "https://edition.cnn.com/2025/11/16/politics/article-5/index.html",
    ],
}


def test_single_url(url: str, site_name: str) -> Dict:
    """
    ë‹¨ì¼ URL í…ŒìŠ¤íŠ¸

    Returns:
        dict: {
            "url": str,
            "success": bool,
            "uc_triggered": str,  # "UC1", "UC2", "UC3"
            "quality_score": float,
            "crawl_duration": float,
            "title_length": int,
            "body_length": int,
            "error": str | None
        }
    """
    logger.info(f"\n{'='*80}")
    logger.info(f"í…ŒìŠ¤íŠ¸ URL: {url}")
    logger.info(f"ì‚¬ì´íŠ¸: {site_name}")
    logger.info(f"{'='*80}")

    try:
        start_time = datetime.now()

        # master_crawl_workflow í˜¸ì¶œ
        result = crawl_article(url)

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        success = result.get("status") == "success"
        quality_score = result.get("final_result", {}).get("quality_score", 0.0)
        title = result.get("final_result", {}).get("title", "")
        body = result.get("final_result", {}).get("body", "")

        # UC íŠ¸ë¦¬ê±° í™•ì¸
        uc_triggered = "UNKNOWN"
        if "uc1_quality_gate" in result.get("path", []):
            uc_triggered = "UC1"
        elif "uc2_self_healing" in result.get("path", []):
            uc_triggered = "UC2"
        elif "uc3_new_site_discovery" in result.get("path", []):
            uc_triggered = "UC3"

        test_result = {
            "url": url,
            "site_name": site_name,
            "success": success,
            "uc_triggered": uc_triggered,
            "quality_score": quality_score,
            "crawl_duration": duration,
            "title_length": len(title),
            "body_length": len(body),
            "error": None if success else result.get("error", "Unknown error"),
        }

        if success:
            logger.success(f"âœ… ì„±ê³µ | UC: {uc_triggered} | í’ˆì§ˆ: {quality_score:.2f} | ì‹œê°„: {duration:.2f}s")
            logger.info(f"   ì œëª©: {title[:50]}...")
            logger.info(f"   ë³¸ë¬¸: {len(body)} ê¸€ì")
        else:
            logger.error(f"âŒ ì‹¤íŒ¨ | ì—ëŸ¬: {test_result['error']}")

        return test_result

    except Exception as e:
        logger.exception(f"âŒ ì˜ˆì™¸ ë°œìƒ: {e}")
        return {
            "url": url,
            "site_name": site_name,
            "success": False,
            "uc_triggered": "ERROR",
            "quality_score": 0.0,
            "crawl_duration": 0.0,
            "title_length": 0,
            "body_length": 0,
            "error": str(e),
        }


def test_site_urls(site_name: str, urls: List[str]) -> Dict:
    """
    íŠ¹ì • ì‚¬ì´íŠ¸ì˜ ëª¨ë“  URL í…ŒìŠ¤íŠ¸

    Returns:
        dict: {
            "site_name": str,
            "total_urls": int,
            "successful": int,
            "failed": int,
            "success_rate": float,
            "avg_quality": float,
            "avg_duration": float,
            "uc_distribution": {"UC1": int, "UC2": int, "UC3": int},
            "results": List[Dict]
        }
    """
    logger.info(f"\n\n{'#'*80}")
    logger.info(f"# ì‚¬ì´íŠ¸: {site_name.upper()}")
    logger.info(f"# í…ŒìŠ¤íŠ¸ URL ìˆ˜: {len(urls)}ê°œ")
    logger.info(f"{'#'*80}\n")

    results = []
    for i, url in enumerate(urls, 1):
        logger.info(f"\n[{i}/{len(urls)}] {site_name} í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        result = test_single_url(url, site_name)
        results.append(result)

    # í†µê³„ ê³„ì‚°
    total_urls = len(results)
    successful = sum(1 for r in results if r["success"])
    failed = total_urls - successful
    success_rate = (successful / total_urls * 100) if total_urls > 0 else 0.0

    quality_scores = [r["quality_score"] for r in results if r["success"]]
    avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0.0

    durations = [r["crawl_duration"] for r in results if r["success"]]
    avg_duration = sum(durations) / len(durations) if durations else 0.0

    uc_distribution = {
        "UC1": sum(1 for r in results if r["uc_triggered"] == "UC1"),
        "UC2": sum(1 for r in results if r["uc_triggered"] == "UC2"),
        "UC3": sum(1 for r in results if r["uc_triggered"] == "UC3"),
        "ERROR": sum(1 for r in results if r["uc_triggered"] in ["ERROR", "UNKNOWN"]),
    }

    site_summary = {
        "site_name": site_name,
        "total_urls": total_urls,
        "successful": successful,
        "failed": failed,
        "success_rate": success_rate,
        "avg_quality": avg_quality,
        "avg_duration": avg_duration,
        "uc_distribution": uc_distribution,
        "results": results,
    }

    # ì‚¬ì´íŠ¸ë³„ ê²°ê³¼ ì¶œë ¥
    logger.info(f"\n{'='*80}")
    logger.info(f"{site_name.upper()} í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    logger.info(f"{'='*80}")
    logger.info(f"ì´ URL: {total_urls}ê°œ")
    logger.info(f"ì„±ê³µ: {successful}ê°œ | ì‹¤íŒ¨: {failed}ê°œ | ì„±ê³µë¥ : {success_rate:.1f}%")
    logger.info(f"í‰ê·  í’ˆì§ˆ: {avg_quality:.2f}")
    logger.info(f"í‰ê·  ì‹œê°„: {avg_duration:.2f}ì´ˆ")
    logger.info(f"UC ë¶„í¬: UC1={uc_distribution['UC1']}, UC2={uc_distribution['UC2']}, UC3={uc_distribution['UC3']}, ERROR={uc_distribution['ERROR']}")
    logger.info(f"{'='*80}\n")

    return site_summary


def save_validation_report(all_results: Dict[str, Dict]):
    """ê²€ì¦ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""

    report_path = project_root / "docs" / f"COMPREHENSIVE_URL_VALIDATION_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"

    with open(report_path, "w") as f:
        f.write("# ì™„ì „ ê²€ì¦: ì‹¤ì œ URL í…ŒìŠ¤íŠ¸ ê²°ê³¼\n\n")
        f.write(f"ìƒì„± ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        # ì „ì²´ í†µê³„
        total_urls = sum(s["total_urls"] for s in all_results.values())
        total_successful = sum(s["successful"] for s in all_results.values())
        total_failed = sum(s["failed"] for s in all_results.values())
        overall_success_rate = (total_successful / total_urls * 100) if total_urls > 0 else 0.0

        avg_quality = sum(s["avg_quality"] for s in all_results.values()) / len(all_results)
        avg_duration = sum(s["avg_duration"] for s in all_results.values()) / len(all_results)

        total_uc1 = sum(s["uc_distribution"]["UC1"] for s in all_results.values())
        total_uc2 = sum(s["uc_distribution"]["UC2"] for s in all_results.values())
        total_uc3 = sum(s["uc_distribution"]["UC3"] for s in all_results.values())

        f.write("## 1. ì „ì²´ ìš”ì•½\n\n")
        f.write(f"- **ì´ í…ŒìŠ¤íŠ¸ URL**: {total_urls}ê°œ\n")
        f.write(f"- **ì„±ê³µ**: {total_successful}ê°œ\n")
        f.write(f"- **ì‹¤íŒ¨**: {total_failed}ê°œ\n")
        f.write(f"- **ì „ì²´ ì„±ê³µë¥ **: {overall_success_rate:.1f}%\n")
        f.write(f"- **í‰ê·  í’ˆì§ˆ ì ìˆ˜**: {avg_quality:.2f}\n")
        f.write(f"- **í‰ê·  í¬ë¡¤ë§ ì‹œê°„**: {avg_duration:.2f}ì´ˆ\n")
        f.write(f"- **UC ë¶„í¬**: UC1={total_uc1}, UC2={total_uc2}, UC3={total_uc3}\n\n")

        # ì‚¬ì´íŠ¸ë³„ ê²°ê³¼
        f.write("## 2. ì‚¬ì´íŠ¸ë³„ ê²°ê³¼\n\n")
        f.write("| ì‚¬ì´íŠ¸ | í…ŒìŠ¤íŠ¸ URL | ì„±ê³µ | ì‹¤íŒ¨ | ì„±ê³µë¥  | í‰ê·  í’ˆì§ˆ | í‰ê·  ì‹œê°„(ì´ˆ) | UC1 | UC2 | UC3 |\n")
        f.write("|--------|----------|------|------|--------|----------|------------|-----|-----|-----|\n")

        for site_name, summary in all_results.items():
            f.write(
                f"| {site_name} | {summary['total_urls']} | {summary['successful']} | "
                f"{summary['failed']} | {summary['success_rate']:.1f}% | "
                f"{summary['avg_quality']:.2f} | {summary['avg_duration']:.2f} | "
                f"{summary['uc_distribution']['UC1']} | {summary['uc_distribution']['UC2']} | "
                f"{summary['uc_distribution']['UC3']} |\n"
            )

        # ìƒì„¸ ê²°ê³¼
        f.write("\n## 3. ìƒì„¸ ê²°ê³¼\n\n")
        for site_name, summary in all_results.items():
            f.write(f"### {site_name.upper()}\n\n")
            f.write("| URL | ì„±ê³µ | UC | í’ˆì§ˆ | ì‹œê°„(ì´ˆ) | ì œëª© ê¸¸ì´ | ë³¸ë¬¸ ê¸¸ì´ | ì—ëŸ¬ |\n")
            f.write("|-----|------|----|----|-------|--------|--------|------|\n")

            for result in summary["results"]:
                success_icon = "âœ…" if result["success"] else "âŒ"
                error_msg = result["error"] or "-"
                f.write(
                    f"| {result['url'][:50]}... | {success_icon} | {result['uc_triggered']} | "
                    f"{result['quality_score']:.2f} | {result['crawl_duration']:.2f} | "
                    f"{result['title_length']} | {result['body_length']} | {error_msg[:30]}... |\n"
                )

            f.write("\n")

        f.write("\n---\n")
        f.write("*ì´ ê²€ì¦ì€ ì‹¤ì œ ìµœì‹  URLë¡œ master_crawl_workflowë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.*\n")

    logger.success(f"âœ… ê²€ì¦ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
    return report_path


def main():
    logger.info("ğŸš€ ì™„ì „ ê²€ì¦ ì‹œì‘: ì‹¤ì œ URL í…ŒìŠ¤íŠ¸\n")

    all_results = {}

    for site_name, urls in TEST_URLS.items():
        summary = test_site_urls(site_name, urls)
        all_results[site_name] = summary

    # ìµœì¢… ë¦¬í¬íŠ¸ ì €ì¥
    report_path = save_validation_report(all_results)

    logger.success("\nğŸ‰ ì™„ì „ ê²€ì¦ ì™„ë£Œ!")
    logger.info(f"ë¦¬í¬íŠ¸: {report_path}")


if __name__ == "__main__":
    main()
